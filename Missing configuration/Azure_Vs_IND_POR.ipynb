{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#KA_Path  = r'E:\\_Projects\\DEP POR\\Key Account\\Input\\\\'\n",
    "\n",
    "Path = r'E:\\_Projects\\DEP POR\\Azure Vs IND POR\\\\'\n",
    "\n",
    "\n",
    "# 0 - Pul Data from SQL / 1 - Load Data from local\n",
    "SQL_Data = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Load Files\n",
    "\n",
    "POR = glob.glob(Path+'Input\\ATLEOS OCP IND POR_???????????.csv')\n",
    "\n",
    "#print(POR)\n",
    "\n",
    "\n",
    "for POR in POR:\n",
    "    IND_POR = pd.read_csv(POR,low_memory=False)\n",
    "    \n",
    "    \n",
    "    IND_POR = IND_POR[IND_POR['Data Series'].str.contains('NEW DEMAND',na=False) & IND_POR['Item Type'].str.contains('Unassigned',na=False)]\n",
    "    \n",
    "    mask = IND_POR['Country Code'].isnull()\n",
    "\n",
    "    IND_POR.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "    #IND_POR.to_csv(Path+'Output.csv')\n",
    "    \n",
    "    print(IND_POR.columns)\n",
    "\n",
    "\n",
    "\n",
    "month=input(\"\\n********************************************************** \\nEnter Date from Header : \")\n",
    "\n",
    "index = IND_POR.columns.get_loc('Comments')+1\n",
    "IND_POR =IND_POR.iloc[:,0:index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd4474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling KA from Oracle SQL...........!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vv185114\\AppData\\Local\\Temp\\9\\ipykernel_7580\\3863649184.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  KA = pd.read_sql(query, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['KEY_ACCOUNT', 'MASTER_CUST_ID', 'CREATION_DATE', 'CREATED_BY',\n",
      "       'LAST_UPDATE_DATE', 'LAST_UPDATED_BY', 'LAST_UPDATE_LOGIN',\n",
      "       'MASTER_CUST_NUM', 'MASTER_CUST_INDST', 'MASTER_CUST_NAME',\n",
      "       'KEY_ACCOUNT_ID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "############################# PUll KA File from Orale SQL #########################\n",
    "#KA = pd.read_excel(Path+'Input\\SQL.xlsx',keep_default_na=False)\n",
    "import oracledb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if SQL_Data == 0:\n",
    "    \n",
    "    print('Pulling KA from Oracle SQL...........!')\n",
    "\n",
    "    # Enable thick mode\n",
    "    oracledb.init_oracle_client(lib_dir=r\"E:\\softwares\\instantclient-basic-windows.x64-23.5.0.24.07\\instantclient_23_5\")\n",
    "\n",
    "    # Connection details\n",
    "    dsn_tns = oracledb.makedsn('sunatm2001-vip.daytonoh.ncr.com', 9007, service_name='ebs_ATMPROD')\n",
    "\n",
    "\n",
    "    # Use a context manager to handle the connection\n",
    "    with oracledb.connect(user='erp_user', password='erp_pass', dsn=dsn_tns) as conn:\n",
    "        KA = pd.DataFrame()\n",
    "\n",
    "        query = f\"\"\"\n",
    "        select DISTINCT  * from\n",
    "        NCR_DEM_KEY_ACCOUNT_ASSG\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch data into a pandas DataFrame and append to the main DataFrame\n",
    "        KA = pd.read_sql(query, con=conn)\n",
    "        \n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    KA.to_csv(Path+'Input\\\\ref\\\\SQL.csv', index=False)\n",
    "else:\n",
    "    KA = pd.read_csv(Path+'Input\\\\ref\\\\SQL.csv')\n",
    "\n",
    "#print(KA.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46efa3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Loading Auzre File\n",
    "Azure = pd.read_csv(Path+\"Input\\Azure.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92607c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling Region from Oracle SQL...........!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vv185114\\AppData\\Local\\Temp\\9\\ipykernel_7580\\3950031983.py:37: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  KA = pd.read_sql(query, con=conn)\n"
     ]
    }
   ],
   "source": [
    "############################# PUll KA File from Orale SQL #########################\n",
    "#KA = pd.read_excel(Path+'Input\\SQL.xlsx',keep_default_na=False)\n",
    "import oracledb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if SQL_Data == 0:\n",
    "    \n",
    "    print('Pulling Region from Oracle SQL...........!')\n",
    "\n",
    "    # Enable thick mode\n",
    "    oracledb.init_oracle_client(lib_dir=r\"E:\\softwares\\instantclient-basic-windows.x64-23.5.0.24.07\\instantclient_23_5\")\n",
    "\n",
    "    # Connection details\n",
    "    dsn_tns = oracledb.makedsn('sunatm2001-vip.daytonoh.ncr.com', 9007, service_name='ebs_ATMPROD')\n",
    "\n",
    "\n",
    "    # Use a context manager to handle the connection\n",
    "    with oracledb.connect(user='erp_user', password='erp_pass', dsn=dsn_tns) as conn:\n",
    "        Region = pd.DataFrame()\n",
    "\n",
    "        query = f\"\"\"\n",
    "        select DISTINCT  * from\n",
    "        apps.ncr_ocp_sales_hierarchy\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch data into a pandas DataFrame and append to the main DataFrame\n",
    "        Region = pd.read_sql(query, con=conn)\n",
    "        \n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    Region.to_csv(Path+'Input\\\\ref\\\\Region.csv', index=False)\n",
    "else:\n",
    "    Region = pd.read_csv(Path+'Input\\\\ref\\\\Region.csv')\n",
    "\n",
    "\n",
    "mask = Region['COUNTRY_CODE'].isnull()\n",
    "\n",
    "Region.loc[mask,'COUNTRY_CODE'] = 'NA'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c460dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ AZURE ##################################################\n",
    "############################## Remove Spaces\n",
    "\n",
    "Azure['ISO Country Code'] = Azure['ISO Country Code'].str.strip()\n",
    "\n",
    "mask = Azure['ISO Country Code'].isnull()\n",
    "\n",
    "Azure.loc[mask,'ISO Country Code'] = 'NA'\n",
    "\n",
    "\n",
    "\n",
    "Azure = pd.merge(Azure, Region[['COUNTRY_CODE','NEW_COUNTRY']],left_on='ISO Country Code',right_on='COUNTRY_CODE',how='left')\n",
    "\n",
    "Azure.loc[Azure['NEW_COUNTRY'].notnull(),'ISO Country Code']=Azure['NEW_COUNTRY']\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*NEW_COUNTRY$)')\n",
    "Azure = Azure.filter(regex='^(?!.*COUNTRY_CODE$)')\n",
    "\n",
    "#################################### SSD & Ship Date filter\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "Azure['Actual Ship Date'] = pd.to_datetime(Azure['Actual Ship Date'])\n",
    "Azure['Scheduled Ship Date'] = pd.to_datetime(Azure['Scheduled Ship Date'])\n",
    "\n",
    "\n",
    "\n",
    "# Create a new column 'SD_Date' with the month and year of the 'Actual Ship Date'\n",
    "Azure['SD_Date'] = Azure['Actual Ship Date'].dt.strftime('%b-%y')\n",
    "\n",
    "# Get the current date\n",
    "latest_month = datetime.now()\n",
    "\n",
    "# Create a list of the last 7 months\n",
    "check_month = [latest_month - relativedelta(months=x) for x in range(7)]\n",
    "\n",
    "# Convert the list of datetime objects to strings in the format '%b-%y'\n",
    "check_month_str = [month.strftime('%b-%y') for month in check_month]\n",
    "\n",
    "# Filter the Azure dataframe to only include rows where 'SD_Date' is in 'check_month_str'\n",
    "Azure_Actual_Ship = Azure[Azure['SD_Date'].isin(check_month_str)]\n",
    "\n",
    "\n",
    "# Create a list of the last 7 months\n",
    "check_month = [latest_month + relativedelta(months=x) for x in range(13)]\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list of datetime objects to strings in the format '%b-%y'\n",
    "check_month_str = [month.strftime('%b-%y') for month in check_month]\n",
    "\n",
    "\n",
    "# Create a new column 'SD_Date' with the month and year of the 'Actual Ship Date'\n",
    "Azure['SD_Date'] = Azure['Scheduled Ship Date'].dt.strftime('%b-%y')\n",
    "\n",
    "\n",
    "# Filter the Azure dataframe to only include rows where 'SD_Date' is in 'check_month_str'\n",
    "Azure_SSD = Azure[Azure['SD_Date'].isin(check_month_str) & Azure['Actual Ship Date'].isnull()]\n",
    "\n",
    "\n",
    "\n",
    "Azure = pd.concat([Azure_Actual_Ship,Azure_SSD],axis=0)\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*SSD_Date$)')\n",
    "Azure = Azure.filter(regex='^(?!.*SD_Date$)')\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "#################################### SSD & Ship Dt Validation process\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('*************************************************\\n')\n",
    "\n",
    "print('Checking Backward 6month Shipment & Foward 12 month SSD \\n')\n",
    "\n",
    "print('*************************************************\\n')\n",
    "####### Ship Date Validation\n",
    "Azure['SD_Date'] = Azure['Actual Ship Date'].dt.strftime('%b-%y')\n",
    "\n",
    "#print(Azure['SD_Date'])\n",
    "\n",
    "Azure['SD_Date'] = Azure['SD_Date'].astype(str)\n",
    "\n",
    "# Get the latest month in the 'month' column\n",
    "latest_month = datetime.now()\n",
    "\n",
    "# Calculate the month and year 4 months before the latest month\n",
    "check_month = latest_month - relativedelta(months=6)\n",
    "\n",
    "#print(type(check_month))\n",
    "\n",
    "# Convert check_month to the same format as Azure['month']\n",
    "check_month_str = check_month.strftime('%b-%y')\n",
    "\n",
    "# Check if the calculated month exists in the 'month' column\n",
    "is_present = (Azure['SD_Date'] == check_month_str).any()\n",
    "\n",
    "print(f\"Backward 6 Month Shipment Date       : {check_month_str} present? {is_present}\")\n",
    "\n",
    "############### SSD Validation\n",
    "\n",
    "\n",
    "Azure['SSD_Date'] = Azure['Scheduled Ship Date'].dt.strftime('%b-%y')\n",
    "\n",
    "#print(Azure['SSD_Date'])\n",
    "\n",
    "Azure['SSD_Date'] = Azure['SSD_Date'].astype(str)\n",
    "\n",
    "# Get the latest month in the 'month' column\n",
    "latest_month = datetime.now()\n",
    "\n",
    "# Calculate the month and year 4 months before the latest month\n",
    "check_month = latest_month + relativedelta(months=12)\n",
    "\n",
    "#print(type(check_month))\n",
    "\n",
    "# Convert check_month to the same format as Azure['month']\n",
    "check_month_str = check_month.strftime('%b-%y')\n",
    "\n",
    "# Check if the calculated month exists in the 'month' column\n",
    "is_present = (Azure['SSD_Date'] == check_month_str).any()\n",
    "\n",
    "print(f\"Forward 12 Month Schedule Ship Date  : {check_month_str} present? {is_present}\")\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*SSD_Date$)')\n",
    "Azure = Azure.filter(regex='^(?!.*SD_Date$)')\n",
    "\n",
    "\n",
    "#Azure.to_csv(Path+'Output.csv')\n",
    "print('*************************************************\\n')\n",
    "continues = input('Press Enter to Continue......... :')\n",
    "\n",
    "#############################################################################\n",
    "############################## Including KA \n",
    "#print(KA.columns)\n",
    "\n",
    "KA = KA[['MASTER_CUST_NUM','KEY_ACCOUNT','MASTER_CUST_NAME']]\n",
    "\n",
    "KA['MASTER_CUST_NUM'] = KA['MASTER_CUST_NUM'].astype(str)\n",
    "Azure['Master Customer Number'] = Azure['Master Customer Number'].astype(str)\n",
    "\n",
    "KA.drop_duplicates(inplace=True)\n",
    "\n",
    "Azure = pd.merge(Azure, KA, left_on='Master Customer Number',right_on='MASTER_CUST_NUM',how='left')\n",
    "\n",
    "Azure.loc[Azure['KEY_ACCOUNT'].isnull(),'KEY_ACCOUNT'] = \"OTHER\"\n",
    "\n",
    "\n",
    "Azure.loc[Azure['KEY_ACCOUNT']=='OTHER','KEY_ACCOUNT'] = 'OTHER-'+Azure['ISO Country Code']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################## Data Series\n",
    "\n",
    "# Create a boolean mask for non-null 'Actual Ship Date'\n",
    "mask = Azure['Actual Ship Date'].notnull()\n",
    "\n",
    "# Update the 'Data Series' column where the mask is True\n",
    "Azure.loc[mask, 'Data Series'] = 'SHIP'\n",
    "\n",
    "# Update the '6M Ship' column where the mask is True\n",
    "Azure.loc[mask, '6M Ship'] = Azure['Net Quantity']\n",
    "\n",
    "\n",
    "\n",
    "mask = Azure['Scheduled Ship Date'].notnull() & Azure['Actual Ship Date'].isnull()\n",
    "\n",
    "Azure.loc[mask,'Data Series'] = 'Order'\n",
    "\n",
    "Azure.loc[mask,'6M Order'] = Azure['Net Quantity']\n",
    "\n",
    "############################################\n",
    "############################################\n",
    "\n",
    "###### Keep w/o blank in Data series\n",
    "Azure = Azure[Azure['Data Series'].notnull()]\n",
    "\n",
    "\n",
    "################### Check list\n",
    "Azure['MCID Class'] = Azure['MCID Class'].astype(int).round(0)\n",
    "\n",
    "Azure['Concat'] = Azure['ISO Country Code']+Azure['KEY_ACCOUNT']+Azure['MCID Class'].astype(str)\n",
    "\n",
    "\n",
    "Azure[['ISO Country Code','Order Number','Order Type Name','Line Number','Shipment Number','Master Customer Number','Master Customer Name','KEY_ACCOUNT','Offering Accounting Type Code','MCID Class','Product ID','Product Description','Order Booked Date','Line Request Date Time','Scheduled Ship Date','Scheduled Arrival Date','Actual Ship Date','Line Status','Data Series','6M Ship','6M Order','Net Quantity','Warehouse Name','Organization Code','PID type','MCID-Net Order Value-US','MCID- MCC-US','Purchase Order Number','Invoice Trigger','Functional Group Short Name','region_name','canceled_quantity']].to_csv(Path+'\\Output\\Azure.csv',index=False)\n",
    "\n",
    "#Azure.to_csv(Path+'\\Output\\Azure_checklist.csv',index=False)\n",
    "\n",
    "print('Azure Completed')\n",
    "\n",
    "############################# \n",
    "\n",
    "Check_List = Azure[['Concat','Product ID']]\n",
    "\n",
    "\n",
    "Check_List.drop_duplicates(inplace=True)\n",
    "\n",
    "#Check_List.to_csv(Path+'Output\\Checklist.csv',index=False)\n",
    "\n",
    "\n",
    "#Check_List = pd.read_csv(Path+'Output\\Checklist.csv')\n",
    "\n",
    "\n",
    "Check_List = Check_List.groupby('Concat')['Product ID'].apply(','.join).reset_index()\n",
    "\n",
    "Check_List.to_csv(Path+'\\Output\\Check List.csv',index=False)\n",
    "\n",
    "print('Check list Completed')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################### IND POR #####################################################\n",
    "\n",
    "########################### insert new publish flag\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Publish Flag')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['Class','Publish flag']]\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class'],keep='first')\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List,on='Class',how='left')\n",
    "\n",
    "#IND_POR = IND_POR[IND_POR['Publish flag'] == \"Y\"]\n",
    "\n",
    "#print('line 119 : ',IND_POR.columns)\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "IND_POR['Concat'] = IND_POR['Country Code']+ IND_POR['Key Account'] +IND_POR['Class'].astype(str)\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Check_List, on='Concat', how='left')\n",
    "\n",
    "\n",
    "#IND_POR.to_csv(Path+'IND_POR.csv',index=False)\n",
    "\n",
    "mask = IND_POR['Product ID'].isnull()\n",
    "\n",
    "if mask.any():\n",
    "    IND_POR['Concat_1'] = IND_POR['Bill to Country'] + IND_POR['KeyAccount_Billing'] + IND_POR['Class'].astype(str)\n",
    "    \n",
    "    IND_POR = pd.merge(IND_POR, Check_List, left_on='Concat_1', right_on='Concat', how='left')\n",
    "    \n",
    "    \n",
    "mask = (IND_POR['Product ID_x'].isnull() & IND_POR['Product ID_y'].notnull())\n",
    "\n",
    "if mask.any():\n",
    "    IND_POR.loc[mask, 'Country Code'] = IND_POR.loc[mask, 'Bill to Country']\n",
    "    IND_POR.loc[mask, 'Key Account'] = IND_POR.loc[mask, 'KeyAccount_Billing']\n",
    "    IND_POR.loc[mask, 'Product ID_x'] = IND_POR.loc[mask, 'Product ID_y']\n",
    "    \n",
    "\n",
    "IND_POR.loc[IND_POR['Product ID_x'].notnull(),'Partial config']='No'\n",
    "IND_POR.loc[IND_POR['Product ID_x'].isnull(),'Partial config']='Yes'\n",
    "\n",
    "#IND_POR.to_csv(Path+'dEMOD.csv',index=False)\n",
    "\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Product ID_x','MCID'))\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Product ID_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Concat_x$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Concat_1$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Concat_y$)')\n",
    "\n",
    "\n",
    "#print('LIne number 150 :',IND_POR.columns)\n",
    "###########################################################################################\n",
    "############################## Update Plant\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "#print(Mix_Check_List.columns)\n",
    "  \n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class','Country Code'],keep='first')\n",
    "\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List[['Country Code','Class','Plant']],on=['Class','Country Code'],how='left')\n",
    "\n",
    "\n",
    "#####################\n",
    "Mix_Check_List = pd.read_excel(Path+'input\\Mix Check List.xlsx',sheet_name='Publish Flag',keep_default_na=False)\n",
    "\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['As per Sourcing Matrix','Country Code']]\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Country Code'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List,on='Country Code',how='left')\n",
    "\n",
    "#IND_POR.to_csv(Path+'IND_POR1.csv',index=False)\n",
    "\n",
    "####################\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Country Code','Class'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "#IND_POR.to_csv(Path+'IND_POR1.csv',index=False)\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List[['Country Code','Class','Plant']],left_on=['Class','As per Sourcing Matrix'],right_on=['Class','Country Code'],how='left')\n",
    "\n",
    "#IND_POR.to_csv(Path+'IND_POR2.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "mask = (IND_POR['Plant_x'].isnull() & IND_POR['Plant_y'].notnull())\n",
    "\n",
    "if mask.any():\n",
    "    IND_POR.loc[mask, 'Plant_x'] = IND_POR.loc[mask, 'Plant_y']\n",
    "\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Country Code_x', 'Country Code'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Plant_x', 'Plant'))\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Country Code_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Plant_y$)')\n",
    "\n",
    "#print('Line 194 : ',IND_POR.columns)\n",
    "\n",
    "##############################################################################\n",
    "######################### Update DTF\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant',keep_default_na=False)\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class.1', 'Plant.1'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List[['Class.1', 'Plant.1', 'DTF']],left_on=['Class','Plant'],right_on=['Class.1', 'Plant.1'],how='left')\n",
    "\n",
    "#print(Mix_Check_List.columns)\n",
    "#IND_POR.to_csv(Path+'IND_POR2.csv',index=False)\n",
    "\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "IND_POR['Start Date'] = pd.to_datetime(IND_POR['Start Date'])\n",
    "\n",
    "   \n",
    "IND_POR['DTF_y'] = IND_POR['DTF_y'].fillna(0).astype(int)  \n",
    "\n",
    "\n",
    " \n",
    "IND_POR.loc[IND_POR['DTF_y'].notnull() , 'End Date_New'] = (IND_POR['Start Date'] + pd.to_timedelta(IND_POR['DTF_y'].astype(int), unit='D')).dt.strftime('%m/%d/%Y')\n",
    "  \n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('DTF_y', 'DTF'))\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Class.1$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Plant.1$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*End Date$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*DTF_x$)')\n",
    "\n",
    " \n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('End Date_New', 'End Date'))\n",
    "\n",
    "############ Transition Month\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class','Country Code'],keep='first')\n",
    "\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['Class','Country Code','Sea','Air']]\n",
    "\n",
    "Mix_Check_List['Sea'] = Mix_Check_List['Sea'].fillna(0)\n",
    "Mix_Check_List['Air'] = Mix_Check_List['Air'].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "IND_POR['Class'] = IND_POR['Class'].astype(str)\n",
    "Mix_Check_List['Class'] = Mix_Check_List['Class'].astype(str)\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List,on=['Class','Country Code'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List,right_on=['Class','Country Code'],left_on=['Class','As per Sourcing Matrix'],how='left')\n",
    "\n",
    "\n",
    "#print(IND_POR.columns)\n",
    "\n",
    "mask = IND_POR['Sea_x'].isnull() & IND_POR['Sea_y']\n",
    "IND_POR.loc[mask,'Sea_x']=IND_POR['Sea_y']\n",
    "\n",
    "mask = IND_POR['Air_x'].isnull() & IND_POR['Air_y']\n",
    "IND_POR.loc[mask,'Air_x']=IND_POR['Air_y']\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Air_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Sea_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Country Code_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*As per Sourcing Matrix$)')\n",
    "\n",
    "    \n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Sea_x', 'Sea'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Air_x', 'Air'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Country Code_x', 'Country Code'))\n",
    "\n",
    "\n",
    "\n",
    "### MROUND\n",
    "\n",
    "# Convert the 'Sea' column to numeric, and replace non-numeric values with NaN\n",
    "IND_POR['Sea'] = pd.to_numeric(IND_POR['Sea'], errors='coerce')\n",
    "IND_POR['Air'] = pd.to_numeric(IND_POR['Air'], errors='coerce')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "IND_POR['Sea'] = IND_POR['Sea'].fillna(0)\n",
    "IND_POR['Air'] = IND_POR['Air'].fillna(0)\n",
    "\n",
    "# Define the mround function\n",
    "def mround(x, base=30):\n",
    "    return base * round(x/base)\n",
    "\n",
    "# Apply the function to the 'Sea' column\n",
    "IND_POR['Transit Month'] = IND_POR['Sea'].astype(int).apply(mround)\n",
    "\n",
    "\n",
    "IND_POR.loc[IND_POR['Product Range'].str.contains('PC Core'),'Transit Month'] =IND_POR['Air'].astype(int).apply(mround)\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Air$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Sea$)')\n",
    "\n",
    "IND_POR_WO = IND_POR\n",
    "\n",
    "\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Region','LOB Region'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Area','LOB Area'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Region1','Region'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Area2','Area'))\n",
    "\n",
    "columns_to_move = ['Demand Stream','Region1','Area2','Region','Theatre','Area','Country Name','Country Code','Bill to Country','Plant','Publish flag','Zone','Customer LOB','Product LOB','Product Range','Parent Model','Class','Description','DTF','DTF cutoff','Start Date','End Date','DTF risk units','DTF risk','Organization Code','MCID','Partial config','Transit Month','KeyAccount_Billing','Key Account','Master Customer','Item Type','ASP','Data Series']\n",
    "\n",
    "remaining_columns = [col for col in IND_POR_WO.columns if col not in columns_to_move]\n",
    "    \n",
    "IND_POR_WO = IND_POR_WO[columns_to_move + remaining_columns]\n",
    "\n",
    "#IND_POR_WO = IND_POR_WO.filter(regex='^(?!.*Comments$)')\n",
    "\n",
    "IND_POR_WO.to_csv(Path+'\\Output\\IND_POR_WO.csv',index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################### Sourcing Action \n",
    "\n",
    "print('*************************************************')\n",
    "print('\\n')\n",
    "\n",
    "print('IND POR WO Sourcing Action Expoted in Output Folder\\n')\n",
    "\n",
    "\n",
    "print('*************************************************')\n",
    "continues = input('Press Enter Sourcing Action Done : ')\n",
    "\n",
    "\n",
    "###################################### Transit Month Updation\n",
    "#IND_POR = pd.read_csv(Path+'\\Output\\IND_POR_WO.csv',keep_default_na=False)\n",
    "IND_POR = pd.read_csv(Path+'\\Output\\IND_POR_WO.csv')\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "IND_POR['Country Code'] = IND_POR['Country Code'].fillna('NA')\n",
    "IND_POR['Bill to Country'] = IND_POR['Bill to Country'].fillna('NA')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(IND_POR.columns)\n",
    "\n",
    "\n",
    "Index_month = IND_POR.columns.get_loc(month)\n",
    "Index_Comments = IND_POR.columns.get_loc('Comments')\n",
    "\n",
    "Index_30_Start = Index_month -1\n",
    "Index_30_End = Index_Comments-1\n",
    "\n",
    "Index_60_Start = Index_month -2\n",
    "Index_60_End = Index_Comments -2\n",
    "\n",
    "Index_90_Start = Index_month -3\n",
    "Index_90_End = Index_Comments -3\n",
    "\n",
    "\n",
    "End_Month = IND_POR.columns[Index_30_End]\n",
    "End_Month_60 = IND_POR.columns[Index_60_End]\n",
    "End_Month_90 = IND_POR.columns[Index_90_End]\n",
    "\n",
    "\n",
    "IND_POR['Transit Month'] = IND_POR['Transit Month'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "condition_30 = (IND_POR['Data Series'].str.contains('NEW DEMAND',na=False)) & (IND_POR['Transit Month'] == 30)\n",
    "condition_60 = (IND_POR['Data Series'].str.contains('NEW DEMAND',na=False)) & (IND_POR['Transit Month'] == 60)\n",
    "condition_90 = (IND_POR['Data Series'].str.contains('NEW DEMAND',na=False)) & (IND_POR['Transit Month'] == 90)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if any row meets the condition\n",
    "if not IND_POR[condition_30].empty:\n",
    "    \n",
    "    cut_data = IND_POR.loc[condition_30].iloc[:, Index_month:Index_Comments].copy()\n",
    "    \n",
    "    IND_POR.loc[condition_30, IND_POR.columns[Index_30_Start:Index_30_End]] = cut_data.values\n",
    "\n",
    "    IND_POR.loc[condition_30, End_Month] = \"\"\n",
    "\n",
    "if not IND_POR[condition_60].empty:\n",
    "    \n",
    "    cut_data = IND_POR.loc[condition_60].iloc[:, Index_month:Index_Comments].copy()\n",
    "    \n",
    "    IND_POR.loc[condition_60, IND_POR.columns[Index_60_Start:Index_60_End]] = cut_data.values\n",
    "\n",
    "    IND_POR.loc[condition_60, End_Month] = \"\"\n",
    "    \n",
    "    IND_POR.loc[condition_60, End_Month_60] = \"\"\n",
    "\n",
    "if not IND_POR[condition_90].empty:\n",
    "    \n",
    "    cut_data = IND_POR.loc[condition_90].iloc[:, Index_month:Index_Comments].copy()\n",
    "    \n",
    "    IND_POR.loc[condition_90, IND_POR.columns[Index_90_Start:Index_90_End]] = cut_data.values\n",
    "\n",
    "    IND_POR.loc[condition_90, End_Month] = \"\"\n",
    "    \n",
    "    IND_POR.loc[condition_90, End_Month_60] = \"\"\n",
    "\n",
    "    IND_POR.loc[condition_90, End_Month_90] = \"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "############### Column Alignments\n",
    "\n",
    "\n",
    "columns_to_move = ['Demand Stream','Region1','Area2','Region','Theatre','Area','Country Name','Country Code','Bill to Country','Plant','Publish flag','Zone','Customer LOB','Product LOB','Product Range','Parent Model','Class','Description','DTF','DTF cutoff','Start Date','End Date','DTF risk units','DTF risk','Organization Code','MCID','Partial config','Transit Month','KeyAccount_Billing','Key Account','Master Customer','Item Type','ASP','Data Series']\n",
    "\n",
    "remaining_columns = [col for col in IND_POR.columns if col not in columns_to_move]\n",
    "    \n",
    "IND_POR = IND_POR[columns_to_move + remaining_columns]\n",
    "\n",
    "######################################## DTF End Date updation #########################\n",
    "try:\n",
    "    IND_POR['End Date'] = pd.to_datetime(IND_POR['End Date'])\n",
    "except:\n",
    "    try:\n",
    "        IND_POR['End Date'] = pd.to_datetime(IND_POR['End Date'], format='%m/%d/%Y')\n",
    "    except:\n",
    "        try:\n",
    "            IND_POR['End Date'] = pd.to_datetime(IND_POR['End Date'], format='mixed')\n",
    "        except:\n",
    "            IND_POR['End Date'] = pd.to_datetime(IND_POR['End Date'], format='%m-%d-%Y')\n",
    "\n",
    "\n",
    "IND_POR['Month Adjustment'] = IND_POR['End Date'].dt.strftime('%b-%y')\n",
    "\n",
    "\n",
    "for i, x in enumerate(IND_POR['Month Adjustment']):\n",
    "    if pd.isnull(x) or x not in IND_POR.columns:  # Check if x is nan or not in columns\n",
    "        continue  # Skip this iteration if x is nan or not a column name\n",
    "    Index_month = IND_POR.columns.get_loc(x)\n",
    "    IND_POR.iloc[i, 34:Index_month+1] = \"\"\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#print(IND_POR['Month Adjustment'])\n",
    "\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Month Adjustment$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Comments$)')\n",
    "\n",
    "IND_POR.to_csv(Path+'\\Output\\IND_POR_W.csv',index=False)\n",
    "\n",
    "print('\\n')\n",
    "print('*************************************************\\n')\n",
    "print('         IND POR With Changes Completed     ')\n",
    "print('\\n*************************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################### Missing Configuation ##############################\n",
    "POR = pd.read_csv(Path+'\\Output\\IND_POR_WO.csv')\n",
    "\n",
    "\n",
    "###### Filter Publish flag / Partial config\n",
    "POR = POR[POR['Partial config'].str.contains('Yes',na=False) & POR['Publish flag'].str.contains('Y',na=False)]\n",
    "\n",
    "#POR.to_csv(Path+'\\Output\\Output.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Start_index = POR.columns.get_loc(month)\n",
    "End_index = Start_index + 9\n",
    "\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['Region', 'Class', 'Key Account', 'Country Code', 'Plant'] \n",
    "\n",
    "#POR = POR.groupby(columns_to_keep).agg({'Jul-24':'sum','Aug-24':'sum','Sep-24':'sum','Oct-24':'sum','Nov-24':'sum','Dec-24':'sum','Jan-25':'sum','Feb-25':'sum','Mar-25':'sum'})\n",
    "\n",
    "POR = POR.groupby(columns_to_keep).agg({POR.columns[Start_index]:'sum',POR.columns[Start_index+1]:'sum',POR.columns[Start_index+2]:'sum',POR.columns[Start_index+3]:'sum',POR.columns[Start_index+4]:'sum',POR.columns[Start_index+5]:'sum',POR.columns[Start_index+6]:'sum',POR.columns[Start_index+7]:'sum',POR.columns[Start_index+8]:'sum'})\n",
    "\n",
    "POR.to_csv(Path+'\\Output\\Missing Config.csv')\n",
    "\n",
    "POR = pd.read_csv(Path+'\\Output\\Missing Config.csv')\n",
    "\n",
    "# Sum the desired indices and store the result in a new column 'Total'\n",
    "POR['Total'] = POR.iloc[:, 5:14].sum(axis=1)\n",
    "\n",
    "POR = POR[POR['Total']>0]\n",
    "\n",
    "POR['CONC'] = POR['Class'].astype(str) +POR['Country Code']+POR['Key Account']\n",
    "\n",
    "####################### Load NFC\n",
    "NFC_Input = '\\\\Input\\\\NFC Tracker.xlsx'\n",
    "\n",
    "NFC = pd.read_excel(Path+NFC_Input,sheet_name='6M NFC')\n",
    "\n",
    "NFC = NFC[['CONC','Config Status']]\n",
    "\n",
    "NFC = NFC[NFC['Config Status'].str.contains('Completed',na=False)]\n",
    "\n",
    "Missing_Config = pd.merge(POR, NFC,on='CONC',how='left')\n",
    "\n",
    "Missing_Config = Missing_Config[Missing_Config['Config Status'].isnull()]\n",
    "\n",
    "Missing_Config = Missing_Config.filter(regex='^(?!.*CONC$)')\n",
    "Missing_Config = Missing_Config.filter(regex='^(?!.*Config Status$)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Missing_Config.loc[Missing_Config['Total']>20,'Status'] = 'Missing Config'\n",
    "Missing_Config.loc[Missing_Config['Total']<=20,'Status'] = 'Below 20'\n",
    "\n",
    "Missing_Config = Missing_Config.sort_values(by='Total',ascending=False)\n",
    "\n",
    "\n",
    "Missing_Config.to_csv(Path+'\\Output\\Missing Config.csv',index=False)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('*************************************************\\n')\n",
    "print('         Missing Configuration Completed     ')\n",
    "print('\\n*************************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5819540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "################# NEW Demand\n",
    "###################################################################################\n",
    "\n",
    "IND_POR.rename(columns={'Region':'LOB Region'},inplace=True)\n",
    "IND_POR.rename(columns={'Area':'LOB Area'},inplace=True)\n",
    "IND_POR.rename(columns={'Region1':'Region'},inplace=True)\n",
    "IND_POR.rename(columns={'Area2':'Area'},inplace=True)\n",
    "\n",
    "IND_POR['CC-Cls+Ctry+KA'] = IND_POR['Class'].astype(str) + IND_POR['Country Code'] + IND_POR['Key Account']\n",
    "\n",
    "\n",
    "IND_POR = IND_POR[IND_POR['Publish flag'].str.contains('Y',na=False)]\n",
    "\n",
    "\n",
    "Start_index = IND_POR.columns.get_loc(month)\n",
    "\n",
    "\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['CC-Cls+Ctry+KA','Region','Area','LOB Region','LOB Area','Country Name','Country Code','Plant','Publish flag','Zone','Customer LOB','Product LOB','Product Range','Parent Model','Class','MCID','Partial config','Key Account','Transit Month'] \n",
    "\n",
    "\n",
    "\n",
    "# Replace blank values with the keyword \"blanks\"\n",
    "IND_POR[columns_to_keep] = IND_POR[columns_to_keep].replace('', 'dummy')\n",
    "IND_POR[columns_to_keep] = IND_POR[columns_to_keep].fillna(\"dummy\")\n",
    "\n",
    "# Define the columns to sum\n",
    "columns_to_sum = IND_POR.columns[Start_index:]\n",
    "\n",
    "# Create a dictionary for aggregation\n",
    "agg_dict = {col: 'sum' for col in columns_to_sum}\n",
    "\n",
    "# Group by the columns to keep and aggregate the columns to sum\n",
    "IND_POR = IND_POR.groupby(columns_to_keep).agg(agg_dict)\n",
    "\n",
    "# Save the result to a csv file\n",
    "IND_POR.to_csv(Path+'Output\\\\New Demand.csv')\n",
    "\n",
    "\n",
    "New_Demand = pd.read_csv(Path+'Output\\\\New Demand.csv')\n",
    "\n",
    "New_Demand[columns_to_keep] = New_Demand[columns_to_keep].replace('dummy', '')\n",
    "\n",
    "# Get the column names for the specified slice\n",
    "column_names = New_Demand.columns[19:26]\n",
    "\n",
    "# Fill NaN values with 0 before converting to integer\n",
    "for column in column_names:\n",
    "    #print(column)\n",
    "    New_Demand[column] = New_Demand[column].fillna(0).astype(int)\n",
    "\n",
    "# Calculate the sum and store it in the new column '7M'\n",
    "New_Demand['7M'] = New_Demand[column_names].sum(axis=1)\n",
    "\n",
    "\n",
    "# Get the column names for the specified slice\n",
    "column_names = New_Demand.columns[19:28]\n",
    "\n",
    "# Fill NaN values with 0 before converting to integer\n",
    "for column in column_names:\n",
    "    New_Demand[column] = New_Demand[column].fillna(0).astype(int)\n",
    "\n",
    "# Calculate the sum and store it in the new column '7M'\n",
    "New_Demand['9M'] = New_Demand[column_names].sum(axis=1)\n",
    "\n",
    "\n",
    "# Get the column names for the specified slice\n",
    "column_names = New_Demand.columns[19:31]\n",
    "\n",
    "# Fill NaN values with 0 before converting to integer\n",
    "for column in column_names:\n",
    "    New_Demand[column] = New_Demand[column].fillna(0).astype(int)\n",
    "\n",
    "# Calculate the sum and store it in the new column '7M'\n",
    "New_Demand['12M'] = New_Demand[column_names].sum(axis=1)\n",
    "\n",
    "# Save the result to a csv file\n",
    "New_Demand.to_csv(Path+'Output\\\\New Demand.csv',index=False)\n",
    "    \n",
    "print('\\n')\n",
    "print('*************************************************\\n')\n",
    "print('         New Demand Completed     ')\n",
    "print('\\n*************************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c670a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "################################# EDW ########################################\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "Azure = pd.read_csv(Path+'\\Output\\Azure.csv')\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "mask = Azure['ISO Country Code'].isnull()\n",
    "\n",
    "Azure.loc[mask,'ISO Country Code'] = 'NA'\n",
    "\n",
    "#print(Mix_Check_List.columns)\n",
    "  \n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class','Country Code'],keep='first')\n",
    "\n",
    "\n",
    "Azure = pd.merge(Azure, Mix_Check_List[['Country Code','Class','Plant']],left_on=['MCID Class','ISO Country Code'],right_on=['Class','Country Code'],how='left')\n",
    "\n",
    "\n",
    "#####################\n",
    "Mix_Check_List = pd.read_excel(Path+'input\\Mix Check List.xlsx',sheet_name='Publish Flag',keep_default_na=False)\n",
    "\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['As per Sourcing Matrix','Country Code']]\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Country Code'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "Azure = pd.merge(Azure, Mix_Check_List,left_on = 'ISO Country Code',right_on='Country Code',how='left')\n",
    "\n",
    "#Azure.to_csv(Path+'Azure1.csv',index=False)\n",
    "\n",
    "####################\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Country Code','Class'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "#Azure.to_csv(Path+'Azure1.csv',index=False)\n",
    "\n",
    "Azure = pd.merge(Azure, Mix_Check_List[['Country Code','Class','Plant']],left_on=['Class','As per Sourcing Matrix'],right_on=['Class','Country Code'],how='left')\n",
    "\n",
    "#Azure.to_csv(Path+'Azure2.csv',index=False)\n",
    "\n",
    "\n",
    "mask = (Azure['Plant_x'].isnull() & Azure['Plant_y'].notnull())\n",
    "\n",
    "if mask.any():\n",
    "    Azure.loc[mask, 'Plant_x'] = Azure.loc[mask, 'Plant_y']\n",
    "\n",
    "Azure = Azure.rename(columns=lambda x: x.replace('Country Code_x', 'Country Code'))\n",
    "Azure = Azure.rename(columns=lambda x: x.replace('Plant_x', 'Plant'))\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*Country Code_y$)')\n",
    "Azure = Azure.filter(regex='^(?!.*Plant_y$)')\n",
    "\n",
    "\n",
    "##################### Publish Flag\n",
    "\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Publish Flag')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['Class','Publish flag']]\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class'],keep='first')\n",
    "\n",
    "Azure = pd.merge(Azure, Mix_Check_List,left_on='MCID Class',right_on='Class',how='left')\n",
    "\n",
    "Azure.rename(columns={'ISO Country Code':'ISO_CC'},inplace=True)\n",
    "\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*As per Sourcing Matrix$)')\n",
    "Azure = Azure.filter(regex='^(?!.*Country Code$)')\n",
    "Azure = Azure.filter(regex='^(?!.*Class_y$)')\n",
    "\n",
    "Azure.rename(columns={'ISO_CC':'ISO Country Code'},inplace=True)\n",
    "\n",
    "#print(Azure.columns)\n",
    "\n",
    "############\n",
    "Azure['CC-Cls+Ctry+KA'] = Azure['MCID Class'].astype(str) + Azure['ISO Country Code'] +Azure['KEY_ACCOUNT']\n",
    "\n",
    "Azure.rename(columns={'KEY_ACCOUNT':'Key Account'},inplace=True)\n",
    "Azure.rename(columns={'KEY_ACCOUNT':'Pblish Flag'},inplace=True)\n",
    "Azure.rename(columns={'Data Series':'Data series'},inplace=True)\n",
    "Azure.rename(columns={'6M Ship':'6M SHP'},inplace=True)\n",
    "Azure.rename(columns={'KEY_ACCOUNT':'Key Account'},inplace=True)\n",
    "Azure.rename(columns={'Plant':'Correct Org'},inplace=True)\n",
    "Azure.rename(columns={'Publish flag':'Pblish Flag'},inplace=True)\n",
    "\n",
    "Azure[['CC-Cls+Ctry+KA','ISO Country Code','Correct Org','Order Number','Order Type Name','Line Number','Shipment Number','Master Customer Number','Master Customer Name','Key Account','Pblish Flag','Offering Accounting Type Code','MCID Class','Product ID','Product Description','Order Booked Date','Line Request Date Time','Scheduled Ship Date','Scheduled Arrival Date','Actual Ship Date','Line Status','Data series','6M SHP','6M Order','Net Quantity','Warehouse Name','Organization Code','PID type','MCID-Net Order Value-US','MCID- MCC-US','Purchase Order Number','Invoice Trigger','Functional Group Short Name']].to_csv(Path+'\\Output\\EDW.csv',index=False)\n",
    "\n",
    "print('\\n')\n",
    "print('*************************************************\\n')\n",
    "print('         EDW Completed     ')\n",
    "print('\\n*************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492dd78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f7dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8347e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d260813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab44330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1a0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
