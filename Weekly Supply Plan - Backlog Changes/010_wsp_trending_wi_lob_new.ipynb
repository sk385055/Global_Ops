{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A script for generating data for input to Material Drive Trending\n",
    "\n",
    "***\n",
    "#### Process steps\n",
    "***\n",
    "<ol>\n",
    "    <li>Search for current file</li>\n",
    "    <li>Load WSP CSV data</li>\n",
    "    <li>GROUP BY subset of columns to compress data</li>\n",
    "    <li>Perform data wrangling</li>\n",
    "    <li>Map Product info, lob and make/buy</li>\n",
    "    <li>Check no nulls on make/buy</li>\n",
    "            * if no nulls:: Write to file <br>\n",
    "            * if nulls___:: Print table of nulls to screen <br>\n",
    "</ol>\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions and Classes\n",
    "\n",
    "\n",
    "Functions:\n",
    "    search_wsps:                --> get list of wsp candidate filenames\n",
    "    get_wsp_filenames:          --> generate list of wsp filenames\n",
    "    create_dataframes:          --> generate list of wsp dataframes\n",
    "    generate_uid:               --> generates a uid based on source, class and unit\n",
    "    format_wsp_for_grouping:    --> format dataframe for grouping (remove nulls)    \n",
    "    group_wsp:                  --> groups the wsp by subset of columns\n",
    "    write_final_data_to_csv:    --> outputs final dataframe to file\n",
    "    map_products_and_makebuy:   --> maps products fields, make/buy and LOB\n",
    "\n",
    "\n",
    "Classes:\n",
    "    WSPFields:                  --> key fields information for grouping, aggregating and formatting\n",
    "\n",
    "\"\"\"\n",
    "# functions\n",
    "def search_wsps(search_criteria: str = None) -> list:\n",
    "    \"\"\" get list of wsp candidate filenames \"\"\"\n",
    "    path = r'.\\wsp_csv'\n",
    "    if search_criteria:\n",
    "        files = glob.glob(os.path.join(path, search_criteria))\n",
    "        files\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_wsp_filenames(wsp_files: list = None) -> list:\n",
    "    \"\"\" generate list of wsp filenames \"\"\"\n",
    "    pattern = re.compile('Wk\\d+')\n",
    "    wsp_files_to_keep = []\n",
    "    if wsp_files:\n",
    "        print(\"Getting filenames...\")\n",
    "        for i, wsp in enumerate(wsp_files):\n",
    "            wsp_found = re.search(pattern, wsp)\n",
    "            if wsp_found:\n",
    "                wsp_files_to_keep.append(wsp)\n",
    "                file_name = wsp.split('\\\\')[-1]\n",
    "                print(i, wsp_found.group(0), \": \", file_name)\n",
    "                print(f\"{file_name} added.\")\n",
    "    return wsp_files_to_keep\n",
    "\n",
    "def create_dataframes(wsp_files: list = None) -> list:\n",
    "    \"\"\" generate list of wsp dataframes \"\"\"\n",
    "    pattern = re.compile('Wk\\d+')\n",
    "    wsp_dataframes = []\n",
    "    if wsp_files:\n",
    "        print(\"Compiling dataframes...\")\n",
    "        for wsp in wsp_files:\n",
    "            wsp_data = pd.read_csv(wsp, thousands=',', low_memory=False)\n",
    "            wsp_data.columns = [c.lower().strip() for c in wsp_data.columns]\n",
    "            wsp_data['fileref'] = re.search(pattern, wsp).group(0) + os.path.splitext(wsp)[0][-2:]\n",
    "            wsp_dataframes.append(wsp_data)\n",
    "            print(f\"{wsp} added.\")\n",
    "        return wsp_dataframes\n",
    "\n",
    "def generate_uid(wsp: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\" generates a uid based on source, class and unit \"\"\"\n",
    "    if not isinstance(wsp, type(None)):\n",
    "        print(\"Generating uid...\")\n",
    "        cols = ['source', 'class', 'unit']\n",
    "        wsp[cols] = wsp[cols].astype(str)\n",
    "        wsp['uid'] = wsp[cols].apply(lambda x: x.str.strip()).apply(lambda x: '-'.join(x), axis=1)\n",
    "    return wsp\n",
    "\n",
    "class WSPFields:\n",
    "    \"\"\" key fields information for grouping, aggregating and formatting \"\"\"\n",
    "    agg_key = {\n",
    "        'quantity': 'sum',\n",
    "        'total rev': 'sum',\n",
    "        'total mcc': 'sum',\n",
    "        'std cost': 'sum'\n",
    "    }\n",
    "\n",
    "    group_key = [\n",
    "        'fileref',\n",
    "        'region',\n",
    "        'source',\n",
    "        'order type',\n",
    "        'order category',\n",
    "        'order category2',\n",
    "        'order category3',\n",
    "        'order category4',\n",
    "#         'Master Customer Name',\n",
    "        'offer pf',\n",
    "        'prod grp',\n",
    "        'class',\n",
    "        'item type',\n",
    "        'unit',\n",
    "        'ssd_qtr',\n",
    "        'ssd mth',\n",
    "        'prq (final)',\n",
    "        'excl',\n",
    "        'excl2',\n",
    "        'lob',\n",
    "        'lob_'\n",
    "    ]\n",
    "\n",
    "    table_key = [\n",
    "        'fileref',\n",
    "        'region',\n",
    "        'source',\n",
    "        'order type',\n",
    "        'order category',\n",
    "        'order category2',\n",
    "        'order category3',\n",
    "        'order category4',\n",
    "#         'Master Customer Name',\n",
    "        'offer pf',\n",
    "        'prod grp',\n",
    "        'class',\n",
    "        'item type',\n",
    "        'unit',\n",
    "        'ssd_qtr',\n",
    "        'ssd mth',\n",
    "        'prq (final)',\n",
    "        'excl',\n",
    "        'excl2',\n",
    "        'lob',\n",
    "        'lob_',\n",
    "        'quantity',\n",
    "        'total rev',\n",
    "        'total mcc',\n",
    "        'std cost'\n",
    "    ]\n",
    "\n",
    "    table_key_final = [\n",
    "        'fileref',\n",
    "        'region',\n",
    "        'source',\n",
    "        'order type',\n",
    "        'order category',\n",
    "        'order category2',\n",
    "        'order category3',\n",
    "        'order category4',\n",
    "#         'Master Customer Name',\n",
    "        'offer pf',\n",
    "        'prod grp',\n",
    "        'products qty',\n",
    "        'products cost',\n",
    "        'class',\n",
    "        'item type',\n",
    "        'unit',\n",
    "        'ssd_qtr',\n",
    "        'ssd mth',\n",
    "        'prq (final)',\n",
    "        'excl',\n",
    "        'excl2',\n",
    "        'lob',\n",
    "        'lob_',\n",
    "        'quantity',\n",
    "        'total rev',\n",
    "        'total mcc',\n",
    "        'std cost',\n",
    "        'make/buy'\n",
    "    ]\n",
    "\n",
    "def format_wsp_for_grouping(wsp: pd.DataFrame = None, wsp_fields: object = None) -> pd.DataFrame:\n",
    "    \"\"\" format dataframe for grouping (remove nulls) \"\"\"\n",
    "    group_key = wsp_fields.group_key\n",
    "    value_key = ['quantity', 'total rev', 'total mcc','std cost']\n",
    "    if not isinstance(wsp, type(None)):\n",
    "        print(\"Munging WSP for grouping...\")\n",
    "        wsp[group_key] = wsp[group_key].apply(lambda x: x.fillna('na'))\n",
    "        wsp[value_key] = wsp[value_key].apply(lambda x: pd.to_numeric(x, errors='coerce').fillna(0))\n",
    "    return wsp\n",
    "\n",
    "def group_wsp(wsp: pd.DataFrame = None, wsp_fields: object = None) -> pd.DataFrame:\n",
    "    \"\"\" group wsp \"\"\"\n",
    "    if not isinstance(wsp, type(None)):\n",
    "        print(\"Grouping WSP...\")\n",
    "        group_key = wsp_fields.group_key\n",
    "        table_key = wsp_fields.table_key\n",
    "        agg_key = wsp_fields.agg_key\n",
    "        wsp_grouped = wsp[table_key].groupby(group_key).agg(agg_key).reset_index()\n",
    "    return wsp_grouped\n",
    "\n",
    "def write_final_data_to_csv(wsp: pd.DataFrame = None, qtr_start:str=None, curr_wk:bool=False) -> None:\n",
    "    \"\"\" outputs final dataframe to file \"\"\"\n",
    "    if not isinstance(wsp, type(None)):\n",
    "        # ensure there are no leading/trailing spaces on the below columns\n",
    "        cols = ['excl', 'excl2', 'lob']\n",
    "        wsp[cols] = wsp[cols].apply(lambda x: x.str.strip())\n",
    "        # set all values in excl, excl2 to 'show'\n",
    "        wsp.loc[:,'excl':'excl2'] = 'Show'\n",
    "\n",
    "        # add new columns for Qtr Start, Duplicate Make/Buy and WkPeriod\n",
    "        wsp.loc[:,'make-buy'] = wsp['make/buy']\n",
    "        wsp.loc[:,'QtrPeriod'] = '' if not qtr_start else qtr_start\n",
    "        wsp.loc[:,'WkPeriod'] = 'CurrWk' if curr_wk else ''\n",
    "\n",
    "        # set file date\n",
    "        file_date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "        filename = f'material_drive_trending_weekly_data_{file_date}.csv'\n",
    "        print(f\"Writing WSP data to --> {filename}...\")\n",
    "        # write to csv\n",
    "        wsp.to_csv(f'.\\\\mdt\\\\{filename}', index=False)\n",
    "\n",
    "def map_products_and_makebuy(wsp: pd.DataFrame = None, wsp_fields: object = None) -> pd.DataFrame:\n",
    "    \"\"\" maps products fields, make/buy and LOB \"\"\"\n",
    "    if not isinstance(wsp, type(None)):\n",
    "        # load pmap table\n",
    "        print(f\"Merging data with pmap...\")\n",
    "        product_mapping_table = pd.read_csv('./product_range_map_for_material_trending.csv', thousands=',', low_memory=False)\n",
    "        # tidy up wsp data\n",
    "        wsp['uid'] = wsp['uid'].str.lower()\n",
    "        wsp.drop(columns='lob', inplace=True)\n",
    "        # tidy up pmap data\n",
    "        product_mapping_table['uid'] = product_mapping_table['uid'].str.lower()\n",
    "        pmap= product_mapping_table[['uid', 'products qty', 'products cost', 'lob', 'make/buy']]\n",
    "        # merge data\n",
    "        print(pmap.iloc[:10,:])\n",
    "        wsp_map = pd.merge(wsp, pmap, how='left')\n",
    "        return wsp_map[wsp_fields.table_key_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify search string\n",
    "wsp_search_string = \"2025*Wk51*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting filenames...\n",
      "0 Wk51 :  2025_12_16 Wk51 NCR Atleos Weekly Supply Plan.csv\n",
      "2025_12_16 Wk51 NCR Atleos Weekly Supply Plan.csv added.\n",
      "Compiling dataframes...\n",
      ".\\wsp_csv\\2025_12_16 Wk51 NCR Atleos Weekly Supply Plan.csv added.\n",
      "Munging WSP for grouping...\n",
      "Grouping WSP...\n",
      "Generating uid...\n",
      "Merging data with pmap...\n",
      "                 uid products qty products cost       lob make/buy\n",
      "0      bri-1924-unit        Other      Commerce  Commerce     make\n",
      "1      bri-1612-unit        Other      Commerce  Commerce     make\n",
      "2      enn-7607-unit          POS           POS  Commerce     make\n",
      "3      enn-7772-unit          POS           POS  Commerce     make\n",
      "4      enn-7702-unit          POS           POS  Commerce     make\n",
      "5      enn-7879-unit     Scanners      Scanners  Commerce     make\n",
      "6      bri-7736-unit          POS           POS  Commerce      buy\n",
      "7  bri-7772-non-unit          POS           POS  Commerce     make\n",
      "8  bri-5915-non-unit        Other      Commerce  Commerce      buy\n",
      "9      bri-5915-unit        Other      Commerce  Commerce      buy\n",
      "Writing WSP data to --> material_drive_trending_weekly_data_20251216.csv...\n",
      "Values missing make/buy...\n",
      "      source class      unit\n",
      "64       CHE  6658      Unit\n",
      "72       CHE  6770      Unit\n",
      "82       CHE  2064  Non Unit\n",
      "1481     ENN  6623  Non Unit\n",
      "1484     ENN  6634  Non Unit\n",
      "...      ...   ...       ...\n",
      "22944    JAB  4450  Non Unit\n",
      "22977    JAB  5187  Non Unit\n",
      "22979    JAB  5188  Non Unit\n",
      "22980    JAB  5189  Non Unit\n",
      "22989    JAB  6035  Non Unit\n",
      "\n",
      "[87 rows x 3 columns]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main script runs functions and generates the final data\n",
    "for material drive trending file\n",
    "\n",
    "Steps\n",
    "    1. search for wsp files\n",
    "    2. concatentate wsp files into a single dataframe\n",
    "    3. format wsps and group by subset of columns\n",
    "    4. generate a uid and merge with product maps table\n",
    "    5. test for nulls from step.4 \n",
    "        5.1 print to screen if nulls, else\n",
    "        5.2 write final output to file\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# load wsp and perform initial munging\n",
    "wsps = create_dataframes(get_wsp_filenames(search_wsps(wsp_search_string)))\n",
    "wsps\n",
    "\n",
    "wsp_data = pd.concat(wsps)\n",
    "wsp_fields = WSPFields()\n",
    "\n",
    "wsp_data = format_wsp_for_grouping(wsp_data, wsp_fields)\n",
    "\n",
    "wsp_data_g = group_wsp(wsp_data, wsp_fields)\n",
    "# print(wsp_data_g)\n",
    "# input('Break....')\n",
    "wsp_data_g = generate_uid(wsp_data_g)\n",
    "wsp_data_g_map = map_products_and_makebuy(wsp_data_g, wsp_fields)\n",
    "\n",
    "# finally check if there are any null entries from mapping...\n",
    "ignore_missing_rows = True # set this to true if the missing rows can be ignored\n",
    "\n",
    "# specify filt and col for checking missing rows\n",
    "filt = wsp_data_g_map['make/buy'].isna()\n",
    "cols = ['source', 'class', 'unit']\n",
    "if all([wsp_data_g_map['make/buy'].isna().sum() > 0, not ignore_missing_rows]):\n",
    "    print(\"Values missing make/buy...\")\n",
    "    print(wsp_data_g_map[filt][cols].drop_duplicates())\n",
    "    wsp_data_g_map[filt][cols].drop_duplicates().to_csv('./uid_missing_mapping.csv')\n",
    "    print(\"Done - check the nulls, update the pmap file and try again!\")\n",
    "else:\n",
    "    write_final_data_to_csv(wsp_data_g_map)\n",
    "    print(\"Values missing make/buy...\")\n",
    "    print(wsp_data_g_map[filt][cols].drop_duplicates())\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRQ Calculation\n",
    "\n",
    "<p>Need to create functions for the below and tidy up for production</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate the PRQ proport splits for each SSD_Qtr\n",
    "\n",
    "Steps\n",
    "    1. calculate the proport splits for prqs in ssd_qtr\n",
    "    2. if there are null proports fill them with 1\n",
    "    3. perform the join onto the fcst kin with the fcst kin with prq splits\n",
    "    4. weight the value columns by the prq splits\n",
    "    5. perform check to before and after values\n",
    "    6. update the prq final column with the prq final values from the splits\n",
    "    7. replace the original fcst kin values with the weighted fcst kin values\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ssd_prqs_col = ['fileref', 'ssd_qtr', 'prod grp', 'prq (final)', 'lob_']\n",
    "ssd_only_col = ['fileref', 'ssd_qtr', 'prod grp']\n",
    "\n",
    "#fcst_dem = wsp_data_g_map[wsp_data_g_map['order category4'] == 'Fcst-DEM'] #jose commented\n",
    "\n",
    "#left = pd.pivot_table(fcst_dem, index=ssd_prqs_col,values='total rev', aggfunc='sum', fill_value=0).reset_index()\n",
    "#right = pd.pivot_table(fcst_dem, index=ssd_only_col,values='total rev', aggfunc='sum', fill_value=0).reset_index()\n",
    "\n",
    "#prq_table = left.merge(right, how='left', on=['fileref', 'ssd_qtr', 'prod grp'], validate='m:1')\n",
    "#prq_table['proport'] = prq_table['total rev_x'] / prq_table['total rev_y']\n",
    "#print(\"==\" * 30)\n",
    "#print(\"Check if any proports fall outside acceptable limits\")\n",
    "#print(\"--\" * 30)\n",
    "#proport_check = prq_table[(prq_table['proport'] < 0) | (prq_table['proport'] > 1)]\n",
    "#if proport_check.empty:\n",
    "    #print(\"No proports outside acceptable limits\")\n",
    "    #print(\"Test Pass\")\n",
    "    #print(\"--\" * 30)\n",
    "#else:\n",
    "    #print(\"Test Fail\")\n",
    "    #print(\"--\" * 30)\n",
    "    #print(proport_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.merge() missing 1 required positional argument: 'right'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m fcst_kin \u001b[38;5;241m=\u001b[39m wsp_data_g_map[wsp_data_g_map[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder category4\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFcst-KIN\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# generate forecast splits and fill blanks\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#fcst_kin_split = fcst_kin.merge(prq_table, how='left', on=['fileref', 'ssd_qtr', 'prod grp'])\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m fcst_kin_split \u001b[38;5;241m=\u001b[39m fcst_kin\u001b[38;5;241m.\u001b[39mmerge(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfileref\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mssd_qtr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprod grp\u001b[39m\u001b[38;5;124m'\u001b[39m], validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm:1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m fcst_kin_split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproport\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m fcst_kin_split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproport\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# value_columns\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: DataFrame.merge() missing 1 required positional argument: 'right'"
     ]
    }
   ],
   "source": [
    "# filter on forecast kinaxis\n",
    "fcst_kin = wsp_data_g_map[wsp_data_g_map['order category4'] == 'Fcst-KIN']\n",
    "\n",
    "# generate forecast splits and fill blanks\n",
    "#fcst_kin_split = fcst_kin.merge(prq_table, how='left', on=['fileref', 'ssd_qtr', 'prod grp'])\n",
    "fcst_kin_split = fcst_kin.merge(how='left', on=['fileref', 'ssd_qtr', 'prod grp'], validate='m:1')\n",
    "fcst_kin_split['proport'] = fcst_kin_split['proport'].fillna(1)\n",
    "\n",
    "# value_columns\n",
    "measures = ['quantity', 'total rev', 'total mcc', 'std cost']\n",
    "\n",
    "# generate a test between the original unadjusted values and the adjusted values\n",
    "check_original_vals__ = fcst_kin[measures].apply(lambda x: x / 1_000_000).sum().round(2)\n",
    "check_post_split_vals = fcst_kin_split[measures].multiply(fcst_kin_split['proport'], axis='index').apply(lambda x: x / 1_000_000).sum().round(2)\n",
    "\n",
    "# check results\n",
    "print('If all values are true then pass else fail')\n",
    "if all((check_original_vals__ == check_post_split_vals).to_list()):\n",
    "    print('Test pass')\n",
    "    generate_prq_splits = True\n",
    "    fcst_kin_split[measures] = fcst_kin_split[measures].multiply(fcst_kin_split['proport'], axis='index')\n",
    "else:\n",
    "    generate_prq_splits = False\n",
    "    print('Test fail')\n",
    "print('--'*30)\n",
    "\n",
    "print(\"Original Vals:\", tuple(check_original_vals__))\n",
    "print(\"Post-PRQ Vals:\", tuple(check_post_split_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the prq split calculation results in an accurate adjustment then \n",
    "# create the final output with the prq splits\n",
    "if generate_prq_splits:\n",
    "    # generate a dictionary of allowable prq values for ssd qtr\n",
    "    ssd_qtr = {}\n",
    "    for i in range(2018, 2030):\n",
    "        for j in range(1, 5):\n",
    "            prq_val = []\n",
    "            ssd_key = str(i) + \"Q\" + str(j)\n",
    "            if j == 4:\n",
    "                prq_val.append(str(i) + \"Q\" + str(j))\n",
    "                prq_val.append(str(i+1) + \"Q1\")\n",
    "            else:\n",
    "                prq_val.append(str(i) + \"Q\" + str(j))\n",
    "                prq_val.append(str(i) + \"Q\" + str(j+1))                \n",
    "            ssd_qtr[ssd_key] = prq_val\n",
    "\n",
    "    # filter wsp to remove fcst kin and then combine fcst kin with splits\n",
    "    wsp_wo_fcst_kin = wsp_data_g_map[wsp_data_g_map['order category4'] != 'Fcst-KIN']\n",
    "\n",
    "    # get the columns from the wsp view and rename the prq final column\n",
    "    standard_columns = wsp_wo_fcst_kin.columns\n",
    "#     print(standard_columns)\n",
    "#     print(fcst_kin_split.columns)\n",
    "    fcst_kin_split.rename(columns={'prq (final)_y': 'prq (final)', 'lob__y': 'lob_'}, inplace=True)\n",
    "\n",
    "    # generate final output with prq splits\n",
    "    final_output = pd.concat([wsp_wo_fcst_kin, fcst_kin_split[standard_columns]])\n",
    "\n",
    "    # generate the list of prq values as either prq eg 2021Q1, 2021Q2, ... or Future\n",
    "    prq_final2 = []\n",
    "    for ssd, prq in final_output[['ssd_qtr', 'prq (final)']].itertuples(index=False, name=None):\n",
    "        try:\n",
    "            if prq in ssd_qtr.get(ssd):\n",
    "                prq_final2.append(prq)\n",
    "            else:\n",
    "                prq_final2.append('Future')\n",
    "        except TypeError:\n",
    "            prq_final2.append('Future')\n",
    "    final_output['prq_final'] = prq_final2\n",
    "\n",
    "    # write final output\n",
    "    write_final_data_to_csv(final_output)\n",
    "else:\n",
    "    print('Writing WSP data to --> Test fail: Output will not be written')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsps[0][['order category3']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filts = (wsps[0]['order category3'].isin(['Completions', 'Backlog', 'PBO', 'Planned Order'])) & (wsps[0]['ssd_qtr'].isin(['2021Q2', '2021Q3']) & (wsps[0]['lob_'].isna()))\n",
    "wsps[0][['order category3', 'item','master customer name', 'lob_']][filts].drop_duplicates().to_csv('./missing_lob_item.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in wsps[0].columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "b0ed4e3fcb08c1c9e9775c8876a1f83c515b1ef3a6e87a3f580c0fa033eb8f32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
