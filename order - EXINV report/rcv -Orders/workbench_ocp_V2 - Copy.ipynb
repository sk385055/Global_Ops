{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbab8e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pandas.tseries.offsets import BDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41aea908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCP_IA_EUMOCP_29-FEB-2024.xlsx\n",
      "CPU times: total: 20.4 s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read OCP IA: if India Time on or before 3PM, then read EUMOCP else AMMOCP\n",
    "# This needs to be changed - script should run as soon the new file is available\n",
    "import pytz\n",
    "from datetime import datetime, timedelta,date\n",
    "d = datetime.now(pytz.timezone('Asia/Calcutta'))\n",
    "if d.hour <= 15:\n",
    "    file = 'OCP_IA_EUMOCP'\n",
    "else:\n",
    "    file = 'OCP_IA_AMMOCP'\n",
    "    \n",
    "d = d.strftime('%d-%b-%Y').upper()\n",
    "\n",
    "#path = r'\\\\susday014.prod.local\\cdunix\\ERP\\RR\\Inbound\\OCP\\IA\\rcv'\n",
    "path = r'E:\\_Projects\\orders\\rcv'\n",
    "# file = 'OCP_IA_EUMOCP'\n",
    "file = 'OCP_IA_EUMOCP'\n",
    "\n",
    "#New Script\n",
    "%run Orders.ipynb\n",
    "d = EUMOCP_File_Name\n",
    "\n",
    "#Old\n",
    "#d = '29-FEB-2024'\n",
    "\n",
    "data = pd.read_excel(path + '\\\\' + file + '_' + d + '.xlsx', sheet_name='Sheet1', engine='openpyxl', \\\n",
    "                     na_values='', keep_default_na=True)\n",
    "\n",
    "print(file + '_' + d + '.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff84a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bkp = data.copy()\n",
    "# data = data_bkp.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc709f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude ERP LINE STATUS = Closed (Updated dt 12-Jun-2023)\n",
    "data = data[data['ERP Line Status'] != 'Closed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55aa81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Product group and Offer PF; +2 cols\n",
    "prod = pd.read_csv(r'E:\\_Projects\\lookups\\class_offerpf_map.csv', \\\n",
    "                   encoding='utf-8', low_memory=False)\n",
    "data['Class'] = data['Class'].astype(str)\n",
    "data = pd.merge(data, prod[['class', 'prod_grp_wb','offer_pf_wb','division_wb']], left_on='Class', right_on='class', how='left',validate='m:1')\n",
    "data[['Products', 'Offer PF','LOB Prod']] = data[['prod_grp_wb', 'offer_pf_wb', 'division_wb']]\n",
    "data.drop(columns={'class', 'offer_pf_wb','prod_grp_wb','division_wb'}, axis=1, inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ff3d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update MCID TAGGING name from RACE; +1 col\n",
    "tag = pd.read_excel(r'E:\\_Projects\\lookups\\MCID_Tagging _Master.xlsx', sheet_name='MCID_Tagging__Master_TABLE' \\\n",
    "                   , engine='openpyxl')\n",
    "tag.drop_duplicates(inplace=True,subset=['MCID'],keep='first')\n",
    "data = pd.merge(data, tag[['MCID','Tag Name']], left_on='PID', right_on='MCID', how='left',validate='m:1')\n",
    "data.drop(columns={'MCID'}, axis=1, inplace=True)   \n",
    "data['Tag Name'] = np.where(data['Tag Name'].isnull(),data['PID Desc'],data['Tag Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f98430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pref offer\n",
    "pref_offer = pd.read_csv(r'E:\\_Projects\\lookups\\preferred_offer.csv', \\\n",
    "                    encoding='utf-8', low_memory=False)\n",
    "pref_offer_grpd = pref_offer.groupby('PID')['Type'].max()\n",
    "data = pd.merge(data, pref_offer_grpd, left_on='PID', right_on='PID', how='left',validate='m:1')\n",
    "data.rename(columns={'Type' : 'Preferred Offer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b30ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prod Cate: 7360 category as Std, Narrow Core, Jarvis; 2012 vs 2021\n",
    "# condtn?\n",
    "data['Prod Cate'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3838b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update DOM / EXPORT for chennai\n",
    "# Delete SS21, Dom/Export and start using Prod Cate itself\n",
    "# \n",
    "df_dom_exp = pd.read_csv(r'E:\\_Projects\\lookups\\bom_details.csv', \\\n",
    "                        encoding='utf-8', low_memory=False)\n",
    "df_dom_exp_grpd = df_dom_exp.groupby('mcid')[['Dom/Export']].max()\n",
    "data = pd.merge(data, df_dom_exp_grpd, left_on='PID', right_on='mcid', how='left',validate='m:1')\n",
    "\n",
    "data['Prod Cate'] = np.where( (data['Dom/Export'] != '') & (data['Whse'].isin(['CHE','CHS'])), \\\n",
    "                                    data['Class'] + data['Dom/Export'].str[:1], '')\n",
    "# data['Prod Cate'] = np.where(data['Prod Cate'] != '' & data['Dom/Export'] != '', \\\n",
    "#                              data['Prod Cate'] + ';' + data['Dom/Export'], \\\n",
    "#                              data['Dom/Export'])\n",
    "data.drop(columns={'Dom/Export'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4daaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on PID Desc like SelfServ*21* \n",
    "# based on TAG Name like *SS21*\n",
    "data['IsSS21']=''\n",
    "data['IsSS21'] = np.where((data['Class']=='2012') & (data['PID Desc'].str.contains('&'.join(['SelfServ','21']))==True), \\\n",
    "                          'SS21', '')\n",
    "data['IsSS21'] = np.where((data['Class']=='2012') & (data['Tag Name'].str.endswith('SS21')==True), 'SS21', data['IsSS21'])\n",
    "data['Prod Cate'] = np.where(data['Prod Cate'].notnull() & data['IsSS21'].notnull(), data['Prod Cate']+';'+data['IsSS21'], \\\n",
    "                            data['IsSS21'])\n",
    "data.drop(columns={'IsSS21'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c56ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PLANT field\n",
    "\n",
    "incl_type = ['PO','PD','Planned Buy', 'Planned Make','Planned order','Flow']\n",
    "pid_type = ['M','K','C']\n",
    "data['Plant'] = ''\n",
    "conditions = [ (data['Demand']!='WIP OSP') & data['Whse'].isin(['CHE','CHS']) , \\\n",
    "             (data['Demand']!='WIP OSP') &  \\\n",
    "                  data['Supplier'].isin(['JABIL CIRCUIT INC - CHIHUAHUA']) , \\\n",
    "             (data['Demand']!='WIP OSP') & data['Whse'].isin(['USL']) & \\\n",
    "                  ~data['Supplier'].isin(['UNIVERSAL SCIENTIFIC INDUSTRIAL DE MEXICO S.A. DE C.V']) , \\\n",
    "             (data['Demand']!='WIP OSP') & data['Whse'].isin(['US','USL','GF1'])  & \\\n",
    "                  data['Supplier'].isin(['UNIVERSAL SCIENTIFIC INDUSTRIAL DE MEXICO S.A. DE C.V']) , \\\n",
    "             (data['Demand']!='WIP OSP') & \\\n",
    "                  data['Supplier'].isin(['ENNOCONN HUNGARY KFT'])\n",
    "             ]\n",
    "choices = ['CHE','JAB','USL','USI','ENN']\n",
    "data['Plant'] = np.select(conditions, choices, default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf1521e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PO Number update for ENN, JAB, USI ************************************************************************\n",
    "lst_splr = ['JABIL CIRCUIT INC - CHIHUAHUA', 'UNIVERSAL SCIENTIFIC INDUSTRIAL DE MEXICO S.A. DE C.V', \\\n",
    "                'UNIVERSAL SCIENTIFIC INDUSTRIAL DE MEXICO S.A. DE C.V', \\\n",
    "                'ENNOCONN HUNGARY KFT']\n",
    "lst_type = ['PD','PO']\n",
    "\n",
    "condtn1 = data['Supplier'].isin(lst_splr)\n",
    "condtn3 = data['Supply Type'].isin(lst_type)\n",
    "\n",
    " # Trim Supply Detail (PO) for OCP-IA records\n",
    "data['Supply PO'] = np.where(condtn1 & condtn3, \\\n",
    "                                   data['Supply Detail'].str.split('(').str[0].copy(), '')\n",
    "\n",
    "# bring in IO/PO field\n",
    "data['IO/PO No.'] = np.where(data['Supply PO']!='', data['Supply PO'], data['IO No.'])\n",
    "\n",
    "data['PO No.'] = data['Supply PO']\n",
    "data.drop(columns={'Supply PO'}, inplace=True)\n",
    "\n",
    "# col = data.pop('IO/PO No.')\n",
    "# data.insert(33, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f935a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting DATE fields\n",
    "data['Booked Dt'] = pd.to_datetime(data['Booked Dt'], format='%Y-%m-%d')\n",
    "data['Request Dt'] = pd.to_datetime(data['Request Dt'], format='%Y-%m-%d', errors='coerce')\n",
    "data['PD_Order'] = pd.to_datetime(data['PD_Order'], format='%Y-%m-%d', errors='coerce')\n",
    "data['Ship Date'] = pd.to_datetime(data['Ship Date'], format='%Y-%m-%d')\n",
    "data['SAD'] = pd.to_datetime(data['SAD'], format='%Y-%m-%d')\n",
    "\n",
    "# data['IO BOOKED DATE'] = pd.to_datetime(data['IO BOOKED DATE'], format='%d-%m-%Y %I:%M:%S %p')\n",
    "data['IO BOOKED DATE'] = pd.to_datetime(data['IO BOOKED DATE'], format='%Y-%m-%d')\n",
    "data['IO BOOKED DATE'] = data['IO BOOKED DATE'].dt.date\n",
    "data['IO SSD'] = pd.to_datetime(data['IO SSD'], format='%Y-%m-%d')\n",
    "data['IO SAD'] = pd.to_datetime(data['IO SAD'], format='%Y-%m-%d')\n",
    "\n",
    "data['PD_PO'] = pd.to_datetime(data['PD_PO'], format='%Y-%m-%d')\n",
    "data['CRSD'] = pd.to_datetime(data['CRSD'], format='%Y-%m-%d')\n",
    "\n",
    "#data['PRD'] = pd.to_datetime(data['PRD'], format='mixed').dt.date()\n",
    "data['PRD'] = pd.to_datetime(data['PRD'], format='%Y-%m-%d')\n",
    "# data['PRD'] = pd.to_datetime(data['PRD'], format='%d-%m-%Y')\n",
    "data['Plant Ship date'] = pd.to_datetime(data['Plant Ship date'], format='%Y-%m-%d')\n",
    "data['NEED BY DATE'] = pd.to_datetime(data['NEED BY DATE'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51051264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #************************ Update IO/PO SSD *************************************\n",
    "data['IO/PO SSD'] = ''\n",
    "data['IO/PO SSD'] = np.where(data['Supplier'].isin(lst_splr) \\\n",
    "                             & data['PD_PO'].notnull() & ~data['Plant'].isin(['']), \\\n",
    "                             data['PD_PO'], data['IO/PO SSD'])\n",
    "\n",
    "data['IO/PO SSD'] = np.where(data['Supplier'].isin(lst_splr) \\\n",
    "                             & data['NEED BY DATE'].notnull() & data['PD_PO'].isnull() & ~data['Plant'].isin(['']), \\\n",
    "                             data['NEED BY DATE'], data['IO/PO SSD'])\n",
    "\n",
    "data['IO/PO SSD'] = np.where(data['Supplier'].isin(lst_splr) \\\n",
    "                             & data['IO SSD'].notnull() & data['PD_PO'].isnull() & data['NEED BY DATE'].isnull() & ~data['Plant'].isin(['']), \\\n",
    "                             data['IO SSD'], data['IO/PO SSD'])\n",
    "\n",
    "\n",
    "# #***** LESS transit days for JABIL: 3 days\n",
    "data['IO/PO SSD'] = np.where((data['Supplier']=='JABIL CIRCUIT INC - CHIHUAHUA') \\\n",
    "                             & ~data['IO/PO SSD'].isin(['']) &data['PD_PO'].notnull() & ~data['Plant'].isin(['']), \\\n",
    "                             pd.to_datetime(data['IO/PO SSD']) - BDay(3), data['IO/PO SSD'])\n",
    "\n",
    "#******** less Transit days for USI: 5 days\n",
    "data['IO/PO SSD'] = np.where(~data['IO/PO SSD'].isin(['']) &data['PD_PO'].notnull() & data['Plant'].isin(['USI']), \\\n",
    "                             pd.to_datetime(data['IO/PO SSD']) - BDay(5), data['IO/PO SSD'])\n",
    "\n",
    "\n",
    "\n",
    "# #*************** Update Plant SSD as IO SSD else Ship Date (for OTHER THAN jabil, ennoconn, usi)\n",
    "data['IO/PO SSD'] = np.where((~data['Supplier'].isin(lst_splr)) \\\n",
    "                             & data['IO SSD'].notnull() & ~data['Plant'].isin(['']), \\\n",
    "                             data['IO SSD'], data['IO/PO SSD'])\n",
    "\n",
    "data['IO/PO SSD'] = pd.to_datetime(data['IO/PO SSD'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bda3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #************************ Update Plant SSD *************************************\n",
    "cond = (data['IO/PO SSD'].isin(['']) | data['IO/PO SSD'].isnull()) & ~(data['Plant'].isin(['']))\n",
    "data['Plant SSD'] = np.where(cond,data['Ship Date'],data['IO/PO SSD'])\n",
    "data['Plant SSD'] = pd.to_datetime(data['Plant SSD'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b2f60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#func to popualte WK, MON, QTR\n",
    "def upd_period(df, fld, fld_wk, fld_mon, fld_qtr):\n",
    "    df[fld_wk] = pd.to_datetime(df[fld], errors='coerce').dt.isocalendar().week.astype(pd.Int64Dtype())\n",
    "    df[fld_mon] = pd.to_datetime(df[fld], errors='coerce').dt.strftime('%Y%m')\n",
    "    df[fld_qtr] = pd.to_datetime(df[fld], errors='coerce').dt.strftime('%Y') + 'Q' + \\\n",
    "                        + pd.to_datetime(data[fld], errors='coerce').dt.quarter.astype(pd.Int64Dtype()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92a3a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#align PAST dates into last week \n",
    "#Sri made changes on 07-Aug-23\n",
    "def align_past(fld):\n",
    "    data[fld] = data[fld].astype('datetime64[ns]')\n",
    "    pre_week = date.today() - timedelta(days=1)\n",
    "    #data['Previous_week_Date'] = pre_week- timedelta(days = pre_week.weekday())\n",
    "    data['Previous_week_Date'] = pre_week\n",
    "    data['Previous_week_Date'] = data['Previous_week_Date'].astype('datetime64[ns]')\n",
    "#     data['final'] =  data[fld]\n",
    "#     cond = data['final'] < datetime.today()\n",
    "    cond = data[fld] < pd.to_datetime('today')\n",
    "    data[fld] = np.where(cond, data['Previous_week_Date'], data[fld])\n",
    "    data[fld] = data[fld].astype('datetime64[ns]')\n",
    "    \n",
    "#align_past('Ship Date')\n",
    "upd_period(data, 'Ship Date', 'Ship Dt Wk', 'Ship Dt Mon', 'Ship Dt Qtr')\n",
    "#data.drop(columns={'Previous_week_Date'}, axis=1, inplace=True)\n",
    "\n",
    "#align_past('IO SSD')\n",
    "upd_period(data, 'IO SSD', 'IO SSD Wk', 'IO SSD Mon', 'IO SSD Qtr')\n",
    "#data.drop(columns={'Previous_week_Date'}, axis=1, inplace=True)\n",
    "\n",
    "align_past('IO/PO SSD')\n",
    "upd_period(data, 'IO/PO SSD', 'IO/PO SSD Wk', 'IO/PO SSD Mon', 'IO/PO SSD Qtr')\n",
    "data.drop(columns={'Previous_week_Date'}, axis=1, inplace=True)\n",
    "\n",
    "align_past('Plant SSD')\n",
    "upd_period(data, 'Plant SSD', 'Plant SSD Wk', 'Plant SSD Mon', 'Plant SSD Qtr')\n",
    "data.drop(columns={'Previous_week_Date'}, axis=1, inplace=True)\n",
    "           \n",
    "upd_period(data, 'PRD', 'PRD Wk', 'PRM', 'PRQ')\n",
    "data.drop(columns={'PRD Wk'}, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d538a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hari changed New PRD GRP formulae 28-06-2023\n",
    "def update_prd_group(fld):\n",
    "    cond = (data[fld]!= '') & (data[fld].astype(str) == data['PRQ'].astype(str))\n",
    "    cond1 =  (data[fld]!= '') & (data['PRQ'].astype(str) > data[fld].astype(str))\n",
    "    cond2 =  (data[fld]!= '') & (data['PRQ'].astype(str) < data[fld].astype(str))\n",
    "    data['PRD Grp'] = np.where(cond,'PRD Curr', np.where(cond1,'PRD Fut', np.where(cond2,'PRD Past','')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5af4c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PRD Grp']=''\n",
    "data['Plant SSD Qtr'] = data['Plant SSD Qtr'].replace(np.nan, '', regex=True)\n",
    "data['Ship Dt Qtr'] = data['Ship Dt Qtr'].replace(np.nan, '', regex=True)\n",
    "update_prd_group('Plant SSD Qtr')\n",
    "#update_prd_group('Ship Dt Qtr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe4880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1e0d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust = pd.read_excel(r'E:\\_Projects\\lookups\\7.3_Country-Key_Account_Heirarchy.xlsb', \\\n",
    "                     sheet_name='Report1', engine='pyxlsb')\n",
    "cust['Master Customer Number'] = cust['Master Customer Number'].astype(str)\n",
    "cust_grpd = cust.groupby(['Master Customer Number'])['Master Customer Name'].max()\n",
    "data = pd.merge(data, cust_grpd, left_on='Master Customer Number', right_on='Master Customer Number', how='left',validate='m:1')\n",
    "keyacc_grpd = cust.groupby(['Master Customer Number'])['Key Account'].max()\n",
    "data = pd.merge(data, keyacc_grpd, left_on='Master Customer Number', right_on='Master Customer Number', how='left',validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d926ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Junk chars in CUST; and 25 chars only\n",
    "#batch1\n",
    "data['junk'] = data['Customer_End Customer'].str.len() - \\\n",
    "    data['Customer_End Customer'].str.count(r'[a-zA-Z0-9.,:;\\'\\\"\\`~!@#$%^& -_-()/\\\\]')\n",
    "\n",
    "data['Customer'] = np.where((data['junk']  <= 3) & (data['Customer_End Customer'] != ''), \\\n",
    "                                 data['Customer_End Customer'], data['Master Customer Name'])\n",
    "\n",
    "data['Customer'] = np.where((data['Customer'] == 'OTHER') | (data['Customer'].isnull()), \\\n",
    "                               data['Org Code']+' CUSTOMER', data['Customer'])\n",
    "#batch2\n",
    "data['junk'] = data['Customer'].str.len() - \\\n",
    "    data['Customer'].str.count(r'[a-zA-Z0-9.,:;\\'\\\"\\`~!@#$%^& -_-()/\\\\]')\n",
    "data['Customer'] = np.where((data['junk']  > 3), \\\n",
    "                               data['Org Code']+' CUSTOMER', data['Customer'])\n",
    "\n",
    "data.drop(columns={'Master Customer Name','junk'}, inplace=True, axis=1)\n",
    "\n",
    "data['Customer'] = np.where(data['Customer'].str.len()>25, data['Customer'].str[:23] + '..', data['Customer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f82e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DEMAND VAL'] = data['DEMAND VAL'].astype('Int64')\n",
    "data['LN'] = data['Line Number'].astype(str).replace(r'\\.', '_', regex=True)\n",
    "cols_risk2 = ['Org Code','DEMAND VAL','LN','PID']\n",
    "data['id2'] = data[cols_risk2].astype(str).apply('_'.join, axis=1)\n",
    "\n",
    "risk = pd.read_csv(r'E:\\_Projects\\orders\\rcv\\\\risk_upside.csv', encoding='utf-8', low_memory=False)\n",
    "risk['order_nbr'] = risk['order_nbr'].astype (\"Int64\").astype(str)\n",
    "risk['order_line_nbr'] = risk['order_line_nbr'].astype(int).astype(str)\n",
    "risk['shipment_nbr'] = risk['shipment_nbr'].astype(int).astype(str)\n",
    "\n",
    "cols_risk1 = ['Country','order_nbr','order_line_nbr','shipment_nbr','item_hyp']\n",
    "risk['id'] = risk[cols_risk1].astype(str).apply('_'.join, axis=1)\n",
    "risk.drop_duplicates(inplace=True,subset=['id'],keep='first')\n",
    "data = pd.merge(data, risk[['id', 'RiskCode', 'RiskMonth', 'UpsideCode', 'UpsideMonth']], \\\n",
    "                left_on=['id2'], right_on=['id'], how='left',validate='m:1')\n",
    "data.drop(columns={'id','id2'}, axis=1, inplace=True) \n",
    "\n",
    "data.drop(columns={'LN'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abde81d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan '1970Q1']\n"
     ]
    }
   ],
   "source": [
    "#ramiya upside Qtr ---> upside Mthn\n",
    "upd_period(data, 'UpsideMonth', 'UpsideMonth Wk', 'UpsideMonth Mon', 'UpsideMonth Qtr')\n",
    "data.drop(columns={'UpsideMonth Wk', 'UpsideMonth Mon'}, axis=1, inplace=True)\n",
    "print(data['UpsideMonth Qtr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16220718",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas = pd.read_excel(r'E:\\_Projects\\lookups\\aas_logic.xlsx', sheet_name='logic', engine='openpyxl')\n",
    "\n",
    "data['As_a_Serv'] = ''\n",
    "for index, row in aas.iterrows():\n",
    "    data['As_a_Serv'] = np.where((data[row['Field']].str.startswith(row['Customer_AAS'])) & \\\n",
    "                                 (data['PID Type'] == row['PID Type']) & (data['Products'] == row['Prod']), \\\n",
    "                                 'As-a-Serv', data['As_a_Serv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79034215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNIQE ID and COMMENTS fields\n",
    "# data['UID'] = data['Whse'].astype(str) + data['SO Ship Org'].astype(str) + data['Org Code'].astype(str) + \\\n",
    "#             data['DEMAND VAL'].astype(str)  + data['Line Number'].astype(str) + data['PID'].astype(str) + \\\n",
    "#             data['IO/PO No.'].astype(str)\n",
    "data['Line Number'] = data['Line Number'].fillna('')\n",
    "data['Line Number'] = data['Line Number'].astype(str)\n",
    "data['UID'] = data['PID'].astype(str) +'|'+data['Org Code'].astype(str)+'|'+ data['DEMAND VAL'].astype(str) + '|'+data['Line Number'].astype(str)\n",
    "\n",
    "\n",
    "# col = data.pop('UID')\n",
    "# data.insert(0, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa6385d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:189: RuntimeWarning: invalid value encountered in cast\n",
      "  return values.astype(dtype, copy=copy)\n"
     ]
    }
   ],
   "source": [
    "# data['IO No.'] = data['IO No.'].astype(str)\n",
    "data['IO/PO No.'] = np.where(data['IO/PO No.'].isnull(),'0',data['IO/PO No.'])\n",
    "data['IO/PO No.'] = pd.to_numeric(data['IO/PO No.']).astype(int).astype(str)\n",
    "data['IO/PO No.'] = np.where(data['IO/PO No.']=='0','',data['IO/PO No.'])\n",
    "\n",
    "data['IID'] =''\n",
    "data['IID'] = np.where(data['IO/PO No.'].notnull(),data['PID']+'|'+data['IO/PO No.'].astype(str), '')\n",
    "# col = data.pop('IID')\n",
    "# data.insert(1, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03438190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concate address\n",
    "data['Ship To Cust'] = data['Ship To Customer Name'] + ', ' + data['Ship To Address1'] +', '+ data['Ship To Address2'] +', '+\\\n",
    "                       data['Ship To City'] + ', ' + data['Ship To State'] +', '+ data['Ship To Zip']\n",
    "data['Ship to Intmed'] = data['Intmed Ship To'] + ', ' + data['Intmed Ship To Address1'] +', '+ data['Intmed Ship To City'] +', '+\\\n",
    "                       data['Intmed Ship To State'] + ', ' + data['Intmed Ship To Zip'] \n",
    "#Drop the column\n",
    "data.drop(columns={'Ship To Customer Name','Ship To Address1','Ship To Address2','Ship To City','Ship To State','Ship To Zip','Intmed Ship To','Intmed Ship To Address1','Intmed Ship To City','Intmed Ship To State','Intmed Ship To Zip'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b11093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2776fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qty field - SupplyQty with OH, RS\n",
    "conditions = [data['Supply Qty'] > 0, data['OH Qty'] > 0, data['Res Qty'] > 0]\n",
    "choices = [data['Supply Qty'], data['OH Qty'], data['Res Qty']]\n",
    "data['Qty'] = np.select(conditions, choices)\n",
    "#Ramya\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60f70fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _SO_Status\n",
    "# SO SSD < Today()\tSO SSD Past\n",
    "# SO SAD >= (SO PD and SO PD not null)\tSO SSD Late\n",
    "# Only PBO: SO SSD < Today()+28d\tPBO within executn per\n",
    "# (SO SSD w/o IO/PO AND SO SSD) < Today()+28d\tSO w/o Supply\n",
    "# SO PD is null\tSO PD Null\n",
    "from pandas.tseries.offsets import BDay\n",
    "lst_sply=['Planned Buy', 'Planned order', 'Purchase requisition']\n",
    "data['current_date'] = datetime.today()\n",
    "condtn1=data['Ship Date'] < data['current_date']\n",
    "condtn2=(data['PD_Order'].notnull()) & (data['SAD'] > data['PD_Order'])\n",
    "\n",
    "condtn3=(data['Demand']=='Order Pre Bld') & (data['Ship Date'] < (data['current_date']+timedelta(days=28))) & \\\n",
    "            (data['IO/PO No.']!='')\n",
    "\n",
    "condtn4=(data['Demand']!='Order Pre Bld') & (data['Ship Date'] < (data['current_date']+timedelta(days=28))) & \\\n",
    "            (data['IO/PO No.']=='')  & data['Supply Type'].isin(lst_sply)\n",
    "\n",
    "condtn5=(data['PD_Order'].isnull() & (data['current_date'] < (data['Booked Dt'] + BDay(5))) & \\\n",
    "             (data['Demand']=='Order'))\n",
    "condtnX=(~data['Demand'].isin(['Order Flow Stk']) & data['Supply Qty']>0)\n",
    "condtnY=(data['Supply Qty']>0)\n",
    "\n",
    "data['_SO_Status'] =''\n",
    "data['_SO_Status'] = np.where(condtn1 & condtnX, 'SO SSD Past',  \\\n",
    "                              np.where(condtn2 & condtnX,'SO SSD Late', \\\n",
    "                                      np.where((condtn3 & condtnY),'PBO w/n Exec Per', \\\n",
    "                                              np.where(condtn4 & condtnY, 'SO w/o Supply', \\\n",
    "                                                      np.where(condtn5 & condtnY, 'SO PD Null','')))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73a3fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Transit Time\n",
    "tt = pd.read_csv(r'E:\\_Projects\\lookups\\sit_transit_days.csv', encoding='utf-8', low_memory=False)\n",
    "data = pd.merge(data, tt, left_on=['Whse','Org Code'], right_on=['SOURCE','CTRY'], how='left',validate='m:1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ce63a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyodbc\n",
    "# Hari/Ramiya: Instead of reading CSV from SUSDAY1322, read directly from MS Acess DB\n",
    "# E:\\RACE\\SIT-II\\SIT-II.accdb; query is \"qry_ship-meth_TT\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "834b3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc TRANSIT DAYS\n",
    "# Land --> Land\n",
    "# Sea/Air --> for all mcids and ATM, SCO, KIOSK products --> Sea\n",
    "# Sea/Air --> for other products --> Air\n",
    "data['Transit_recom_mode']=''\n",
    "condtn1 = data['Default']=='Land'\n",
    "condtn2 = (data['Default']=='Sea/Air') & (data['Products'].isin(['ATM','SCO','Kiosk'])) & (data['PID Type'].isin(['M','U']))\n",
    "condtn3 = (data['Default'].notnull()) & (data['Transit_recom_mode']=='')\n",
    "data['Transit_recom_mode'] = np.where(condtn1, 'Land', \\\n",
    "                                     np.where(condtn2, 'Sea', \\\n",
    "                                             np.where(condtn3, 'Air', '')))\n",
    "condtns = data['Transit_recom_mode']=='Land', data['Transit_recom_mode']=='Sea', data['Transit_recom_mode']=='Air'\n",
    "values = data['Land'], data['Sea'], data['Air']\n",
    "data['Transit_recom_days'] = np.select(condtns, values)\n",
    "\n",
    "data.drop(columns={'SOURCE','CTRY','Default','Sea','Land','Air'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9825a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _IO/PO_Status\n",
    "# (IO/PO PD/SSD + Transit Time) > SO SSD\n",
    "# PO PD is null, only if IO SSD + 5 days\n",
    "# sply_type=['PD','PO','Purchase requisition','InTransit']\n",
    "# sply_type2=['IREQ','PD','Planned Buy','Planned Order','PO','Purchase requisition']\n",
    "# # data['IO_PO check'] =''\n",
    "# # data['IO_PO check'] = np.where(data['PD_PO'].notnull(), data['PD_PO'], \\\n",
    "# #                               np.where(data['IO SSD'].notnull(), data['IO SSD'],data['IO_PO check']))\n",
    "# # data['IO_PO check'] = pd.to_datetime(data['IO_PO check'])\n",
    "# data['IO_PO check2'] =  data['Plant SSD'] + pd.to_timedelta(data['Transit_recom_days'], unit='D')\n",
    "\n",
    "# condtn1a = (data['IO_PO check2'] > data['Ship Date']) & (data['Supply Type']=='PD')\n",
    "# condtn1b = (data['IO_PO check2'] > data['Ship Date']) & (data['PD_PO'].isnull()) & ~(data['Supply Type'].isin(sply_type))\n",
    "\n",
    "# condtn2 = data['IO/PO No.'].notnull() & data['PD_PO'].isnull() & (data['Supply Type'].isin(sply_type2)) & \\\n",
    "#             (data['current_date'] < data['IO BOOKED DATE']+timedelta(days=5) )\n",
    "# condtn3 = 'intransit'\n",
    "# data['_IO_Status'] = np.where(condtn1a | condtn1b, 'IO SSD Late', \\\n",
    "#                              np.where(condtn2, 'PO PD Null',''))\n",
    "# data.drop(columns={'current_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08d62387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrubbing\n",
    "data['Region'] = data['Region'].str.upper()\n",
    "if data['LOB'].dtype == object:\n",
    "    data['LOB'] = data['LOB'].str[0:3]\n",
    "data['LOB'] = np.where((data['Products']=='SCO') & (data['LOB']=='FIN'), '', data['LOB'])\n",
    "data['LOB Prod'] = np.where(data['Products'].isin(['ATM','PC Core']),'Bank','Com') \n",
    "data['LOB Prod'] = np.where(data['LOB Prod'].isnull(),'Com',data['LOB Prod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83acbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns={'current_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6499d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hari/Ramiya\n",
    "# Concatenate Default\tSea\tLand\tAir\n",
    "# data['Transit_Days'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4820ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#including Dataref corresponding to current date.\n",
    "file_name = file[7:].lower()\n",
    "data['Dataref'] = str(d) + ' ' + file_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "358f0f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updaing Offer pfbase don Pit type: 20-02-2023\n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['K']),'kits',data['Offer PF']) \n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['C']),'Cables',data['Offer PF']) \n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['CPP']),'CPP',data['Offer PF'])\n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['IPP']),'IPP',data['Offer PF'])\n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['RAW']),'RAW',data['Offer PF'])\n",
    "data['Products'] = np.where(data['Products'].isnull(),'Other',data['Products'])\n",
    "data['Offer PF'] = np.where(data['Offer PF'].isnull(),'Other',data['Offer PF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f3bfb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated validation flag and Supply Type \n",
    "data['Validation Flag'] = data['Validation Flag'].replace({'On hand - RS' : 'OH-RS','On hand':'OH', 'PLAN':'Plan', 'InTransit':'IT', \\\n",
    "                                                          'PD OK':'PD OK', 'WIP/Planned Order':'WIP/Plnd Ord', 'New/Release PO':'New/Rel PO',\\\n",
    "                                                          'Scheduled':'Schd', 'Flow':'Flow', 'Purchase requisition':'Pur Req', 'Planned order':'Plnd Ord',\\\n",
    "                                                          'Shipment in receiving':'In Rcvng', 'Past Due PD':'Past PD', 'WIP/PlannedOrder':'WIP/Plnd Ord'})\n",
    "\n",
    "data['Supply Type'] = data['Supply Type'].replace({'On hand - RS' : 'OH-RS','On hand':'OH', 'Planned Buy':'Plnd-Buy', 'InTransit':'IT', \\\n",
    "                                                          'PD OK':'PD OK', 'WIP/Planned Order':'WIP/Plnd Ord', 'New/Release PO':'New/Rel PO',\\\n",
    "                                                          'Scheduled':'Schd', 'Flow':'Flow', 'Purchase requisition':'Pur Req', 'Planned order':'Plnd Ord',\\\n",
    "                                                          'Shipment in receiving':'In Rcvng', 'Past Due PD':'Past PD', 'WIP/PlannedOrder':'WIP/Plnd Ord', 'Planned Make':'Plnd-Make', 'Planned Buy':'Plnd-Buy'})\n",
    "\n",
    "#Update by ramya\n",
    "cond= data['Validation Flag'].isin(['']) & data['Supply Type'].isin(['PO'])\n",
    "data['Validation Flag'] = np.where(cond,'PO',data['Validation Flag'])\n",
    "\n",
    "#updated LOB \n",
    "data['LOB'] = np.where(data['LOB'].isin(['CUS']),'',data['LOB'])\n",
    "data['LOB'] = np.where(data['LOB'].isin(['HSR']),'HOSP',data['LOB'])\n",
    "\n",
    "#Update Demand\n",
    "data['Demand'] = np.where(data['Demand'].isin(['Order Int']),'Ord Int',data['Demand'])\n",
    "data['Demand'] = np.where(data['Demand'].isin(['Order Pre Bld']),'PBO',data['Demand'])\n",
    "data['Demand'] = np.where(data['Demand'].isin(['Order Flow Stk']),'Flow Stk',data['Demand'])\n",
    "\n",
    "conditions = [data['Inv Trigger']=='Shipment',data['Inv Trigger']=='Delivery',data['Inv Trigger']=='Other', \\\n",
    "              data['Inv Trigger']=='Installation',data['Inv Trigger']=='Customer Certification']\n",
    "choices = ['Ship','Del','Oth','Inst','Cert']\n",
    "data['Inv Trigger'] = np.select(conditions, choices)\n",
    "data['Inv Trigger'] = np.where (data['Inv Trigger'].astype(str)=='0','',data['Inv Trigger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cc0ed3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vv185114\\AppData\\Local\\Temp\\3\\ipykernel_10908\\3332769156.py:6: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df = df.groupby(['DEMAND VAL'])['OH Qty','Res Qty','Supply Qty'].sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Calc data['CTS']\n",
    "# Calc: considering ATS status and OH qty by order level)\n",
    "# For one order, If \"SO Avail to Ship\"==OK/Yes; and Check enough OH/RS for rest of the lines --> then CTS\n",
    "df = data.copy()\n",
    "df = df[['DEMAND VAL','OH Qty','Res Qty','Supply Qty']]\n",
    "df = df.groupby(['DEMAND VAL'])['OH Qty','Res Qty','Supply Qty'].sum().reset_index()\n",
    "df['total'] = df['OH Qty'] + df['Res Qty']\n",
    "df['CTS'] = np.where(df['total'] > df['Supply Qty'],'CTS','No CTS')\n",
    "data = pd.merge(data, df[['DEMAND VAL','CTS']], left_on=['DEMAND VAL'], right_on=['DEMAND VAL'],how='left',validate='m:1')\n",
    "\n",
    "data['Dmd Cnt'] =  1 / data.groupby('DEMAND VAL')['DEMAND VAL'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fb4c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPM names\n",
    "\n",
    "cpm_names = pd.read_csv(r'E:\\_Projects\\orders\\rcv\\\\CPM_Names.csv',encoding='utf-8', low_memory=False)\n",
    "cpm_names.rename(columns = {'Customer Number': 'Master Customer Number','CPM Name 2023':'CPM','Manager Name 2023':'CPM Mngr'}, inplace = True)\n",
    "cpm_names.drop_duplicates(inplace=True,subset=['Master Customer Number'],keep='first')\n",
    "data = data.merge(cpm_names[['Master Customer Number','CPM','CPM Mngr']],on='Master Customer Number',how='left',validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e3451c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#comments\n",
    "path = r'E:\\_Projects\\workbench'\n",
    "comments =  pd.read_csv(path + r'\\comments_log\\comments_unique.csv', encoding='utf-8', low_memory=False)\n",
    "#read wb_cols\n",
    "col_map = pd.read_excel(path + r'\\map\\wb_cols.xlsx', sheet_name='field_settings_ms_STD', engine='openpyxl')\n",
    "\n",
    "#iid columns\n",
    "col_iid_list = col_map[col_map['Key ID'].isin(['IID'])]\n",
    "col_iid_list = ['IID'] + col_iid_list['Field Name'].tolist()\n",
    "comments_iid = comments[col_iid_list]\n",
    "comments_iid = comments_iid[comments_iid['IID'].notnull()]\n",
    "\n",
    "#uid columns\n",
    "col_uid_list = col_map[col_map['Key ID'].isin(['UID'])]\n",
    "col_uid_list = ['UID'] + col_uid_list['Field Name'].tolist()\n",
    "comments_uid = comments[col_uid_list]\n",
    "comments_uid = comments_uid[comments_uid['UID'].notnull()]\n",
    "\n",
    "#merge uiid and iid\n",
    "data = data.merge(comments_iid, left_on='IID', right_on = 'IID', how='left',validate='m:1')\n",
    "data = data.merge(comments_uid, left_on='UID', right_on = 'UID', how='left',validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ac1138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\timedeltas.py:908: RuntimeWarning: invalid value encountered in cast\n",
      "  base = data.astype(np.int64)\n",
      "e:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\timedeltas.py:912: RuntimeWarning: invalid value encountered in cast\n",
      "  data = (base * m + (frac * m).astype(np.int64)).view(\"timedelta64[ns]\")\n"
     ]
    }
   ],
   "source": [
    "# Calculate DMD SSD and SPLY SSD\n",
    "data['Calc SO SSD']=''\n",
    "cond1 = (data['Whse']==data['SO Ship Org'])\n",
    "cond2 = (data['Whse'].isin(['CHE','CHS','GF1','US','USL','UF2','EPL','EPI']))\n",
    "data['Calc SO SSD']=np.where(cond1 & cond2, data['Ship Date'],data['Calc SO SSD'])\n",
    "data['Calc SO SSD']=np.where((~cond1) & cond2, data['Ship Date'] - pd.to_timedelta(data['Transit_recom_days'], unit='D'), \\\n",
    "                              data['Calc SO SSD'])\n",
    "align_past('Calc SO SSD')\n",
    "# data['Calc SO SSD'] = data['final']\n",
    "data.drop(columns={'Previous_week_Date'}, inplace=True)\n",
    "\n",
    "data['SO SSD Override++'] = pd.to_datetime(data['SO SSD Override++'],errors='coerce')\n",
    "data['Calc SO SSD Final'] = np.where(data['SO SSD Override++'].notnull(), data['SO SSD Override++'], \\\n",
    "                                np.where(data['Calc SO SSD'].notnull(), data['Calc SO SSD'], data['Ship Date']))\n",
    "#----------------------------------------------------------------------------#\n",
    "\n",
    "# data['Calc Sply SSD']=''\n",
    "# data['temp'] = pd.to_datetime('today')\n",
    "# cond3 = (data['Supply Type'].isin(['OH','OH-RS']))\n",
    "# cond4 = (data['Supply Type'].isin(['PD']))\n",
    "# data['Calc Sply SSD'] = np.where(cond2 & cond3, data['temp'] - timedelta(days=14), data['Calc Sply SSD'])\n",
    "# data['Calc Sply SSD'] = np.where(cond2 & cond4, data['PD_PO'] + pd.to_timedelta(data['Transit_recom_days'], unit='D'),\\\n",
    "#                                                                                          data['Calc Sply SSD'])\n",
    "# data['Calc Sply SSD'] = data['Calc Sply SSD'].astype('datetime64[ns]')\n",
    "# align_past('Calc Sply SSD')\n",
    "# data['Calc Sply SSD'] = data['final']\n",
    "# data.drop(columns={'final'}, inplace=True)\n",
    "\n",
    "# data.drop(columns={'temp'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09ebaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  data['Open Qty'] = data['Open Qty'].astype('Int64')\n",
    "# data['Open Qty'] = data['Open Qty'].values.astype(np.int64)\n",
    "# data['Res Qty'] = data['Res Qty'].values.astype(np.int64)\n",
    "# data['OH Qty'] = data['OH Qty'].values.astype(np.int64)\n",
    "# data['Supply Qty'] = data['Supply Qty'].values.astype(np.int64)\n",
    "# data['Qty'] = data['Supply Qty'].values.astype(np.int64)\n",
    "# data['Supply Qty'].head(10)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17cf3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change dtypes str to qty\n",
    "# col_list=['Open Qty','Res Qty','OH Qty','Supply Qty','Qty']\n",
    "# for col in col_list:\n",
    "#     data[col] = data[col].values.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d16b2695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\softwares\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "e:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\timedeltas.py:908: RuntimeWarning: invalid value encountered in cast\n",
      "  base = data.astype(np.int64)\n",
      "e:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\timedeltas.py:912: RuntimeWarning: invalid value encountered in cast\n",
      "  data = (base * m + (frac * m).astype(np.int64)).view(\"timedelta64[ns]\")\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# Status on IO: Late\n",
    "race_df  =  pd.read_excel(r'E:\\_Projects\\lookups\\RACE_IO_SSD.xlsx', engine='openpyxl')\n",
    "race_df.sort_values(by=['Actual Ship Date'], ascending = True,inplace = True)\n",
    "race_df.drop_duplicates(subset=['Order Number'],keep='last',inplace=True)\n",
    "data = data.merge(race_df[['Order Number','Actual Ship Date']],left_on='IO No.',right_on='Order Number',how='left',validate='m:1')\n",
    "data['Actual Ship Date'] = np.where(data['Actual Ship Date'].isnull(),data['IO SSD'],data['Actual Ship Date'])\n",
    "data['Actual Ship Date']  = data['Actual Ship Date'] + pd.to_timedelta(data['Transit_recom_days'], unit='D')\n",
    "data['Plant SSD Trans'] =  data['Plant SSD'] + pd.to_timedelta(data['Transit_recom_days'], unit='D')\n",
    "\n",
    "\n",
    "cond = (data['Plant SSD'].isnull() | data['Validation Flag'].isin(['Plan']))\n",
    "logic_1= (data['Whse'] == data['SO Ship Org']) & (data['Plant SSD']>=data['Ship Date'])\n",
    "logic_2 = (data['Whse'] != data['SO Ship Org']) & (data['Supply Type']=='IT') & (data['Actual Ship Date'] >=data['Ship Date'])\n",
    "logic_3 = (data['Whse'] != data['SO Ship Org']) & ( data['Plant SSD Trans'] >=data['Ship Date']) & (data['Supply Type']!='IT')\n",
    "\n",
    "conds = [~cond & logic_1,~cond & logic_2,~cond & logic_3]\n",
    "choices = ['Late','Late','Late']\n",
    "\n",
    "data['_IO_Status'] = np.select(condlist=conds, choicelist=choices)\n",
    "data['_IO_Status']  = np.where(data['_IO_Status']=='0','',data['_IO_Status'])\n",
    "data.drop(columns={'Plant SSD Trans','Actual Ship Date','Order Number'}, axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24f5120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_map = pd.read_excel(r'E:\\_Projects\\workbench\\map\\wb_cols.xlsx', sheet_name='field_list', engine='openpyxl')\n",
    "col_map = col_map['Fields'].unique().tolist()\n",
    "data = data[col_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d9fdb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove enter in all columns\n",
    "data = data.replace('\\n',';', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c3980b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export wb data into local folder\n",
    "out_name = 'workbench_data_' + file[7:].lower() + \\\n",
    "            '_' + str(datetime.strptime(d, '%d-%b-%Y').year).zfill(4) + str(datetime.strptime(d, '%d-%b-%Y').month).zfill(2) + \\\n",
    "            str(datetime.strptime(d, '%d-%b-%Y').day).zfill(2)\n",
    "\n",
    "out_name_wPO = 'workbench_data_' + file[7:].lower() + \\\n",
    "            '_wPO_' + str(datetime.strptime(d, '%d-%b-%Y').year).zfill(4) + str(datetime.strptime(d, '%d-%b-%Y').month).zfill(2) + \\\n",
    "            str(datetime.strptime(d, '%d-%b-%Y').day).zfill(2)\n",
    "\n",
    "\n",
    "#save local in csv\n",
    "dst_local = r'E:\\_Projects\\_outputs\\orders\\snd\\\\' \n",
    "data.to_csv(dst_local + out_name + '_v2.csv', index=False)\n",
    "\n",
    "#save server tool in csv\n",
    "dst_tool = r'E:\\_Projects\\workbench\\snd\\\\'\n",
    "data.to_csv(dst_tool + out_name + '.zip', compression={'method': 'zip', 'archive_name': out_name + '.csv'}, index=False)\n",
    "#input()\n",
    "sharpoint_Path =r'\\\\wtc1501cifs.prod.local\\ORGSHARE\\TEAMS\\ERP Shared Folder\\NCR Atleos\\Atleos Operations Planning\\Production\\workbench\\snd\\\\'\n",
    "data.to_csv(sharpoint_Path + out_name + '.zip', compression={'method': 'zip', 'archive_name': out_name + '.csv'}, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a2e4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_name: enn; data: eumocp_20240301\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\_Projects\\\\_outputs\\\\orders\\\\snd\\\\workbench_data_eumocp_20240301_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mE:\\_Projects\\orders\\alloc_v2.py:239\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msite_name: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mv_site\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; data: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mfile_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdate_str)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m#print(v_site)\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     out \u001b[38;5;241m=\u001b[39m allocation(v_site,file_name,date_str)\n\u001b[0;32m    240\u001b[0m     data_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data_all,out],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m site\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALL\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m#data_all.to_csv(r'E:\\_Projects\\orders\\test\\wb_all_allocation_' + date_str + '.csv', index=False)\u001b[39;00m\n",
      "File \u001b[1;32mE:\\_Projects\\orders\\alloc_v2.py:41\u001b[0m, in \u001b[0;36mallocation\u001b[1;34m(site, ocp_name, date_str)\u001b[0m\n\u001b[0;32m     39\u001b[0m      data_en \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data_en, data_tmp], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#read ocp data\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m data_wb \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_Projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_outputs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124morders\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msnd\u001b[39m\u001b[38;5;124m'\u001b[39m, file_wb),encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, keep_default_na\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m cols_en \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBooked Dt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPID\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQty\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShip Dt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPD\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequest Dt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTot Qty\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrg PD\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCritical\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriority\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments-1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments-2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomer\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProd Cate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     43\u001b[0m data_en \u001b[38;5;241m=\u001b[39m data_en[cols_en]\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\_Projects\\\\_outputs\\\\orders\\\\snd\\\\workbench_data_eumocp_20240301_v2.csv'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_name: jab; data: eumocp_20240301\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\_Projects\\\\_outputs\\\\orders\\\\snd\\\\workbench_data_eumocp_20240301_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mE:\\_Projects\\orders\\alloc_v2.py:239\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msite_name: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mv_site\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; data: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mfile_name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdate_str)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m#print(v_site)\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     out \u001b[38;5;241m=\u001b[39m allocation(v_site,file_name,date_str)\n\u001b[0;32m    240\u001b[0m     data_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data_all,out],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m site\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALL\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m#data_all.to_csv(r'E:\\_Projects\\orders\\test\\wb_all_allocation_' + date_str + '.csv', index=False)\u001b[39;00m\n",
      "File \u001b[1;32mE:\\_Projects\\orders\\alloc_v2.py:41\u001b[0m, in \u001b[0;36mallocation\u001b[1;34m(site, ocp_name, date_str)\u001b[0m\n\u001b[0;32m     39\u001b[0m      data_en \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data_en, data_tmp], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#read ocp data\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m data_wb \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_Projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_outputs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124morders\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msnd\u001b[39m\u001b[38;5;124m'\u001b[39m, file_wb),encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, keep_default_na\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m cols_en \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBooked Dt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPID\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQty\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPO\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShip Dt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPD\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequest Dt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTot Qty\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrg PD\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCritical\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriority\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments-1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments-2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomer\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProd Cate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     43\u001b[0m data_en \u001b[38;5;241m=\u001b[39m data_en[cols_en]\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\_Projects\\\\_outputs\\\\orders\\\\snd\\\\workbench_data_eumocp_20240301_v2.csv'"
     ]
    }
   ],
   "source": [
    "# CAll allocation script\n",
    "file_name = file[7:].lower()\n",
    "if file_name == 'eumocp':\n",
    "    %run Orders_Allocation.ipynb\n",
    "    #%run alloc_v2.py enn eumocp 20240514\n",
    "    #%run alloc_v2.py jab eumocp 20240514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ddebb178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240301\n"
     ]
    }
   ],
   "source": [
    "today_date = datetime.now(pytz.timezone('Asia/Calcutta'))\n",
    "today_date = today_date.strftime('%Y%m%d')\n",
    "pervious_date='20240115'\n",
    "pervious_date_enn='20231225'\n",
    "pervious_date_USI='20230828'\n",
    "print(today_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f13bbdb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\_Projects\\\\_outputs\\\\orders\\\\snd\\\\wb_enn_allocation_20240301.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrders.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PD_Enn \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 4\u001b[0m     enn_alloc\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_Projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m_outputs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124morders\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msnd\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mwb_enn_allocation_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mtoday_date\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keep_default_na\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     enn_alloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEN2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m     enn_alloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDemand\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPO-ENN\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\_Projects\\\\_outputs\\\\orders\\\\snd\\\\wb_enn_allocation_20240301.csv'"
     ]
    }
   ],
   "source": [
    "if file_name =='eumocp':\n",
    "    %run Orders.ipynb\n",
    "    if PD_Enn ==0:\n",
    "        enn_alloc= pd.read_csv(r'E:\\_Projects\\_outputs\\orders\\snd\\wb_enn_allocation_'+today_date+'.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "        enn_alloc['Plant'] = 'EN2'\n",
    "        enn_alloc['Demand'] = 'PO-ENN'\n",
    "    else:\n",
    "        enn_alloc= pd.read_csv(r'E:\\_Projects\\_outputs\\orders\\snd\\wb_enn_allocation_'+pervious_date_ENN+'.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "        enn_alloc['Plant'] = 'EN2'\n",
    "        enn_alloc['Demand'] = 'PO-ENN'\n",
    "    if PD_Jab == 0:    \n",
    "        jab_alloc= pd.read_csv(r'E:\\_Projects\\_outputs\\orders\\snd\\wb_jab_allocation_'+today_date+'.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "        jab_alloc['Plant'] = 'JB2'\n",
    "        jab_alloc['Demand'] = 'PO-JAB'\n",
    "    else:\n",
    "        jab_alloc= pd.read_csv(r'E:\\_Projects\\_outputs\\orders\\snd\\wb_jab_allocation_'+pervious_date_JAB+'.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "        jab_alloc['Plant'] = 'JB2'\n",
    "        jab_alloc['Demand'] = 'PO-JAB'\n",
    "\n",
    "    usi_alloc= pd.read_csv(r'E:\\_Projects\\_outputs\\orders\\snd\\wb_usi_allocation_'+pervious_date_USI+'.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "    usi_alloc['Plant'] = 'UN2'\n",
    "    usi_alloc['Demand'] = 'PO-USI'\n",
    "\n",
    "    alloc = pd.concat([enn_alloc,jab_alloc,usi_alloc],axis=0)\n",
    "    alloc = alloc[data.columns]\n",
    "\n",
    "    set_date_frmt = ['Booked Dt','Request Dt','PD_Order','Ship Date','SAD','IO BOOKED DATE','IO SSD','IO SAD','PD_PO','CRSD','PRD','Plant Ship date','NEED BY DATE','Plant SSD']\n",
    "    for field in set_date_frmt:\n",
    "        alloc[field] = pd.to_datetime(alloc[field])\n",
    "\n",
    "    data = pd.concat([data,alloc],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dst_local = 'E:\\_Projects\\_outputs\\orders\\snd\\\\'\n",
    "data.to_csv(dst_local + out_name_wPO + '_v2.csv', index=False)\n",
    "\n",
    "# dst_dump = r'\\\\dayorg1\\orgshare\\TEAMS\\ERP Shared Folder\\Global S&OP\\workbench\\snd\\\\'\n",
    "# data.to_csv(dst_dump + out_name + '.csv', index=False)\n",
    "\n",
    "# dst_tool = r'\\\\dayorg1\\orgshare\\TEAMS\\ERP Shared Folder\\Global S&OP\\Supply Plan\\workbench\\snd\\\\'\n",
    "# data.to_csv(dst_tool + out_name + '.zip', compression={'method': 'zip', 'archive_name': out_name + '.csv'}, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c996f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
