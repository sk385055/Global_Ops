{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b9fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#KA_Path  = r'E:\\_Projects\\DEP POR\\Key Account\\Input\\\\'\n",
    "\n",
    "Path = r'E:\\_Projects\\DEP POR\\Azure Vs IND POR\\\\'\n",
    "\n",
    "\n",
    "# 0 - Pul Data from SQL / 1 - Load Data from local\n",
    "SQL_Data = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c26d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ Load Files\n",
    "\n",
    "POR = glob.glob(Path+'Input\\ATLEOS OCP IND POR_???????????.csv')\n",
    "\n",
    "#print(POR)\n",
    "\n",
    "\n",
    "for POR in POR:\n",
    "    IND_POR = pd.read_csv(POR,low_memory=False)\n",
    "    \n",
    "    \n",
    "    IND_POR = IND_POR[IND_POR['Data Series'].str.contains('NEW DEMAND',na=False) & IND_POR['Item Type'].str.contains('Unassigned',na=False)]\n",
    "    \n",
    "    mask = IND_POR['Country Code'].isnull()\n",
    "\n",
    "    IND_POR.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "    #IND_POR.to_csv(Path+'Output.csv')\n",
    "    \n",
    "    #print(IND_POR.columns)\n",
    "\n",
    "\n",
    "#IND_POR.columns[10:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591c4ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************** \n",
      "Enter Date MMM-YY : OCT-25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "month=input(\"\\n********************************************************** \\nEnter Date MMM-YY : \")\n",
    "\n",
    "index = IND_POR.columns.get_loc('Comments')+1\n",
    "IND_POR =IND_POR.iloc[:,0:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd4474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling KA from Oracle SQL...........!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ak250659\\AppData\\Local\\Temp\\7\\ipykernel_7036\\1756664495.py:29: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  KA = pd.read_sql(query, con=conn)\n"
     ]
    }
   ],
   "source": [
    "############################# PUll KA File from Orale SQL #########################\n",
    "#KA = pd.read_excel(Path+'Input\\SQL.xlsx',keep_default_na=False)\n",
    "import oracledb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if SQL_Data == 0:\n",
    "    \n",
    "    print('Pulling KA from Oracle SQL...........!')\n",
    "\n",
    "    # Enable thick mode\n",
    "    oracledb.init_oracle_client(lib_dir=r\"E:\\softwares\\instantclient-basic-windows.x64-23.5.0.24.07\\instantclient_23_5\")\n",
    "\n",
    "    # Connection details\n",
    "    dsn_tns = oracledb.makedsn('153.84.75.216', 1521, service_name='ebs_ATMPROD')\n",
    "\n",
    "\n",
    "    # Use a context manager to handle the connection\n",
    "    with oracledb.connect(user='erp_user', password='erp_pass', dsn=dsn_tns) as conn:\n",
    "        KA = pd.DataFrame()\n",
    "\n",
    "        query = f\"\"\"\n",
    "        select DISTINCT  * from\n",
    "        NCR_DEM_KEY_ACCOUNT_ASSG\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch data into a pandas DataFrame and append to the main DataFrame\n",
    "        KA = pd.read_sql(query, con=conn)\n",
    "        \n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    KA.to_csv(Path+'Input\\\\ref\\\\SQL.csv', index=False)\n",
    "else:\n",
    "    KA = pd.read_csv(Path+'Input\\\\ref\\\\SQL.csv')\n",
    "\n",
    "#print(KA.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46efa3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Loading Auzre File\n",
    "Azure = pd.read_csv(Path+\"Input\\Azure.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92607c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling Region from Oracle SQL...........!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ak250659\\AppData\\Local\\Temp\\7\\ipykernel_7036\\2688549565.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  Region = pd.read_sql(query, con=conn)\n"
     ]
    }
   ],
   "source": [
    "############################# PUll KA File from Orale SQL #########################\n",
    "#KA = pd.read_excel(Path+'Input\\SQL.xlsx',keep_default_na=False)\n",
    "import oracledb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if SQL_Data == 0:\n",
    "    \n",
    "    print('Pulling Region from Oracle SQL...........!')\n",
    "\n",
    "    # Enable thick mode\n",
    "    oracledb.init_oracle_client(lib_dir=r\"E:\\softwares\\instantclient-basic-windows.x64-23.5.0.24.07\\instantclient_23_5\")\n",
    "\n",
    "    # Connection details\n",
    "    dsn_tns = oracledb.makedsn('153.84.75.216', 1521, service_name='ebs_ATMPROD')\n",
    "\n",
    "\n",
    "    # Use a context manager to handle the connection\n",
    "    with oracledb.connect(user='erp_user', password='erp_pass', dsn=dsn_tns) as conn:\n",
    "        Region = pd.DataFrame()\n",
    "\n",
    "        query = f\"\"\"\n",
    "        select DISTINCT  * from\n",
    "        apps.ncr_ocp_sales_hierarchy\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch data into a pandas DataFrame and append to the main DataFrame\n",
    "        Region = pd.read_sql(query, con=conn)\n",
    "        \n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    Region.to_csv(Path+'Input\\\\ref\\\\Region.csv', index=False)\n",
    "else:\n",
    "    Region = pd.read_csv(Path+'Input\\\\ref\\\\Region.csv')\n",
    "\n",
    "\n",
    "mask = Region['COUNTRY_CODE'].isnull()\n",
    "\n",
    "Region.loc[mask,'COUNTRY_CODE'] = 'NA'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b20757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c460dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*************************************************\n",
      "\n",
      "Checking Backward 6month Shipment & Foward 12 month SSD \n",
      "\n",
      "*************************************************\n",
      "\n",
      "Backward 6 Month Shipment Date       : Apr-25 present? True\n",
      "Forward 12 Month Schedule Ship Date  : Oct-26 present? True\n",
      "*************************************************\n",
      "\n",
      "Press Enter to Continue......... :\n",
      "Azure Completed\n",
      "Check list Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ak250659\\AppData\\Local\\Temp\\7\\ipykernel_7036\\4063889966.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Check_List.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "################################################ AZURE ##################################################\n",
    "############################## Remove Spaces\n",
    "\n",
    "Azure['ISO Country Code'] = Azure['ISO Country Code'].str.strip()\n",
    "\n",
    "mask = Azure['ISO Country Code'].isnull()\n",
    "\n",
    "Azure.loc[mask,'ISO Country Code'] = 'NA'\n",
    "\n",
    "\n",
    "\n",
    "Azure = pd.merge(Azure, Region[['COUNTRY_CODE','NEW_COUNTRY']],left_on='ISO Country Code',right_on='COUNTRY_CODE',how='left')\n",
    "\n",
    "Azure.loc[Azure['NEW_COUNTRY'].notnull(),'ISO Country Code']=Azure['NEW_COUNTRY']\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*NEW_COUNTRY$)')\n",
    "Azure = Azure.filter(regex='^(?!.*COUNTRY_CODE$)')\n",
    "\n",
    "#################################### SSD & Ship Date filter\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "Azure['Actual Ship Date'] = pd.to_datetime(Azure['Actual Ship Date'])\n",
    "Azure['Scheduled Ship Date'] = pd.to_datetime(Azure['Scheduled Ship Date'])\n",
    "\n",
    "\n",
    "\n",
    "# Create a new column 'SD_Date' with the month and year of the 'Actual Ship Date'\n",
    "Azure['SD_Date'] = Azure['Actual Ship Date'].dt.strftime('%b-%y')\n",
    "\n",
    "# Get the current date\n",
    "latest_month = datetime.now()\n",
    "\n",
    "# Create a list of the last 7 months\n",
    "check_month = [latest_month - relativedelta(months=x) for x in range(7)]\n",
    "\n",
    "# Convert the list of datetime objects to strings in the format '%b-%y'\n",
    "check_month_str = [month.strftime('%b-%y') for month in check_month]\n",
    "\n",
    "# Filter the Azure dataframe to only include rows where 'SD_Date' is in 'check_month_str'\n",
    "Azure_Actual_Ship = Azure[Azure['SD_Date'].isin(check_month_str)]\n",
    "\n",
    "\n",
    "# Create a list of the last 7 months\n",
    "check_month = [latest_month + relativedelta(months=x) for x in range(13)]\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list of datetime objects to strings in the format '%b-%y'\n",
    "check_month_str = [month.strftime('%b-%y') for month in check_month]\n",
    "\n",
    "\n",
    "# Create a new column 'SD_Date' with the month and year of the 'Actual Ship Date'\n",
    "Azure['SD_Date'] = Azure['Scheduled Ship Date'].dt.strftime('%b-%y')\n",
    "\n",
    "\n",
    "# Filter the Azure dataframe to only include rows where 'SD_Date' is in 'check_month_str'\n",
    "Azure_SSD = Azure[Azure['SD_Date'].isin(check_month_str) & Azure['Actual Ship Date'].isnull()]\n",
    "\n",
    "\n",
    "\n",
    "Azure = pd.concat([Azure_Actual_Ship,Azure_SSD],axis=0)\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*SSD_Date$)')\n",
    "Azure = Azure.filter(regex='^(?!.*SD_Date$)')\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "#################################### SSD & Ship Dt Validation process\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('*************************************************\\n')\n",
    "\n",
    "print('Checking Backward 6month Shipment & Foward 12 month SSD \\n')\n",
    "\n",
    "print('*************************************************\\n')\n",
    "####### Ship Date Validation\n",
    "Azure['SD_Date'] = Azure['Actual Ship Date'].dt.strftime('%b-%y')\n",
    "\n",
    "#print(Azure['SD_Date'])\n",
    "\n",
    "Azure['SD_Date'] = Azure['SD_Date'].astype(str)\n",
    "\n",
    "# Get the latest month in the 'month' column\n",
    "latest_month = datetime.now()\n",
    "\n",
    "# Calculate the month and year 4 months before the latest month\n",
    "check_month = latest_month - relativedelta(months=6)\n",
    "\n",
    "#print(type(check_month))\n",
    "\n",
    "# Convert check_month to the same format as Azure['month']\n",
    "check_month_str = check_month.strftime('%b-%y')\n",
    "\n",
    "# Check if the calculated month exists in the 'month' column\n",
    "is_present = (Azure['SD_Date'] == check_month_str).any()\n",
    "\n",
    "print(f\"Backward 6 Month Shipment Date       : {check_month_str} present? {is_present}\")\n",
    "\n",
    "############### SSD Validation\n",
    "\n",
    "\n",
    "Azure['SSD_Date'] = Azure['Scheduled Ship Date'].dt.strftime('%b-%y')\n",
    "\n",
    "#print(Azure['SSD_Date'])\n",
    "\n",
    "Azure['SSD_Date'] = Azure['SSD_Date'].astype(str)\n",
    "\n",
    "# Get the latest month in the 'month' column\n",
    "latest_month = datetime.now()\n",
    "\n",
    "# Calculate the month and year 4 months before the latest month\n",
    "check_month = latest_month + relativedelta(months=12)\n",
    "\n",
    "#print(type(check_month))\n",
    "\n",
    "# Convert check_month to the same format as Azure['month']\n",
    "check_month_str = check_month.strftime('%b-%y')\n",
    "\n",
    "# Check if the calculated month exists in the 'month' column\n",
    "is_present = (Azure['SSD_Date'] == check_month_str).any()\n",
    "\n",
    "print(f\"Forward 12 Month Schedule Ship Date  : {check_month_str} present? {is_present}\")\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*SSD_Date$)')\n",
    "Azure = Azure.filter(regex='^(?!.*SD_Date$)')\n",
    "\n",
    "\n",
    "#Azure.to_csv(Path+'Output.csv')\n",
    "print('*************************************************\\n')\n",
    "continues = input('Press Enter to Continue......... :')\n",
    "\n",
    "#############################################################################\n",
    "############################## Including KA \n",
    "#print(KA.columns)\n",
    "\n",
    "KA = KA[['MASTER_CUST_NUM','KEY_ACCOUNT','MASTER_CUST_NAME']]\n",
    "\n",
    "KA['KEY_ACCOUNT'] = KA['KEY_ACCOUNT'].str.strip()\n",
    "\n",
    "KA['MASTER_CUST_NUM'] = KA['MASTER_CUST_NUM'].astype(str)\n",
    "Azure['Master Customer Number'] = Azure['Master Customer Number'].astype(str)\n",
    "\n",
    "KA.drop_duplicates(inplace=True)\n",
    "\n",
    "Azure = pd.merge(Azure, KA, left_on='Master Customer Number',right_on='MASTER_CUST_NUM',how='left')\n",
    "\n",
    "Azure.loc[Azure['KEY_ACCOUNT'].isnull(),'KEY_ACCOUNT'] = \"OTHER\"\n",
    "\n",
    "\n",
    "Azure.loc[Azure['KEY_ACCOUNT']=='OTHER','KEY_ACCOUNT'] = 'OTHER-'+Azure['ISO Country Code']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################## Data Series\n",
    "\n",
    "# Create a boolean mask for non-null 'Actual Ship Date'\n",
    "mask = Azure['Actual Ship Date'].notnull()\n",
    "\n",
    "# Update the 'Data Series' column where the mask is True\n",
    "Azure.loc[mask, 'Data Series'] = 'SHIP'\n",
    "\n",
    "# Update the '6M Ship' column where the mask is True\n",
    "Azure.loc[mask, '6M Ship'] = Azure['Net Quantity']\n",
    "\n",
    "\n",
    "\n",
    "mask = Azure['Scheduled Ship Date'].notnull() & Azure['Actual Ship Date'].isnull()\n",
    "\n",
    "Azure.loc[mask,'Data Series'] = 'Order'\n",
    "\n",
    "Azure.loc[mask,'6M Order'] = Azure['Net Quantity']\n",
    "\n",
    "############################################\n",
    "############################################\n",
    "\n",
    "###### Keep w/o blank in Data series\n",
    "Azure = Azure[Azure['Data Series'].notnull()]\n",
    "\n",
    "\n",
    "################### Check list\n",
    "Azure['MCID Class'] = Azure['MCID Class'].astype(int).round(0)\n",
    "\n",
    "Azure['Concat'] = Azure['ISO Country Code']+Azure['KEY_ACCOUNT']+Azure['MCID Class'].astype(str)\n",
    "\n",
    "\n",
    "Azure[['ISO Country Code','Order Number','Order Type Name','Line Number','Shipment Number','Master Customer Number','Master Customer Name','KEY_ACCOUNT','Offering Accounting Type Code','MCID Class','Product ID','Product Description','Order Booked Date','Line Request Date Time','Scheduled Ship Date','Scheduled Arrival Date','Actual Ship Date','Line Status','Data Series','6M Ship','6M Order','Net Quantity','Warehouse Name','Organization Code','PID type','MCID-Net Order Value-US','MCID- MCC-US','Purchase Order Number','Invoice Trigger','Functional Group Short Name','region_name','canceled_quantity']].to_csv(Path+'\\Output\\Azure.csv',index=False)\n",
    "\n",
    "#Azure.to_csv(Path+'\\Output\\Azure_checklist.csv',index=False)\n",
    "\n",
    "print('Azure Completed')\n",
    "\n",
    "############################# \n",
    "\n",
    "Check_List = Azure[['Concat','Product ID']]\n",
    "\n",
    "\n",
    "Check_List.drop_duplicates(inplace=True)\n",
    "\n",
    "#Check_List.to_csv(Path+'Output\\Checklist.csv',index=False)\n",
    "\n",
    "\n",
    "#Check_List = pd.read_csv(Path+'Output\\Checklist.csv')\n",
    "\n",
    "\n",
    "Check_List = Check_List.groupby('Concat')['Product ID'].apply(','.join).reset_index()\n",
    "\n",
    "Check_List.to_csv(Path+'\\Output\\Check List.csv',index=False)\n",
    "\n",
    "print('Check list Completed')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab05796e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2012\n",
       "1       2018\n",
       "2       6620\n",
       "3       6623\n",
       "4       6627\n",
       "        ... \n",
       "3806    6629\n",
       "3807    4053\n",
       "3808    6658\n",
       "3809    6770\n",
       "3810    6771\n",
       "Name: Class, Length: 3801, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "####################################### IND POR #####################################################\n",
    "\n",
    "########################### insert new publish flag\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Publish Flag')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['Class','Publish flag']]\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class'],keep='first')\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List,on='Class',how='left')\n",
    "\n",
    "#IND_POR = IND_POR[IND_POR['Publish flag'] == \"Y\"]\n",
    "\n",
    "#print('line 119 : ',IND_POR.columns)\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "IND_POR['Key Account'] = IND_POR['Key Account'].str.strip()\n",
    "\n",
    "IND_POR['Concat'] = IND_POR['Country Code']+ IND_POR['Key Account'] +IND_POR['Class'].astype(str)\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Check_List, on='Concat', how='left')\n",
    "\n",
    "\n",
    "#IND_POR.to_csv(Path+'IND_POR.csv',index=False)\n",
    "\n",
    "mask = IND_POR['Product ID'].isnull()\n",
    "\n",
    "if mask.any():\n",
    "    IND_POR['Concat_1'] = IND_POR['Bill to Country'] + IND_POR['KeyAccount_Billing'] + IND_POR['Class'].astype(str)\n",
    "    \n",
    "    IND_POR = pd.merge(IND_POR, Check_List, left_on='Concat_1', right_on='Concat', how='left')\n",
    "    \n",
    "    \n",
    "mask = (IND_POR['Product ID_x'].isnull() & IND_POR['Product ID_y'].notnull())\n",
    "\n",
    "if mask.any():\n",
    "    IND_POR.loc[mask, 'Country Code'] = IND_POR.loc[mask, 'Bill to Country']\n",
    "    IND_POR.loc[mask, 'Key Account'] = IND_POR.loc[mask, 'KeyAccount_Billing']\n",
    "    IND_POR.loc[mask, 'Product ID_x'] = IND_POR.loc[mask, 'Product ID_y']\n",
    "    \n",
    "\n",
    "IND_POR.loc[IND_POR['Product ID_x'].notnull(),'Partial config']='No'\n",
    "IND_POR.loc[IND_POR['Product ID_x'].isnull(),'Partial config']='Yes'\n",
    "\n",
    "#IND_POR.to_csv(Path+'dEMOD.csv',index=False)\n",
    "\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Product ID_x','MCID'))\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Product ID_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Concat_x$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Concat_1$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Concat_y$)')\n",
    "\n",
    "\n",
    "#print('LIne number 150 :',IND_POR.columns)\n",
    "###########################################################################################\n",
    "############################## Update Plant\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "#print(Mix_Check_List.columns)\n",
    "  \n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class','Country Code'],keep='first')\n",
    "\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List[['Country Code','Class','Plant']],on=['Class','Country Code'],how='left')\n",
    "\n",
    "\n",
    "#####################\n",
    "Mix_Check_List = pd.read_excel(Path+'input\\Mix Check List.xlsx',sheet_name='Publish Flag',keep_default_na=False)\n",
    "\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['As per Sourcing Matrix','Country Code']]\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Country Code'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List,on='Country Code',how='left')\n",
    "\n",
    "#IND_POR.to_csv(Path+'IND_POR1.csv',index=False)\n",
    "\n",
    "####################\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Country Code','Class'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "#IND_POR.to_csv(Path+'IND_POR1.csv',index=False)\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List[['Country Code','Class','Plant']],left_on=['Class','As per Sourcing Matrix'],right_on=['Class','Country Code'],how='left')\n",
    "\n",
    "#IND_POR.to_csv(Path+'IND_POR2.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "mask = (IND_POR['Plant_x'].isnull() & IND_POR['Plant_y'].notnull())\n",
    "\n",
    "if mask.any():\n",
    "    IND_POR.loc[mask, 'Plant_x'] = IND_POR.loc[mask, 'Plant_y']\n",
    "\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Country Code_x', 'Country Code'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Plant_x', 'Plant'))\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Country Code_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Plant_y$)')\n",
    "\n",
    "#print('Line 194 : ',IND_POR.columns)\n",
    "\n",
    "##############################################################################\n",
    "######################### Update DTF\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant',keep_default_na=False)\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class.1', 'Plant.1'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List[['Class.1', 'Plant.1', 'DTF']],left_on=['Class','Plant'],right_on=['Class.1', 'Plant.1'],how='left')\n",
    "\n",
    "#print(Mix_Check_List.columns)\n",
    "#IND_POR.to_csv(Path+'IND_POR2.csv',index=False)\n",
    "\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "IND_POR['Start Date'] = pd.to_datetime(IND_POR['Start Date'])\n",
    "\n",
    "   \n",
    "IND_POR['DTF_y'] = IND_POR['DTF_y'].fillna(0).astype(int)  \n",
    "\n",
    "\n",
    " \n",
    "IND_POR.loc[IND_POR['DTF_y'].notnull() , 'End Date_New'] = (IND_POR['Start Date'] + pd.to_timedelta(IND_POR['DTF_y'].astype(int), unit='D')).dt.strftime('%m/%d/%Y')\n",
    "  \n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('DTF_y', 'DTF'))\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Class.1$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Plant.1$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*End Date$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*DTF_x$)')\n",
    "\n",
    " \n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('End Date_New', 'End Date'))\n",
    "\n",
    "############ Transition Month\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class','Country Code'],keep='first')\n",
    "\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['Class','Country Code','Sea','Air']]\n",
    "\n",
    "Mix_Check_List['Sea'] = Mix_Check_List['Sea'].fillna(0)\n",
    "Mix_Check_List['Air'] = Mix_Check_List['Air'].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "IND_POR['Class'] = IND_POR['Class'].astype(str)\n",
    "Mix_Check_List['Class'] = Mix_Check_List['Class'].astype(str)\n",
    "\n",
    "Mix_Check_List['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88019410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6623\n",
       "1       6623\n",
       "2       6623\n",
       "3       6623\n",
       "4       6623\n",
       "        ... \n",
       "3089    6623\n",
       "3090    6623\n",
       "3091    2019\n",
       "3092    2012\n",
       "3093    6658\n",
       "Name: Class, Length: 3094, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IND_POR['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb28462",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'E:\\\\_Projects\\\\DEP POR\\\\Azure Vs IND POR\\\\\\\\\\\\Output\\\\IND_POR_WO.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 67\u001b[0m\n\u001b[0;32m     63\u001b[0m IND_POR_WO \u001b[38;5;241m=\u001b[39m IND_POR_WO[columns_to_move \u001b[38;5;241m+\u001b[39m remaining_columns]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#IND_POR_WO = IND_POR_WO.filter(regex='^(?!.*Comments$)')\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m IND_POR_WO\u001b[38;5;241m.\u001b[39mto_csv(Path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIND_POR_WO.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3721\u001b[0m     path_or_buf,\n\u001b[0;32m   3722\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3723\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3724\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3725\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3726\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3727\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3728\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3729\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3730\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3731\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3732\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3733\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3734\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3735\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3736\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3737\u001b[0m )\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'E:\\\\_Projects\\\\DEP POR\\\\Azure Vs IND POR\\\\\\\\\\\\Output\\\\IND_POR_WO.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List,on=['Class','Country Code'],how='left')\n",
    "\n",
    "\n",
    "\n",
    "IND_POR = pd.merge(IND_POR, Mix_Check_List,right_on=['Class','Country Code'],left_on=['Class','As per Sourcing Matrix'],how='left')\n",
    "\n",
    "\n",
    "#print(IND_POR.columns)\n",
    "\n",
    "mask = IND_POR['Sea_x'].isnull() & IND_POR['Sea_y']\n",
    "IND_POR.loc[mask,'Sea_x']=IND_POR['Sea_y']\n",
    "\n",
    "mask = IND_POR['Air_x'].isnull() & IND_POR['Air_y']\n",
    "IND_POR.loc[mask,'Air_x']=IND_POR['Air_y']\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Air_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Sea_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Country Code_y$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*As per Sourcing Matrix$)')\n",
    "\n",
    "    \n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Sea_x', 'Sea'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Air_x', 'Air'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Country Code_x', 'Country Code'))\n",
    "\n",
    "\n",
    "\n",
    "### MROUND\n",
    "\n",
    "# Convert the 'Sea' column to numeric, and replace non-numeric values with NaN\n",
    "IND_POR['Sea'] = pd.to_numeric(IND_POR['Sea'], errors='coerce')\n",
    "IND_POR['Air'] = pd.to_numeric(IND_POR['Air'], errors='coerce')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "IND_POR['Sea'] = IND_POR['Sea'].fillna(0)\n",
    "IND_POR['Air'] = IND_POR['Air'].fillna(0)\n",
    "\n",
    "# Define the mround function\n",
    "def mround(x, base=30):\n",
    "    return base * round(x/base)\n",
    "\n",
    "# Apply the function to the 'Sea' column\n",
    "IND_POR['Transit Month'] = IND_POR['Sea'].astype(int).apply(mround)\n",
    "\n",
    "\n",
    "IND_POR.loc[IND_POR['Product Range'].str.contains('PC Core'),'Transit Month'] =IND_POR['Air'].astype(int).apply(mround)\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Air$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Sea$)')\n",
    "\n",
    "IND_POR_WO = IND_POR\n",
    "\n",
    "\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Region','LOB Region'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Area','LOB Area'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Region1','Region'))\n",
    "IND_POR = IND_POR.rename(columns=lambda x: x.replace('Area2','Area'))\n",
    "\n",
    "columns_to_move = ['Demand Stream','Region1','Area2','Region','Theatre','Area','Country Name','Country Code','Bill to Country','Plant','Publish flag','Zone','Customer LOB','Product LOB','Product Range','Parent Model','Class','Description','DTF','DTF cutoff','Start Date','End Date','DTF risk units','DTF risk','Organization Code','MCID','Partial config','Transit Month','KeyAccount_Billing','Key Account','Master Customer','Item Type','ASP','Data Series']\n",
    "\n",
    "remaining_columns = [col for col in IND_POR_WO.columns if col not in columns_to_move]\n",
    "    \n",
    "IND_POR_WO = IND_POR_WO[columns_to_move + remaining_columns]\n",
    "\n",
    "#IND_POR_WO = IND_POR_WO.filter(regex='^(?!.*Comments$)')\n",
    "\n",
    "IND_POR_WO.to_csv(Path+'\\Output\\IND_POR_WO.csv',index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202dafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Open the file with the default application\n",
    "os.startfile(Path+'\\Output\\\\IND_POR_WO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################### Sourcing Action \n",
    "\n",
    "print('*************************************************')\n",
    "print('\\n')\n",
    "\n",
    "print('IND POR WO Sourcing Action Expoted in Output Folder\\n')\n",
    "\n",
    "\n",
    "print('*************************************************')\n",
    "continues = input('Press Enter Sourcing Action Done : ')\n",
    "\n",
    "\n",
    "###################################### Transit Month Updation\n",
    "#IND_POR = pd.read_csv(Path+'\\Output\\IND_POR_WO.csv',keep_default_na=False)\n",
    "IND_POR = pd.read_csv(Path+'\\Output\\IND_POR_WO.csv')\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "IND_POR['Country Code'] = IND_POR['Country Code'].fillna('NA')\n",
    "IND_POR['Bill to Country'] = IND_POR['Bill to Country'].fillna('NA')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(IND_POR.columns)\n",
    "\n",
    "\n",
    "Index_month = IND_POR.columns.get_loc(month)\n",
    "Index_Comments = IND_POR.columns.get_loc('Comments')\n",
    "\n",
    "Index_30_Start = Index_month -1\n",
    "Index_30_End = Index_Comments-1\n",
    "\n",
    "Index_60_Start = Index_month -2\n",
    "Index_60_End = Index_Comments -2\n",
    "\n",
    "Index_90_Start = Index_month -3\n",
    "Index_90_End = Index_Comments -3\n",
    "\n",
    "\n",
    "End_Month = IND_POR.columns[Index_30_End]\n",
    "End_Month_60 = IND_POR.columns[Index_60_End]\n",
    "End_Month_90 = IND_POR.columns[Index_90_End]\n",
    "\n",
    "\n",
    "IND_POR['Transit Month'] = IND_POR['Transit Month'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "condition_30 = (IND_POR['Data Series'].str.contains('NEW DEMAND',na=False)) & (IND_POR['Transit Month'] == 30)\n",
    "condition_60 = (IND_POR['Data Series'].str.contains('NEW DEMAND',na=False)) & (IND_POR['Transit Month'] == 60)\n",
    "condition_90 = (IND_POR['Data Series'].str.contains('NEW DEMAND',na=False)) & (IND_POR['Transit Month'] == 90)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if any row meets the condition\n",
    "if not IND_POR[condition_30].empty:\n",
    "    \n",
    "    cut_data = IND_POR.loc[condition_30].iloc[:, Index_month:Index_Comments].copy()\n",
    "    \n",
    "    IND_POR.loc[condition_30, IND_POR.columns[Index_30_Start:Index_30_End]] = cut_data.values\n",
    "\n",
    "    IND_POR.loc[condition_30, End_Month] = \"\"\n",
    "\n",
    "if not IND_POR[condition_60].empty:\n",
    "    \n",
    "    cut_data = IND_POR.loc[condition_60].iloc[:, Index_month:Index_Comments].copy()\n",
    "    \n",
    "    IND_POR.loc[condition_60, IND_POR.columns[Index_60_Start:Index_60_End]] = cut_data.values\n",
    "\n",
    "    IND_POR.loc[condition_60, End_Month] = \"\"\n",
    "    \n",
    "    IND_POR.loc[condition_60, End_Month_60] = \"\"\n",
    "\n",
    "if not IND_POR[condition_90].empty:\n",
    "    \n",
    "    cut_data = IND_POR.loc[condition_90].iloc[:, Index_month:Index_Comments].copy()\n",
    "    \n",
    "    IND_POR.loc[condition_90, IND_POR.columns[Index_90_Start:Index_90_End]] = cut_data.values\n",
    "\n",
    "    IND_POR.loc[condition_90, End_Month] = \"\"\n",
    "    \n",
    "    IND_POR.loc[condition_90, End_Month_60] = \"\"\n",
    "\n",
    "    IND_POR.loc[condition_90, End_Month_90] = \"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "############### Column Alignments\n",
    "\n",
    "\n",
    "columns_to_move = ['Demand Stream','Region1','Area2','Region','Theatre','Area','Country Name','Country Code','Bill to Country','Plant','Publish flag','Zone','Customer LOB','Product LOB','Product Range','Parent Model','Class','Description','DTF','DTF cutoff','Start Date','End Date','DTF risk units','DTF risk','Organization Code','MCID','Partial config','Transit Month','KeyAccount_Billing','Key Account','Master Customer','Item Type','ASP','Data Series']\n",
    "\n",
    "remaining_columns = [col for col in IND_POR.columns if col not in columns_to_move]\n",
    "    \n",
    "IND_POR = IND_POR[columns_to_move + remaining_columns]\n",
    "\n",
    "######################################## DTF End Date updation #########################\n",
    "try:\n",
    "    IND_POR['End Date'] = pd.to_datetime(IND_POR['End Date'])\n",
    "except:\n",
    "    try:\n",
    "        IND_POR['End Date'] = pd.to_datetime(IND_POR['End Date'], format='%m/%d/%Y')\n",
    "    except:\n",
    "        try:\n",
    "            IND_POR['End Date'] = pd.to_datetime(IND_POR['End Date'], format='mixed')\n",
    "        except:\n",
    "            IND_POR['End Date'] = pd.to_datetime(IND_POR['End Date'], format='%m-%d-%Y')\n",
    "\n",
    "\n",
    "IND_POR['Month Adjustment'] = IND_POR['End Date'].dt.strftime('%b-%y')\n",
    "\n",
    "\n",
    "for i, x in enumerate(IND_POR['Month Adjustment']):\n",
    "    if pd.isnull(x) or x not in IND_POR.columns:  # Check if x is nan or not in columns\n",
    "        continue  # Skip this iteration if x is nan or not a column name\n",
    "    Index_month = IND_POR.columns.get_loc(x)\n",
    "    IND_POR.iloc[i, 34:Index_month+1] = \"\"\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#print(IND_POR['Month Adjustment'])\n",
    "\n",
    "\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Month Adjustment$)')\n",
    "IND_POR = IND_POR.filter(regex='^(?!.*Comments$)')\n",
    "\n",
    "IND_POR.to_csv(Path+'\\Output\\IND_POR_W.csv',index=False)\n",
    "\n",
    "print('\\n')\n",
    "print('*************************************************\\n')\n",
    "print('         IND POR With Changes Completed     ')\n",
    "print('\\n*************************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################### Missing Configuation ##############################\n",
    "POR = pd.read_csv(Path+'\\Output\\IND_POR_WO.csv')\n",
    "\n",
    "POR['Country Code'] = POR['Country Code'].fillna('NA')\n",
    "\n",
    "###### Filter Publish flag / Partial config\n",
    "POR = POR[POR['Partial config'].str.contains('Yes',na=False) & POR['Publish flag'].str.contains('Y',na=False)]\n",
    "\n",
    "#POR.to_csv(Path+'\\Output\\Output.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Start_index = POR.columns.get_loc(month)\n",
    "End_index = Start_index + 9\n",
    "\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['Region', 'Class', 'Key Account', 'Country Code', 'Plant'] \n",
    "\n",
    "#POR = POR.groupby(columns_to_keep).agg({'Jul-24':'sum','Aug-24':'sum','Sep-24':'sum','Oct-24':'sum','Nov-24':'sum','Dec-24':'sum','Jan-25':'sum','Feb-25':'sum','Mar-25':'sum'})\n",
    "\n",
    "POR = POR.groupby(columns_to_keep).agg({POR.columns[Start_index]:'sum',POR.columns[Start_index+1]:'sum',POR.columns[Start_index+2]:'sum',POR.columns[Start_index+3]:'sum',POR.columns[Start_index+4]:'sum',POR.columns[Start_index+5]:'sum',POR.columns[Start_index+6]:'sum',POR.columns[Start_index+7]:'sum',POR.columns[Start_index+8]:'sum'})\n",
    "\n",
    "POR.to_csv(Path+'\\Output\\Missing Config.csv')\n",
    "\n",
    "POR = pd.read_csv(Path+'\\Output\\Missing Config.csv')\n",
    "\n",
    "# Sum the desired indices and store the result in a new column 'Total'\n",
    "POR['Total'] = POR.iloc[:, 5:14].sum(axis=1)\n",
    "\n",
    "POR = POR[POR['Total']>0]\n",
    "\n",
    "POR['CONC'] = POR['Class'].astype(str) +POR['Country Code']+POR['Key Account']\n",
    "\n",
    "####################### Load NFC\n",
    "NFC_Input = '\\\\Input\\\\NFC Tracker.xlsx'\n",
    "\n",
    "NFC = pd.read_excel(Path+NFC_Input,sheet_name='6M NFC')\n",
    "\n",
    "NFC = NFC[['CONC','Config Status']]\n",
    "\n",
    "NFC = NFC[NFC['Config Status'].str.contains('Completed',na=False)]\n",
    "\n",
    "Missing_Config = pd.merge(POR, NFC,on='CONC',how='left')\n",
    "\n",
    "Missing_Config = Missing_Config[Missing_Config['Config Status'].isnull()]\n",
    "\n",
    "Missing_Config = Missing_Config.filter(regex='^(?!.*CONC$)')\n",
    "Missing_Config = Missing_Config.filter(regex='^(?!.*Config Status$)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Missing_Config.loc[Missing_Config['Total']>20,'Status'] = 'Missing Config'\n",
    "Missing_Config.loc[Missing_Config['Total']<=20,'Status'] = 'Below 20'\n",
    "\n",
    "Missing_Config = Missing_Config.sort_values(by='Total',ascending=False)\n",
    "\n",
    "\n",
    "Missing_Config.to_csv(Path+'\\Output\\Missing Config.csv',index=False)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('*************************************************\\n')\n",
    "print('         Missing Configuration Completed     ')\n",
    "print('\\n*************************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5819540",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "################# NEW Demand\n",
    "###################################################################################\n",
    "\n",
    "IND_POR.rename(columns={'Region':'LOB Region'},inplace=True)\n",
    "IND_POR.rename(columns={'Area':'LOB Area'},inplace=True)\n",
    "IND_POR.rename(columns={'Region1':'Region'},inplace=True)\n",
    "IND_POR.rename(columns={'Area2':'Area'},inplace=True)\n",
    "\n",
    "IND_POR['CC-Cls+Ctry+KA'] = IND_POR['Class'].astype(str) + IND_POR['Country Code'] + IND_POR['Key Account']\n",
    "\n",
    "\n",
    "IND_POR = IND_POR[IND_POR['Publish flag'].str.contains('Y',na=False)]\n",
    "\n",
    "\n",
    "Start_index = IND_POR.columns.get_loc(month)\n",
    "\n",
    "\n",
    "\n",
    "# Define the columns to keep\n",
    "columns_to_keep = ['CC-Cls+Ctry+KA','Region','Area','LOB Region','LOB Area','Country Name','Country Code','Plant','Publish flag','Zone','Customer LOB','Product LOB','Product Range','Parent Model','Class','MCID','Partial config','Key Account','Transit Month'] \n",
    "\n",
    "\n",
    "\n",
    "# Replace blank values with the keyword \"blanks\"\n",
    "IND_POR[columns_to_keep] = IND_POR[columns_to_keep].replace('', 'dummy')\n",
    "IND_POR[columns_to_keep] = IND_POR[columns_to_keep].fillna(\"dummy\")\n",
    "\n",
    "# Define the columns to sum\n",
    "columns_to_sum = IND_POR.columns[Start_index:]\n",
    "\n",
    "# Create a dictionary for aggregation\n",
    "agg_dict = {col: 'sum' for col in columns_to_sum}\n",
    "\n",
    "# Group by the columns to keep and aggregate the columns to sum\n",
    "IND_POR = IND_POR.groupby(columns_to_keep).agg(agg_dict)\n",
    "\n",
    "# Save the result to a csv file\n",
    "IND_POR.to_csv(Path+'Output\\\\New Demand.csv')\n",
    "\n",
    "\n",
    "New_Demand = pd.read_csv(Path+'Output\\\\New Demand.csv')\n",
    "\n",
    "New_Demand['Country Code'] = New_Demand['Country Code'].fillna('NA')\n",
    "\n",
    "New_Demand[columns_to_keep] = New_Demand[columns_to_keep].replace('dummy', '')\n",
    "\n",
    "# Get the column names for the specified slice\n",
    "column_names = New_Demand.columns[19:26]\n",
    "\n",
    "# Fill NaN values with 0 before converting to integer\n",
    "for column in column_names:\n",
    "    #print(column)\n",
    "    New_Demand[column] = New_Demand[column].fillna(0).astype(int)\n",
    "\n",
    "# Calculate the sum and store it in the new column '7M'\n",
    "New_Demand['7M'] = New_Demand[column_names].sum(axis=1)\n",
    "\n",
    "\n",
    "# Get the column names for the specified slice\n",
    "column_names = New_Demand.columns[19:28]\n",
    "\n",
    "# Fill NaN values with 0 before converting to integer\n",
    "for column in column_names:\n",
    "    New_Demand[column] = New_Demand[column].fillna(0).astype(int)\n",
    "\n",
    "# Calculate the sum and store it in the new column '7M'\n",
    "New_Demand['9M'] = New_Demand[column_names].sum(axis=1)\n",
    "\n",
    "\n",
    "# Get the column names for the specified slice\n",
    "column_names = New_Demand.columns[19:31]\n",
    "\n",
    "# Fill NaN values with 0 before converting to integer\n",
    "for column in column_names:\n",
    "    New_Demand[column] = New_Demand[column].fillna(0).astype(int)\n",
    "\n",
    "# Calculate the sum and store it in the new column '7M'\n",
    "New_Demand['12M'] = New_Demand[column_names].sum(axis=1)\n",
    "\n",
    "# List of columns to delete\n",
    "columns_to_delete = ['CC-Cls+Ctry+KA.1']\n",
    "\n",
    "# Drop the specified columns\n",
    "New_Demand.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "# Save the result to a csv file\n",
    "New_Demand.to_csv(Path+'Output\\\\New Demand.csv',index=False)\n",
    "    \n",
    "print('\\n')\n",
    "print('*************************************************\\n')\n",
    "print('         New Demand Completed     ')\n",
    "print('\\n*************************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c670a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "################################# EDW ########################################\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "Azure = pd.read_csv(Path+'\\Output\\Azure.csv')\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "mask = Azure['ISO Country Code'].isnull()\n",
    "\n",
    "Azure.loc[mask,'ISO Country Code'] = 'NA'\n",
    "\n",
    "#print(Mix_Check_List.columns)\n",
    "  \n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class','Country Code'],keep='first')\n",
    "\n",
    "\n",
    "Azure = pd.merge(Azure, Mix_Check_List[['Country Code','Class','Plant']],left_on=['MCID Class','ISO Country Code'],right_on=['Class','Country Code'],how='left')\n",
    "\n",
    "\n",
    "#####################\n",
    "Mix_Check_List = pd.read_excel(Path+'input\\Mix Check List.xlsx',sheet_name='Publish Flag',keep_default_na=False)\n",
    "\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['As per Sourcing Matrix','Country Code']]\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Country Code'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "Azure = pd.merge(Azure, Mix_Check_List,left_on = 'ISO Country Code',right_on='Country Code',how='left')\n",
    "\n",
    "#Azure.to_csv(Path+'Azure1.csv',index=False)\n",
    "\n",
    "####################\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Plant')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Country Code','Class'],keep='first')\n",
    "\n",
    "\n",
    "\n",
    "mask = Mix_Check_List['Country Code'].isnull()\n",
    "\n",
    "Mix_Check_List.loc[mask,'Country Code'] = 'NA'\n",
    "\n",
    "#Azure.to_csv(Path+'Azure1.csv',index=False)\n",
    "\n",
    "Azure = pd.merge(Azure, Mix_Check_List[['Country Code','Class','Plant']],left_on=['Class','As per Sourcing Matrix'],right_on=['Class','Country Code'],how='left')\n",
    "\n",
    "#Azure.to_csv(Path+'Azure2.csv',index=False)\n",
    "\n",
    "\n",
    "mask = (Azure['Plant_x'].isnull() & Azure['Plant_y'].notnull())\n",
    "\n",
    "if mask.any():\n",
    "    Azure.loc[mask, 'Plant_x'] = Azure.loc[mask, 'Plant_y']\n",
    "\n",
    "Azure = Azure.rename(columns=lambda x: x.replace('Country Code_x', 'Country Code'))\n",
    "Azure = Azure.rename(columns=lambda x: x.replace('Plant_x', 'Plant'))\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*Country Code_y$)')\n",
    "Azure = Azure.filter(regex='^(?!.*Plant_y$)')\n",
    "\n",
    "\n",
    "##################### Publish Flag\n",
    "\n",
    "\n",
    "Mix_Check_List = pd.read_excel(Path+'Input\\Mix Check List.xlsx',sheet_name='Publish Flag')\n",
    "\n",
    "Mix_Check_List = Mix_Check_List[['Class','Publish flag']]\n",
    "\n",
    "Mix_Check_List = Mix_Check_List.drop_duplicates(subset=['Class'],keep='first')\n",
    "\n",
    "Azure = pd.merge(Azure, Mix_Check_List,left_on='MCID Class',right_on='Class',how='left')\n",
    "\n",
    "Azure.rename(columns={'ISO Country Code':'ISO_CC'},inplace=True)\n",
    "\n",
    "\n",
    "Azure = Azure.filter(regex='^(?!.*As per Sourcing Matrix$)')\n",
    "Azure = Azure.filter(regex='^(?!.*Country Code$)')\n",
    "Azure = Azure.filter(regex='^(?!.*Class_y$)')\n",
    "\n",
    "Azure.rename(columns={'ISO_CC':'ISO Country Code'},inplace=True)\n",
    "\n",
    "#print(Azure.columns)\n",
    "\n",
    "############\n",
    "Azure['CC-Cls+Ctry+KA'] = Azure['MCID Class'].astype(str) + Azure['ISO Country Code'] +Azure['KEY_ACCOUNT']\n",
    "\n",
    "Azure.rename(columns={'KEY_ACCOUNT':'Key Account'},inplace=True)\n",
    "Azure.rename(columns={'KEY_ACCOUNT':'Pblish Flag'},inplace=True)\n",
    "Azure.rename(columns={'Data Series':'Data series'},inplace=True)\n",
    "Azure.rename(columns={'6M Ship':'6M SHP'},inplace=True)\n",
    "Azure.rename(columns={'KEY_ACCOUNT':'Key Account'},inplace=True)\n",
    "Azure.rename(columns={'Plant':'Correct Org'},inplace=True)\n",
    "Azure.rename(columns={'Publish flag':'Pblish Flag'},inplace=True)\n",
    "\n",
    "Azure[['CC-Cls+Ctry+KA','ISO Country Code','Correct Org','Order Number','Order Type Name','Line Number','Shipment Number','Master Customer Number','Master Customer Name','Key Account','Pblish Flag','Offering Accounting Type Code','MCID Class','Product ID','Product Description','Order Booked Date','Line Request Date Time','Scheduled Ship Date','Scheduled Arrival Date','Actual Ship Date','Line Status','Data series','6M SHP','6M Order','Net Quantity','Warehouse Name','Organization Code','PID type','MCID-Net Order Value-US','MCID- MCC-US','Purchase Order Number','Invoice Trigger','Functional Group Short Name']].to_csv(Path+'\\Output\\EDW.csv',index=False)\n",
    "\n",
    "print('\\n')\n",
    "print('*************************************************\\n')\n",
    "print('         EDW Completed     ')\n",
    "print('\\n*************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b38cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Flash File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c9c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492dd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### Missing Configuation ##############################\n",
    "IND_PO = pd.read_csv(Path+'\\Output\\IND_POR_WO.csv')\n",
    "\n",
    "IND_PO['Country Code'] = IND_PO['Country Code'].fillna('NA')\n",
    "\n",
    "current_index = IND_PO.columns.get_loc(month)\n",
    "End_index = current_index + 6\n",
    "#print(current_index)\n",
    "#print(End_index)\n",
    "\n",
    "# Create new column [7M] and store the sum of values from current_index to End_index\n",
    "IND_PO['7M'] = IND_PO.iloc[:, current_index:End_index + 1].sum(axis=1)\n",
    "\n",
    "#IND_PO.to_csv(r'C:\\Vasanthan\\Projects\\Azure Vs IND POR\\Azure Vs IND POR\\Output\\Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "IND_PO = IND_PO[IND_PO['Publish flag']=='Y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Publish Flag Pivot\n",
    "pivot_total = IND_PO.pivot_table(index=['Plant','Class'], values='7M', aggfunc='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d260813",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Partial Config Pivot\n",
    "IND_PO = IND_PO[IND_PO['Partial config']=='Yes']\n",
    "pivot_Partial = IND_PO.pivot_table(index=['Plant','Class'], values='7M', aggfunc='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab44330",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flash_File = pd.merge(pivot_total, pivot_Partial , on=['Plant','Class'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flash_File['7M_y'] = Flash_File['7M_y'].fillna(0)\n",
    "Flash_File['7M_x'] = Flash_File['7M_x'].fillna(0)\n",
    "\n",
    "Flash_File['Mix'] =  (Flash_File['7M_y'] / Flash_File['7M_x']).round(2)\n",
    "\n",
    "######## Rename Column\n",
    "Flash_File.rename(columns={'7M_x' : '7M New Demand'}, inplace=True)\n",
    "Flash_File.rename(columns={'7M_y' : 'Miss configuration Qty'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Flash_File.to_csv(Path+'\\\\Input\\\\ref\\\\Flash_File.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e752375",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flash_File = pd.read_csv(Path+'Input\\\\ref\\\\Flash_File.csv')\n",
    "\n",
    "#Rename Chennai\n",
    "Flash_File.loc[Flash_File['Plant']=='CHS','Plant'] = 'CHE'\n",
    "\n",
    "\n",
    "pivot_Partial = Flash_File.pivot_table(index=['Plant','Class'], values=['7M New Demand','Miss configuration Qty','Mix'], aggfunc='sum')\n",
    "\n",
    "pivot_Partial = pivot_Partial[(pivot_Partial['Miss configuration Qty'] != 0) & (pivot_Partial['Miss configuration Qty'] != 0)]\n",
    "\n",
    "\n",
    "pivot_Partial.to_csv(Path+'Output\\\\Flash_File.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d0b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print('*************************************************\\n')\n",
    "print('         Flash File Completed     ')\n",
    "print('\\n*************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = Path+\"\\\\Output\"\n",
    "\n",
    "# Open the folder with the default file explorer\n",
    "os.startfile(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9446d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
