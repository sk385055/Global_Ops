{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio.windows_events import NULL\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime,timedelta\n",
    "from CTO_Shipment_mail import read_file,copy_mail,remove_mail\n",
    "import sys\n",
    "import os\n",
    "import pyodbc\n",
    "import time\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#today\n",
    "dt = datetime.now()\n",
    "date_str = dt.strftime(\"%Y%m%d\")\n",
    "#yesterday\n",
    "dt_yest = datetime.now() - timedelta(days=1)\n",
    "year_yest = dt_yest.strftime(\"%Y\")\n",
    "month_yest = dt_yest.strftime(\"%b\").upper()\n",
    "str_month = dt_yest.strftime(\"%Y%m\")\n",
    "#month in \n",
    "#------------------------------------------------------------------------------------------>start process\n",
    "def completions(name,file_name):\n",
    "    \n",
    "    if name=='enn':\n",
    "        out_file = 'completions_enn'  \n",
    "    elif name=='jab':\n",
    "        out_file = 'completions_jab'\n",
    "        mail_in_path = r'.\\CTO Shipments\\mail_src'\n",
    "        file_out_path = r'.\\rcv'\n",
    "        mail_out_path = r'.\\CTO Shipments\\mail_dst'\n",
    "    elif name =='usi':\n",
    "        out_file = 'completions_usi'\n",
    "    elif name =='gut':\n",
    "        out_file = 'completions_gut'\n",
    "        gut_columns = [\"Org\", \"ProductID\",\"SystemClosedDate\", \"Qty\"]\n",
    "    elif name == 'zeb':\n",
    "        out_file = 'completions_zeb'\n",
    "    in_file=file_name \n",
    "    col_map_file_name ='completions_col_map' \n",
    "    prod_grp_file_name='class_offerpf_map'\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # get the input file in rcv location \n",
    "    if name =='enn':\n",
    "        data = pd.read_excel(r'.\\rcv\\\\'+ in_file , sheet_name='Data_Source', engine='openpyxl') \n",
    "        data.columns = data.columns.str.strip( )\n",
    "    elif name=='jab':\n",
    "        in_file='jab_'+date_str\n",
    "        #CTO_shipments mail scraping process(CTO_shipment_mail.py file in completion folder)------>start\n",
    "        #reading eml in CTO Shipments\\mail_src folder\n",
    "        in_list_ctopath =glob.glob(mail_in_path+'\\*.eml')\n",
    "\n",
    "        for ctopath in in_list_ctopath:\n",
    "            str_ctopath = str(ctopath)\n",
    "            #get the file name only\n",
    "            cto_file_name = str_ctopath[25:]\n",
    "            #get the table inside the mail and convert to dataframe save in rcv folder\n",
    "            read_file(cto_file_name,file_out_path,mail_in_path)\n",
    "            #meanwhile take a copy file CTO Shipments\\mail_dst folder\n",
    "            copy_mail(cto_file_name,mail_in_path,mail_out_path)\n",
    "            #remove a file CTO Shipments\\mail_src folder\n",
    "            remove_mail(cto_file_name,mail_in_path)\n",
    "        #------------------------------------------------------------------------------------------------>end\n",
    "\n",
    "        # CTO shipments data append the Daily jabil shipments --------------------------------------------->start\n",
    "        data = pd.read_csv(r'.\\rcv\\CTO_shipement.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "        data = data.rename(columns={\"MC\":\"PN\",\"LastUpdated\":\"SHIPPED DATE\"})\n",
    "        #jabil update type column input\n",
    "        catalog_sht = pd.read_excel(r'.\\rcv\\lookup\\\\jabil_type.xlsx', sheet_name='Catalog', engine='openpyxl')\n",
    "       \n",
    "       \n",
    "        #---------------------------------------------------------------------------------------------->end\n",
    "        \n",
    "    elif name=='usi':\n",
    "        #concat the data of  Data ground shipments sheet and Air & Sea Shipments into single sheet----------------------->start\n",
    "        usi_sht=['Data ground shipments','Air & Sea Shipments']\n",
    "        data = pd.read_excel(r'.\\rcv\\\\'+ in_file, sheet_name=[usi_sht[0],usi_sht[1]], engine='openpyxl')\n",
    "        #remove blant in this dataframe columns and values\n",
    "        data[usi_sht[0]].columns = data[usi_sht[0]].columns.str.strip() \n",
    "        data[usi_sht[0]]['MODEL'] = data[usi_sht[0]]['MODEL'].str.strip()\n",
    "        # drop unnamed column which are in this dataframe columns\n",
    "        data[usi_sht[0]]=data[usi_sht[0]].loc[:,~data[usi_sht[0]].columns.str.match(\"Unnamed\")]\n",
    "        #change column names\n",
    "        dict = {'HAWB': 'TRACKING',\n",
    "        'Comments  / Comments Aditonal': 'Comments',\n",
    "        }\n",
    "        # # call rename () method column names\n",
    "        data[usi_sht[1]].rename(columns=dict,inplace=True)\n",
    "        data[usi_sht[1]]['Shipped Type']=usi_sht[1]\n",
    "        data[usi_sht[0]]['Shipped Type']=usi_sht[0]\n",
    "        #merge the 2 sheet into single dataframes\n",
    "        data = pd.concat([data[usi_sht[0]],data[usi_sht[1]]],axis=0)\n",
    "        #------------------------------------------------------------------------------------------------------------------>end\n",
    "    elif name=='gut':\n",
    "        #get the data in database------------------------------------------------------------------------------------------->start\n",
    "        data = pd.read_csv(r'E:\\_Projects\\completions\\rcv\\\\ERPMFGGUT-'+month_yest+'-'+year_yest+'.csv',delimiter='|')\n",
    "        print('ERPMFGGUT-'+month_yest+'-'+year_yest+'.csv')\n",
    "        # print(data.shape)\n",
    "        #for month end only\n",
    "        # data = pd.read_csv(r'\\\\wtc1501cifs\\cdunix\\ERP\\mpm\\fsdmfg\\prod\\TOOLBOX\\rcv\\\\ERPMFGGUT-APR-2023.csv',delimiter='|')\n",
    "        data = data.rename(columns={\"ORGANIZATION\": \"Org\",\"ASSEMBLY\":\"ProductID\",\"COMPLETION DATE\":\"SystemClosedDate\",\"COMPLETION QUANTITY\":\"Qty\"})\n",
    "        data = data[gut_columns]\n",
    "        #----------------------------------------------------------------------------------------------------------------------->end\n",
    "    elif name =='zeb':\n",
    "        in_file='zeb_'+date_str\n",
    "        data = pd.read_excel(r'.\\rcv\\\\7895_Comp.xlsx' , sheet_name='Report1', engine='openpyxl') \n",
    "        data.columns = data.columns.str.strip( )\n",
    "        data['Product ID'] = data['Product ID'].astype(str)\n",
    "        #only fillter startwith 7895\n",
    "        data = data[data['Product ID'].str.startswith(\"7895\")]\n",
    "        data['Product ID'] = data['Product ID'].str.slice(0,4)+'-'+data['Product ID'].str.slice(4,8)+'-'+data['Product ID'].str.slice(8,12)\n",
    "        data = data[~data['Order Type Name'].isin(['INTERNAL','REQ INTERNAL','RMA NOCREDIT-SHIPPED ONLY','RMA NOCREDIT-INVOICED ONLY','RMA WITH CREDIT'])]\n",
    "       \n",
    "    #-------------------------------------------------------------------------------------------------------   \n",
    "    # Reading lookup files----------------------------------------------------------------------------------------------->start        \n",
    "    #get the input file in col_map_file\n",
    "    col_map_file = pd.read_excel(r'.\\rcv\\lookup\\\\'+ col_map_file_name + '.xlsx', sheet_name='Sheet1', engine='openpyxl')\n",
    "    #take a required columns\n",
    "    col_map_file =col_map_file.iloc[:,0:13]\n",
    "    #get the input file in prod_rng_file\n",
    "    prod_rng_file = pd.read_csv(r'E:\\_Projects\\lookups\\\\'+ prod_grp_file_name + '.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "    #get the input file in bom_detail_file\n",
    "    bom_detail_file = pd.read_csv(r'.\\rcv\\lookup\\\\bom_details.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "    #IPM for ss21\n",
    "    ss21_file = pd.read_csv(r'.\\rcv\\lookup\\\\Inventory Parts Mgmt Report.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "    #7360 split for CHS\n",
    "    bom_chs = pd.read_csv(r'.\\rcv\\lookup\\\\BOM Explosion.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "\n",
    "    #Read Historical file\n",
    "    historical_file = pd.read_csv(r'E:\\_Projects\\_outputs\\completion\\\\'+out_file+'_hist.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "      \n",
    "    #--------------------------------------------------------------------------------------------------------------------end\n",
    "    #data cook as per completion----------------------------------------------------------------------------------------->start\n",
    "    data = pd.concat([data,col_map_file],axis=1)\n",
    "    if name =='enn':\n",
    "        data['Parent']= data['Parent'].astype(str)\n",
    "        data['Parent'] = data['Parent'].str.strip()\n",
    "        data['class'] = data['Parent'].str.slice(0,4)\n",
    "        data['item']= data['Parent']\n",
    "        data['site']=\"BUD\"\n",
    "        data['item_type']= np.where(data['Parent'].str.contains('M'),'M',data['item_type'])\n",
    "        data['item_type']= np.where(data['Parent'].str.contains('MCC'),'MCC',data['item_type'])\n",
    "        data['item_type']= np.where(data['Parent'].str.contains('-K'),'K',data['item_type'])\n",
    "        data['item_type']= np.where(data['Parent'].str.contains('-C'),'C',data['item_type'])\n",
    "        data['qty']=data['Quantity'].astype(int)\n",
    "        data['Shipped']=data['Shipped'].astype('datetime64[ns]')\n",
    "        data['ship_date']=data['Shipped']\n",
    "        data['ship_date_str'] = data['Shipped'].dt.strftime(\"%Y%m%d\")\n",
    "    elif name == 'jab':\n",
    "        data['PN']= data['PN'].astype(str)\n",
    "        data['PN'] = data['PN'].str.strip()\n",
    "        data['class'] = data['PN'].str.slice(0,4)\n",
    "        data['item']= data['PN']\n",
    "        data['site']=\"JAB\"\n",
    "        data['item_type']= np.where(data['PN'].str.contains('M'),'M',data['item_type'])\n",
    "        data['item_type']= np.where(data['PN'].str.contains('MCC'),'MCC',data['item_type'])\n",
    "        data['item_type']= np.where(data['PN'].str.contains('-K'),'K',data['item_type'])\n",
    "        data['item_type']= np.where(data['PN'].str.contains('-C'),'C',data['item_type'])\n",
    "        # data['SHIPPED DATE']= pd.to_datetime(data['SHIPPED DATE']).dt.date\n",
    "        data['SHIPPED DATE']= pd.to_datetime(data['SHIPPED DATE'],format='mixed').dt.date\n",
    "        data['SHIPPED DATE']= data['SHIPPED DATE'].astype('datetime64[ns]')\n",
    "        data['ship_date']=data['SHIPPED DATE']\n",
    "        data['qty']= data['Qty'].astype(int)\n",
    "        data['ship_date_str'] = data['SHIPPED DATE'].dt.strftime(\"%Y%m%d\")\n",
    "            \n",
    "    elif name =='usi':\n",
    "        data['MODEL']= data['MODEL'].astype(str)\n",
    "        data['MODEL'] = data['MODEL'].str.strip()\n",
    "        data['class'] = data['MODEL'].str.slice(0,4)\n",
    "        data['item']= data['MODEL']\n",
    "        data['site']=\"USI\"\n",
    "        data['item_type']= np.where(data['MODEL'].str.contains('M'),'M',data['item_type'])\n",
    "        data['item_type']= np.where(data['MODEL'].str.contains('MCC'),'MCC',data['item_type'])\n",
    "        data['item_type']= np.where(data['MODEL'].str.contains('-K'),'K',data['item_type'])\n",
    "        data['item_type']= np.where(data['MODEL'].str.contains('-C'),'C',data['item_type'])\n",
    "        data['qty']=data['QTY'].astype(int)\n",
    "        data['SHIPPING DATE']=data['SHIPPING DATE'].astype('datetime64[ns]')\n",
    "        data['ship_date']=data['SHIPPING DATE']\n",
    "        data['ship_date_str'] = data['SHIPPING DATE'].dt.strftime(\"%Y%m%d\")\n",
    "        \n",
    "    elif name =='gut':\n",
    "        data['ProductID'] = data['ProductID'].astype(str)\n",
    "        data['ProductID'] = data['ProductID'].str.strip()\n",
    "        data['class'] = data['ProductID'].str.slice(0,4)\n",
    "        data['item']= data['ProductID']\n",
    "        data['site']= data['Org']\n",
    "        data['item_type']= np.where(data['ProductID'].str.contains('M'),'M',data['item_type'])\n",
    "        data['item_type']= np.where(data['ProductID'].str.contains('MCC'),'MCC',data['item_type'])\n",
    "        data['item_type']= np.where(data['ProductID'].str.contains('-K'),'K',data['item_type'])\n",
    "        data['item_type']= np.where(data['ProductID'].str.contains('-C'),'C',data['item_type'])\n",
    "        data['qty']=data['Qty']\n",
    "        data['SystemClosedDate']=data['SystemClosedDate'].astype('datetime64[ns]')\n",
    "        data['ship_date']=data['SystemClosedDate']\n",
    "        data['ship_date_str'] = data['SystemClosedDate'].dt.strftime(\"%Y%m%d\")\n",
    "        # print(date_str)\n",
    "        # data = data[~data['ship_date_str'].isin([date_str])]\n",
    "        # data.drop(columns=gut_columns, axis=1,inplace=True)\n",
    "        \n",
    "    elif name == 'zeb':\n",
    "        data['file_ref'] = in_file\n",
    "        data['site']=\"ZEB\"\n",
    "        data['item'] = data['Product ID']\n",
    "        data['item'] = data['item'].str.strip()\n",
    "        data['class']= data['item'].str.slice(0,4)\n",
    "        data['item_type']= np.where(data['item'].str.contains('-K'),'K','M')\n",
    "        data['qty'] = data['Net Quantity']\n",
    "        data['Actual Ship Date']  = data['Actual Ship Date'].astype('datetime64[ns]')\n",
    "        data['ship_date'] = data['Actual Ship Date']\n",
    "        data['ship_date_str'] = data['Actual Ship Date'].dt.strftime(\"%Y%m%d\")\n",
    "        data['unit']= np.where(data['item_type'] =='M','unit','non-unit')\n",
    "    data['file_ref']=in_file\n",
    "    data['unit']= np.where(data['item_type'] =='M','unit','non-unit')\n",
    "    data['ship_date_str'] = data['ship_date_str'].astype(str)\n",
    "    data['create_date']= date_str\n",
    "    data['fyear'] = data['ship_date_str'].str.slice(0,4)\n",
    "    data = data[~data['ship_date_str'].isin([date_str])]\n",
    "    data = data[data['fyear'].isin([year_yest])]\n",
    "    # print(data.shape)\n",
    "    #------------------------------------------------------------------------------------------------------->end\n",
    "    #update the week,month, qtr in this dataframe\n",
    "    def upd_period(df, fld, fld_wk, fld_mon, fld_qtr):\n",
    "        df[fld_wk] = pd.to_datetime(df[fld], errors='coerce').dt.isocalendar().week.astype(pd.Int64Dtype())\n",
    "        df[fld_mon] = pd.to_datetime(df[fld], errors='coerce').dt.strftime('%Y%m')\n",
    "        df[fld_qtr] = pd.to_datetime(df[fld], errors='coerce').dt.strftime('%Y') + 'Q' + pd.to_datetime(data[fld], errors='coerce').dt.quarter.astype(pd.Int64Dtype()).astype(str)    \n",
    "    upd_period(data, 'ship_date_str','ship_date_wk','ship_date_mth','ship_date_qtr')\n",
    "    data = data[data['ship_date_mth'].isin([str_month])]\n",
    "    # print(data.shape)\n",
    "   #--------------------------------------------------------------------------------------------\n",
    "   # update product group/range\n",
    "    rng_fld_col=['range','prod_grp_wb','offer_pf_wb']\n",
    "    catalog_fld_col =['type']\n",
    "    data = data.merge(prod_rng_file[['class']+rng_fld_col], on='class', how='left',validate='m:1')\n",
    "    #ss21 for lpm\n",
    "    ss21_file = ss21_file[ss21_file['Inventory Item Description'].str.startswith(\"SelfServ21\") | ss21_file['Inventory Item Description'].str.startswith(\"SelfServ 21\") ]\n",
    "    ss21_file['ss21'] ='Y' \n",
    "    ss21_file = ss21_file.rename(columns={\"Inventory Item\": \"item\"})\n",
    "    ss21_file = ss21_file[['item','ss21']]\n",
    "    ss21_file = ss21_file.drop_duplicates()\n",
    "    ss21_fld_col =['ss21']\n",
    "    data = data.merge(ss21_file[['item']+ss21_fld_col], on='item', how='left',validate='m:1')\n",
    "    data['ss21']= np.where(data['ss21']==\"Y\", data['ss21'],'N')\n",
    "\n",
    "    #bom details file\n",
    "    bom_detail_file =  bom_detail_file.rename(columns={\"Dom/Export\":\"dom_exp\",\"mcid\":\"item\"})\n",
    "    rng_fld_bom_col = ['dom_exp']\n",
    "    data = data.merge(bom_detail_file[['item']+rng_fld_bom_col], on='item', how='left',validate='m:1')\n",
    "\n",
    "    # update type column in completion data---------------------------------------------------------------------->start\n",
    "\n",
    "    if name=='jab':\n",
    "        catalog_sht =  catalog_sht.rename(columns={\"MC\": \"PN\",\"Type\":\"type\"})\n",
    "        data = data.merge(catalog_sht[['PN']+catalog_fld_col], on='PN', how='left',validate='m:1')\n",
    "        data['type']= np.where(data['type']=='7360 Narrow','7360N',data['type'])\n",
    "        data['type']= np.where(data['type']=='7360 Jarvis','7360J',data['type'])\n",
    "        data['type'] = np.where(((data['class']=='7360') & (data['type'].isnull())),'7360S',data['type'])\n",
    "\n",
    "        \n",
    "    elif name =='gut':\n",
    "        bom_chs =  bom_chs.rename(columns={\"Top Parent Inventory Item\": \"item\",\"Parent Inventory Item\":\"Assembly\"})\n",
    "        bom_chs =bom_chs[['item','Assembly']]\n",
    "        bom_chs = bom_chs[bom_chs['Assembly'].isin(['7360-F001','7360-F002','7360-F003'])]  \n",
    "        bom_chs = bom_chs[~bom_chs['item'].isin(['7360MC','7360MCC','7360M'])] \n",
    "        bom_chs = bom_chs.drop_duplicates()\n",
    "        bom_chs['type'] = ''\n",
    "        bom_chs['type']= np.where(bom_chs['Assembly']=='7360-F002','7360N',bom_chs['type'])\n",
    "        bom_chs['type']= np.where(bom_chs['Assembly']=='7360-F003','7360J',bom_chs['type'])\n",
    "        bom_chs['type']= np.where(bom_chs['Assembly']=='7360-F001','7360S',bom_chs['type'])\n",
    "        data = data.merge(bom_chs[['item']+catalog_fld_col], on='item', how='left',validate='m:1')\n",
    "\n",
    "    else:\n",
    "        data[catalog_fld_col]=''\n",
    "    # data.to_csv(r'.\\snd\\check.csv',index=False)\n",
    "    # input(\"Break.....................\")\n",
    "    #------------------------------------------------------------------------------------------------------------>end\n",
    "    #export the histroical file\n",
    "    # delete dates which are in input file from histical and append it\n",
    "    historical_file['ship_date_str'] = historical_file['ship_date_str'].astype(str)\n",
    "    date_list = data['ship_date_str'].unique()\n",
    "    historical_file = historical_file[~historical_file['ship_date_str'].isin(date_list)]\n",
    "    historical_file = pd.concat([historical_file,data],axis=0)\n",
    "    historical_file.to_csv(r'E:\\_Projects\\_outputs\\completion' + '\\\\' + out_file +'_hist.csv',encoding='utf-8', index=False) \n",
    "    #historical_file.to_csv(r'\\\\Dayorg1\\ORGSHARE\\TEAMS\\ERP Shared Folder\\Global S&OP\\workbench\\completions' + '\\\\' + out_file +'.csv',encoding='utf-8', index=False) \n",
    "    #-----------------------------------------------------------------------------------------   \n",
    "   \n",
    "    # filter completion neccesary columns\n",
    "    req_col = list(col_map_file.columns)+rng_fld_col+ss21_fld_col+rng_fld_bom_col+catalog_fld_col\n",
    "    data= data[req_col]\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # export into csv ->eg: name = completion_plantname.csv\n",
    "    #local save\n",
    "    data.to_csv(r'E:\\_Projects\\_outputs\\completion' + '\\\\' + out_file +'.csv',encoding='utf-8', index=False)\n",
    "\n",
    "  \n",
    "# take some Example on gived last line                     \n",
    "if __name__ == \"__main__\":\n",
    "    print('Filter only '+str_month+' month records')\n",
    "    sources =['enn','jab','usi','gut','zeb']\n",
    "    #get passing second arguments\n",
    "    in_list = sys.argv[2:]\n",
    "    #get passing first arguments\n",
    "    in_date = sys.argv[1]\n",
    "    #get the list of all xlsx file in below folder\n",
    "    files_list = glob.glob(r'.\\rcv\\\\'+'\\*xlsx')\n",
    "    #searching the file name as per you passing the plants arguments\n",
    "    file_search_str ={\"enn\":\"Shipped\",\"jab\":\"CTO_shipement\",\"gut\":\"gut\",\"usi\":\"USI Outbound tracker\",\"zeb\":\"zeb\"}\n",
    "    # if you pass the second arguments as 'all' it automatically run all the plant['enn','jab','usi','gut','zeb']------>start\n",
    "    if in_list[0]=='all':\n",
    "        for name in sources:\n",
    "            if name == 'gut':\n",
    "                file_name = name+date_str\n",
    "                completions(name,file_name)\n",
    "            elif name == 'zeb':\n",
    "                file_name = name+date_str\n",
    "                print(file_name)\n",
    "                completions(name,file_name)\n",
    "            elif name =='jab':\n",
    "                file_name = name+date_str\n",
    "                print(file_name)\n",
    "                completions(name,file_name)\n",
    "\n",
    "            else:  \n",
    "                for file in files_list: \n",
    "                    str_file = str(file)\n",
    "                    file_name = os.path.basename(file)\n",
    "                    if file_name.__contains__(file_search_str[name]) & file_name.__contains__(in_date):\n",
    "                        print(file_name)\n",
    "                        completions(name,file_name)  \n",
    "        #----------------------------------------------------------------------------------------------------------------->end \n",
    "        # if you pass the decond arguments as particullar plant it run you passing aruguments only ------------------------>start                 \n",
    "    else:\n",
    "        for name in in_list:\n",
    "            if name == 'gut':\n",
    "                file_name = name+date_str\n",
    "                completions(name,file_name)\n",
    "            elif name == 'zeb':\n",
    "                file_name = name+date_str\n",
    "                print(file_name)\n",
    "                completions(name,file_name)\n",
    "            elif name =='jab':\n",
    "                file_name = name+date_str\n",
    "                print(file_name)\n",
    "                completions(name,file_name)\n",
    "\n",
    "            else:\n",
    "                for file in files_list:\n",
    "                    str_file = str(file)\n",
    "                    file_name = os.path.basename(file)\n",
    "                    if file_name.__contains__(file_search_str[name]) & file_name.__contains__(in_date): \n",
    "                        print(file_name)\n",
    "                        completions(name,file_name)\n",
    "        #---------------------------------------------------------------------------------------------------------------------->end\n",
    "    #How to pass the argument?\n",
    "    #run the command prompt or power shell -> type the below text\n",
    "    #1. first argument ->date ->should be 'Ymd' Eg: 20230123\n",
    "    #2. second argument -> plant name ->should be [enn jab usi gut zeb] only\n",
    "    # if you pass second argument all it run all palnt in default----->    eg: python completion.py 20230123 all\n",
    "    # else you mention particular plants it run particular plant only-->   eg: python completion.py 20230123 enn jab usi zeb gut\n",
    "    # full ex\\mple-> python completion.py first argument second argument\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                     \n",
    "                    \n",
    "      \n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
