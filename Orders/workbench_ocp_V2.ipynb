{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab8e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pandas.tseries.offsets import BDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aea908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# read OCP IA: if India Time on or before 3PM, then read EUMOCP else AMMOCP\n",
    "# This needs to be changed - script should run as soon the new file is available\n",
    "import pytz\n",
    "from datetime import datetime, timedelta,date\n",
    "d = datetime.now(pytz.timezone('Asia/Calcutta'))\n",
    "if d.hour <= 15:\n",
    "    file = 'OCP_IA_EUMOCP'\n",
    "else:\n",
    "    file = 'OCP_IA_AMMOCP'\n",
    "    \n",
    "d = d.strftime('%d-%b-%Y').upper()\n",
    "\n",
    "try:\n",
    "    path = r'\\\\susday014.prod.local\\cdunix\\ERP\\RR\\Inbound\\OCP\\IA\\rcv'\n",
    "    # path = r'E:\\_Projects\\orders\\rcv'\n",
    "except:\n",
    "    try:\n",
    "        path = r'E:\\_Projects\\orders\\rcv'\n",
    "    except:\n",
    "        print('EUMOCP File Not found')\n",
    "    \n",
    "# file = 'OCP_IA_EUMOCP'\n",
    "file = 'OCP_IA_EUMOCP'\n",
    "#file = 'OCP_IA_AMMOCP'\n",
    "\n",
    "#New Script\n",
    "%run Orders.ipynb\n",
    "d = EUMOCP_File_Name\n",
    "\n",
    "#Old\n",
    "#d = '03-FEB-2025'\n",
    "\n",
    "data = pd.read_excel(path + '\\\\' + file + '_' + d + '.xlsx', sheet_name='Sheet1', engine='openpyxl', \\\n",
    "                     na_values='', keep_default_na=True)\n",
    "\n",
    "print(file + '_' + d + '.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff84a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bkp = data.copy()\n",
    "# data = data_bkp.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc709f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude ERP LINE STATUS = Closed (Updated dt 12-Jun-2023)\n",
    "data = data[data['ERP Line Status'] != 'Closed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Product group and Offer PF; +2 cols\n",
    "prod = pd.read_csv(r'E:\\_Projects\\lookups\\class_offerpf_map.csv', \\\n",
    "                   encoding='utf-8', low_memory=False)\n",
    "data['Class'] = data['Class'].astype(str)\n",
    "data = pd.merge(data, prod[['class', 'prod_grp_wb','offer_pf_wb','division_wb']], left_on='Class', right_on='class', how='left',validate='m:1')\n",
    "data[['Products', 'Offer PF','LOB Prod']] = data[['prod_grp_wb', 'offer_pf_wb', 'division_wb']]\n",
    "data.drop(columns={'class', 'offer_pf_wb','prod_grp_wb','division_wb'}, axis=1, inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff3d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update MCID TAGGING name from RACE; +1 col\n",
    "tag = pd.read_excel(r'E:\\_Projects\\lookups\\MCID_Tagging _Master.xlsx', sheet_name='MCID_Tagging__Master_TABLE' \\\n",
    "                   , engine='openpyxl')\n",
    "tag.drop_duplicates(inplace=True,subset=['MCID'],keep='first')\n",
    "data = pd.merge(data, tag[['MCID','Tag Name']], left_on='PID', right_on='MCID', how='left',validate='m:1')\n",
    "data.drop(columns={'MCID'}, axis=1, inplace=True)   \n",
    "data['Tag Name'] = np.where(data['Tag Name'].isnull(),data['PID Desc'],data['Tag Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pref offer\n",
    "#pref_offer = pd.read_csv(r'E:\\_Projects\\lookups\\preferred_offer.csv', \\\n",
    "#                    encoding='utf-8', low_memory=False)\n",
    "#pref_offer_grpd = pref_offer.groupby('PID')['Type'].max()\n",
    "#data = pd.merge(data, pref_offer_grpd, left_on='PID', right_on='PID', how='left',validate='m:1')\n",
    "#data.rename(columns={'Type' : 'Preferred Offer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b30ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prod Cate: 7360 category as Std, Narrow Core, Jarvis; 2012 vs 2021\n",
    "# condtn?\n",
    "data['Prod Cate'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update DOM / EXPORT for chennai\n",
    "# Delete SS21, Dom/Export and start using Prod Cate itself\n",
    "# \n",
    "df_dom_exp = pd.read_csv(r'E:\\_Projects\\lookups\\bom_details.csv', \\\n",
    "                        encoding='utf-8', low_memory=False)\n",
    "df_dom_exp_grpd = df_dom_exp.groupby('mcid')[['Dom/Export']].max()\n",
    "data = pd.merge(data, df_dom_exp_grpd, left_on='PID', right_on='mcid', how='left',validate='m:1')\n",
    "\n",
    "data['Prod Cate'] = np.where( (data['Dom/Export'] != '') & (data['Whse'].isin(['CHE','CHS'])), \\\n",
    "                                    data['Class'] + data['Dom/Export'].str[:1], '')\n",
    "# data['Prod Cate'] = np.where(data['Prod Cate'] != '' & data['Dom/Export'] != '', \\\n",
    "#                              data['Prod Cate'] + ';' + data['Dom/Export'], \\\n",
    "#                              data['Dom/Export'])\n",
    "data.drop(columns={'Dom/Export'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4daaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on PID Desc like SelfServ*21* \n",
    "# based on TAG Name like *SS21*\n",
    "data['IsSS21']=''\n",
    "data['IsSS21'] = np.where((data['Class']=='2012') & (data['PID Desc'].str.contains('&'.join(['SelfServ','21']))==True), \\\n",
    "                          'SS21', '')\n",
    "data['IsSS21'] = np.where((data['Class']=='2012') & (data['Tag Name'].str.endswith('SS21')==True), 'SS21', data['IsSS21'])\n",
    "data['Prod Cate'] = np.where(data['Prod Cate'].notnull() & data['IsSS21'].notnull(), data['Prod Cate']+';'+data['IsSS21'], \\\n",
    "                            data['IsSS21'])\n",
    "data.drop(columns={'IsSS21'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c56ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PLANT field\n",
    "\n",
    "incl_type = ['PO','PD','Planned Buy', 'Planned Make','Planned order','Flow']\n",
    "pid_type = ['M','K','C']\n",
    "data['Plant'] = ''\n",
    "conditions = [ (data['Demand']!='WIP OSP') & data['Whse'].isin(['CHE','CHS']) , \\\n",
    "             (data['Demand']!='WIP OSP') & data['Whse'].isin(['USL']) & \\\n",
    "                  ~data['Supplier'].isin(['UNIVERSAL SCIENTIFIC INDUSTRIAL DE MEXICO S.A. DE C.V']) , \\\n",
    "             (data['Demand']!='WIP OSP') & data['Whse'].isin(['US','USL','GF1'])  & \\\n",
    "                  data['Supplier'].isin(['UNIVERSAL SCIENTIFIC INDUSTRIAL DE MEXICO S.A. DE C.V']) , \\\n",
    "             (data['Demand']!='WIP OSP') & \\\n",
    "                  data['Supplier'].isin(['ENNOCONN HUNGARY KFT'])\n",
    "             ]\n",
    "choices = ['CHE','USL','USI','ENN']\n",
    "data['Plant'] = np.select(conditions, choices, default='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1521e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PO Number update for ENN, JAB, USI ************************************************************************\n",
    "lst_splr = ['UNIVERSAL SCIENTIFIC INDUSTRIAL DE MEXICO S.A. DE C.V', \\\n",
    "                'UNIVERSAL SCIENTIFIC INDUSTRIAL DE MEXICO S.A. DE C.V', \\\n",
    "                'ENNOCONN HUNGARY KFT']\n",
    "lst_type = ['PD','PO']\n",
    "\n",
    "condtn1 = data['Supplier'].isin(lst_splr)\n",
    "condtn3 = data['Supply Type'].isin(lst_type)\n",
    "\n",
    " # Trim Supply Detail (PO) for OCP-IA records\n",
    "data['Supply PO'] = np.where(condtn1 & condtn3, \\\n",
    "                                   data['Supply Detail'].str.split('(').str[0].copy(), '')\n",
    "\n",
    "# bring in IO/PO field\n",
    "data['IO/PO No.'] = np.where(data['Supply PO']!='', data['Supply PO'], data['IO No.'])\n",
    "\n",
    "data['PO No.'] = data['Supply PO']\n",
    "data.drop(columns={'Supply PO'}, inplace=True)\n",
    "\n",
    "# col = data.pop('IO/PO No.')\n",
    "# data.insert(33, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting DATE fields\n",
    "data['Booked Dt'] = pd.to_datetime(data['Booked Dt'], format='%Y-%m-%d')\n",
    "data['Request Dt'] = pd.to_datetime(data['Request Dt'], format='%Y-%m-%d', errors='coerce')\n",
    "data['PD_Order'] = pd.to_datetime(data['PD_Order'], format='%Y-%m-%d', errors='coerce')\n",
    "data['Ship Date'] = pd.to_datetime(data['Ship Date'], format='%Y-%m-%d')\n",
    "data['SAD'] = pd.to_datetime(data['SAD'], format='%Y-%m-%d')\n",
    "\n",
    "# data['IO BOOKED DATE'] = pd.to_datetime(data['IO BOOKED DATE'], format='%d-%m-%Y %I:%M:%S %p')\n",
    "data['IO BOOKED DATE'] = pd.to_datetime(data['IO BOOKED DATE'], format='%Y-%m-%d')\n",
    "data['IO BOOKED DATE'] = data['IO BOOKED DATE'].dt.date\n",
    "data['IO SSD'] = pd.to_datetime(data['IO SSD'], format='%Y-%m-%d')\n",
    "data['IO SAD'] = pd.to_datetime(data['IO SAD'], format='%Y-%m-%d')\n",
    "\n",
    "data['PD_PO'] = pd.to_datetime(data['PD_PO'], format='%Y-%m-%d')\n",
    "data['CRSD'] = pd.to_datetime(data['CRSD'], format='%Y-%m-%d')\n",
    "\n",
    "#data['PRD'] = pd.to_datetime(data['PRD'], format='mixed').dt.date()\n",
    "data['PRD'] = pd.to_datetime(data['PRD'], format='%Y-%m-%d')\n",
    "# data['PRD'] = pd.to_datetime(data['PRD'], format='%d-%m-%Y')\n",
    "data['Plant Ship date'] = pd.to_datetime(data['Plant Ship date'], format='%Y-%m-%d')\n",
    "data['NEED BY DATE'] = pd.to_datetime(data['NEED BY DATE'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51051264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #************************ Update IO/PO SSD *************************************\n",
    "data['IO/PO SSD'] = ''\n",
    "data['IO/PO SSD'] = np.where(data['Supplier'].isin(lst_splr) \\\n",
    "                             & data['PD_PO'].notnull() & ~data['Plant'].isin(['']), \\\n",
    "                             data['PD_PO'], data['IO/PO SSD'])\n",
    "\n",
    "data['IO/PO SSD'] = np.where(data['Supplier'].isin(lst_splr) \\\n",
    "                             & data['NEED BY DATE'].notnull() & data['PD_PO'].isnull() & ~data['Plant'].isin(['']), \\\n",
    "                             data['NEED BY DATE'], data['IO/PO SSD'])\n",
    "\n",
    "data['IO/PO SSD'] = np.where(data['Supplier'].isin(lst_splr) \\\n",
    "                             & data['IO SSD'].notnull() & data['PD_PO'].isnull() & data['NEED BY DATE'].isnull() & ~data['Plant'].isin(['']), \\\n",
    "                             data['IO SSD'], data['IO/PO SSD'])\n",
    "\n",
    "\n",
    "\n",
    "#******** less Transit days for USI: 5 days\n",
    "data['IO/PO SSD'] = np.where(~data['IO/PO SSD'].isin(['']) &data['PD_PO'].notnull() & data['Plant'].isin(['USI']), \\\n",
    "                             pd.to_datetime(data['IO/PO SSD']) - BDay(5), data['IO/PO SSD'])\n",
    "\n",
    "\n",
    "\n",
    "# #*************** Update Plant SSD as IO SSD else Ship Date (for OTHER THAN jabil, ennoconn, usi)\n",
    "data['IO/PO SSD'] = np.where((~data['Supplier'].isin(lst_splr)) \\\n",
    "                             & data['IO SSD'].notnull() & ~data['Plant'].isin(['']), \\\n",
    "                             data['IO SSD'], data['IO/PO SSD'])\n",
    "\n",
    "data['IO/PO SSD'] = pd.to_datetime(data['IO/PO SSD'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #************************ Update Plant SSD *************************************\n",
    "cond = (data['IO/PO SSD'].isin(['']) | data['IO/PO SSD'].isnull()) & ~(data['Plant'].isin(['']))\n",
    "data['Plant SSD'] = np.where(cond,data['Ship Date'],data['IO/PO SSD'])\n",
    "data['Plant SSD'] = pd.to_datetime(data['Plant SSD'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#func to popualte WK, MON, QTR\n",
    "def upd_period(df, fld, fld_wk, fld_mon, fld_qtr):\n",
    "    df[fld_wk] = pd.to_datetime(df[fld], errors='coerce').dt.isocalendar().week.astype(pd.Int64Dtype())\n",
    "    df[fld_mon] = pd.to_datetime(df[fld], errors='coerce').dt.strftime('%Y%m')\n",
    "    df[fld_qtr] = pd.to_datetime(df[fld], errors='coerce').dt.strftime('%Y') + 'Q' + \\\n",
    "                        + pd.to_datetime(data[fld], errors='coerce').dt.quarter.astype(pd.Int64Dtype()).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#align PAST dates into last week \n",
    "#Sri made changes on 07-Aug-23\n",
    "def align_past(fld):\n",
    "    data[fld] = data[fld].astype('datetime64[ns]')\n",
    "    pre_week = date.today() - timedelta(days=1)\n",
    "    #data['Previous_week_Date'] = pre_week- timedelta(days = pre_week.weekday())\n",
    "    data['Previous_week_Date'] = pre_week\n",
    "    data['Previous_week_Date'] = data['Previous_week_Date'].astype('datetime64[ns]')\n",
    "#     data['final'] =  data[fld]\n",
    "#     cond = data['final'] < datetime.today()\n",
    "    cond = data[fld] < pd.to_datetime('today')\n",
    "    data[fld] = np.where(cond, data['Previous_week_Date'], data[fld])\n",
    "    data[fld] = data[fld].astype('datetime64[ns]')\n",
    "    \n",
    "#align_past('Ship Date')\n",
    "upd_period(data, 'Ship Date', 'Ship Dt Wk', 'Ship Dt Mon', 'Ship Dt Qtr')\n",
    "#data.drop(columns={'Previous_week_Date'}, axis=1, inplace=True)\n",
    "\n",
    "#align_past('IO SSD')\n",
    "upd_period(data, 'IO SSD', 'IO SSD Wk', 'IO SSD Mon', 'IO SSD Qtr')\n",
    "#data.drop(columns={'Previous_week_Date'}, axis=1, inplace=True)\n",
    "\n",
    "align_past('IO/PO SSD')\n",
    "upd_period(data, 'IO/PO SSD', 'IO/PO SSD Wk', 'IO/PO SSD Mon', 'IO/PO SSD Qtr')\n",
    "data.drop(columns={'Previous_week_Date'}, axis=1, inplace=True)\n",
    "\n",
    "align_past('Plant SSD')\n",
    "upd_period(data, 'Plant SSD', 'Plant SSD Wk', 'Plant SSD Mon', 'Plant SSD Qtr')\n",
    "data.drop(columns={'Previous_week_Date'}, axis=1, inplace=True)\n",
    "           \n",
    "upd_period(data, 'PRD', 'PRD Wk', 'PRM', 'PRQ')\n",
    "data.drop(columns={'PRD Wk'}, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hari changed New PRD GRP formulae 28-06-2023\n",
    "def update_prd_group(fld):\n",
    "    cond = (data[fld]!= '') & (data[fld].astype(str) == data['PRQ'].astype(str))\n",
    "    cond1 =  (data[fld]!= '') & (data['PRQ'].astype(str) > data[fld].astype(str))\n",
    "    cond2 =  (data[fld]!= '') & (data['PRQ'].astype(str) < data[fld].astype(str))\n",
    "    data['PRD Grp'] = np.where(cond,'PRD Curr', np.where(cond1,'PRD Fut', np.where(cond2,'PRD Past','')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PRD Grp']=''\n",
    "data['Plant SSD Qtr'] = data['Plant SSD Qtr'].replace(np.nan, '', regex=True)\n",
    "data['Ship Dt Qtr'] = data['Ship Dt Qtr'].replace(np.nan, '', regex=True)\n",
    "update_prd_group('Plant SSD Qtr')\n",
    "#update_prd_group('Ship Dt Qtr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe4880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e0d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling KA from Oracle SQL...........!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vv185114\\AppData\\Local\\Temp\\2\\ipykernel_15460\\2914451015.py:29: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  cust = pd.read_sql(query, con=conn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Key Account', 'MASTER_CUST_ID', 'CREATION_DATE', 'CREATED_BY',\n",
       "       'LAST_UPDATE_DATE', 'LAST_UPDATED_BY', 'LAST_UPDATE_LOGIN',\n",
       "       'Master Customer Number', 'MASTER_CUST_INDST', 'Master Customer Name',\n",
       "       'KEY_ACCOUNT_ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################# PUll KA File from Orale SQL #########################\n",
    "#KA = pd.read_excel(Path+'Input\\SQL.xlsx',keep_default_na=False)\n",
    "import oracledb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "SQL_Data = 0\n",
    "\n",
    "if SQL_Data == 0:\n",
    "    \n",
    "    print('Pulling KA from Oracle SQL...........!')\n",
    "\n",
    "    # Enable thick mode\n",
    "    oracledb.init_oracle_client(lib_dir=r\"E:\\softwares\\instantclient-basic-windows.x64-23.5.0.24.07\\instantclient_23_5\")\n",
    "\n",
    "    # Connection details\n",
    "    dsn_tns = oracledb.makedsn('153.84.75.216', 1521, service_name='ebs_ATMPROD')\n",
    "\n",
    "\n",
    "    # Use a context manager to handle the connection\n",
    "    with oracledb.connect(user='erp_user', password='erp_pass', dsn=dsn_tns) as conn:\n",
    "        cust = pd.DataFrame()\n",
    "\n",
    "        query = f\"\"\"\n",
    "        select * from\n",
    "        NCR_DEM_KEY_ACCOUNT_ASSG\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch data into a pandas DataFrame and append to the main DataFrame\n",
    "        cust = pd.read_sql(query, con=conn)\n",
    "\n",
    "else:\n",
    "    print('SQL Error')\n",
    "\n",
    "#print(KA.columns)\n",
    "value = {'MASTER_CUST_NUM':'Master Customer Number','MASTER_CUST_NAME':'Master Customer Name','KEY_ACCOUNT':'Key Account'}\n",
    "cust.rename(columns=value, inplace=True)\n",
    "cust.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce1508eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Master Customer Number'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Master Customer Number'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#cust = pd.read_excel(r'E:\\_Projects\\lookups\\Key_Account_map.xlsx',sheet_name='Export Worksheet')\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m cust[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaster Customer Number\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cust[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaster Customer Number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      4\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaster Customer Number\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaster Customer Number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      6\u001b[0m cust \u001b[38;5;241m=\u001b[39m cust[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaster Customer Number\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaster Customer Name\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKey Account\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32me:\\softwares\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Master Customer Number'"
     ]
    }
   ],
   "source": [
    "#cust = pd.read_excel(r'E:\\_Projects\\lookups\\Key_Account_map.xlsx',sheet_name='Export Worksheet')\n",
    "\n",
    "cust['Master Customer Number'] = cust['Master Customer Number'].astype(str)\n",
    "data['Master Customer Number'] = data['Master Customer Number'].astype(str)\n",
    "\n",
    "cust = cust[['Master Customer Number','Master Customer Name','Key Account']]\n",
    "\n",
    "cust.drop_duplicates(inplace=True)\n",
    "\n",
    "data = pd.merge(data,cust,on='Master Customer Number',how='left')\n",
    "\n",
    "data['Key Account'] = data['Key Account'].apply(lambda x: \"OTHER\" if pd.isna(x) or x.strip() == \"\" else x)\n",
    "\n",
    "data['Key Account'] = data['Key Account'].fillna(\"OTHER\")\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926ced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Junk chars in CUST; and 25 chars only\n",
    "#batch1\n",
    "data['junk'] = data['Customer_End Customer'].str.len() - \\\n",
    "    data['Customer_End Customer'].str.count(r'[a-zA-Z0-9.,:;\\'\\\"\\`~!@#$%^& -_-()/\\\\]')\n",
    "\n",
    "data['Customer'] = np.where((data['junk']  <= 3) & (data['Customer_End Customer'] != ''), \\\n",
    "                                 data['Customer_End Customer'], data['Master Customer Name'])\n",
    "\n",
    "data['Customer'] = np.where((data['Customer'] == 'OTHER') | (data['Customer'].isnull()), \\\n",
    "                               data['Org Code']+' CUSTOMER', data['Customer'])\n",
    "#batch2\n",
    "data['junk'] = data['Customer'].str.len() - \\\n",
    "    data['Customer'].str.count(r'[a-zA-Z0-9.,:;\\'\\\"\\`~!@#$%^& -_-()/\\\\]')\n",
    "data['Customer'] = np.where((data['junk']  > 3), \\\n",
    "                               data['Org Code']+' CUSTOMER', data['Customer'])\n",
    "\n",
    "data.drop(columns={'Master Customer Name','junk'}, inplace=True, axis=1)\n",
    "\n",
    "data['Customer'] = np.where(data['Customer'].str.len()>25, data['Customer'].str[:23] + '..', data['Customer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f82e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DEMAND VAL'] = data['DEMAND VAL'].astype('Int64')\n",
    "data['LN'] = data['Line Number'].astype(str).replace(r'\\.', '_', regex=True)\n",
    "cols_risk2 = ['Org Code','DEMAND VAL','LN','PID']\n",
    "data['id2'] = data[cols_risk2].astype(str).apply('_'.join, axis=1)\n",
    "\n",
    "risk = pd.read_csv(r'E:\\_Projects\\orders\\rcv\\\\risk_upside.csv', encoding='utf-8', low_memory=False)\n",
    "risk['order_nbr'] = risk['order_nbr'].astype (\"Int64\").astype(str)\n",
    "risk['order_line_nbr'] = risk['order_line_nbr'].astype(int).astype(str)\n",
    "risk['shipment_nbr'] = risk['shipment_nbr'].astype(int).astype(str)\n",
    "\n",
    "cols_risk1 = ['Country','order_nbr','order_line_nbr','shipment_nbr','item_hyp']\n",
    "risk['id'] = risk[cols_risk1].astype(str).apply('_'.join, axis=1)\n",
    "risk.drop_duplicates(inplace=True,subset=['id'],keep='first')\n",
    "data = pd.merge(data, risk[['id', 'RiskCode', 'RiskMonth', 'UpsideCode', 'UpsideMonth']], \\\n",
    "                left_on=['id2'], right_on=['id'], how='left',validate='m:1')\n",
    "data.drop(columns={'id','id2'}, axis=1, inplace=True) \n",
    "\n",
    "data.drop(columns={'LN'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ramiya upside Qtr ---> upside Mthn\n",
    "upd_period(data, 'UpsideMonth', 'UpsideMonth Wk', 'UpsideMonth Mon', 'UpsideMonth Qtr')\n",
    "data.drop(columns={'UpsideMonth Wk', 'UpsideMonth Mon'}, axis=1, inplace=True)\n",
    "print(data['UpsideMonth Qtr'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16220718",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas = pd.read_excel(r'E:\\_Projects\\lookups\\aas_logic.xlsx', sheet_name='logic', engine='openpyxl')\n",
    "\n",
    "data['As_a_Serv'] = ''\n",
    "for index, row in aas.iterrows():\n",
    "    data['As_a_Serv'] = np.where((data[row['Field']].str.startswith(row['Customer_AAS'])) & \\\n",
    "                                 (data['PID Type'] == row['PID Type']) & (data['Products'] == row['Prod']), \\\n",
    "                                 'As-a-Serv', data['As_a_Serv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79034215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UNIQE ID and COMMENTS fields\n",
    "# data['UID'] = data['Whse'].astype(str) + data['SO Ship Org'].astype(str) + data['Org Code'].astype(str) + \\\n",
    "#             data['DEMAND VAL'].astype(str)  + data['Line Number'].astype(str) + data['PID'].astype(str) + \\\n",
    "#             data['IO/PO No.'].astype(str)\n",
    "data['Line Number'] = data['Line Number'].fillna('')\n",
    "data['Line Number'] = data['Line Number'].astype(str)\n",
    "data['UID'] = data['PID'].astype(str) +'|'+data['Org Code'].astype(str)+'|'+ data['DEMAND VAL'].astype(str) + '|'+data['Line Number'].astype(str)\n",
    "\n",
    "\n",
    "# col = data.pop('UID')\n",
    "# data.insert(0, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6385d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['IO No.'] = data['IO No.'].astype(str)\n",
    "data['IO/PO No.'] = np.where(data['IO/PO No.'].isnull(),'0',data['IO/PO No.'])\n",
    "data['IO/PO No.'] = pd.to_numeric(data['IO/PO No.']).astype(int).astype(str)\n",
    "data['IO/PO No.'] = np.where(data['IO/PO No.']=='0','',data['IO/PO No.'])\n",
    "\n",
    "data['IID'] =''\n",
    "data['IID'] = np.where(data['IO/PO No.'].notnull(),data['PID']+'|'+data['IO/PO No.'].astype(str), '')\n",
    "# col = data.pop('IID')\n",
    "# data.insert(1, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03438190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concate address\n",
    "data['Ship To Cust'] = data['Ship To Customer Name'] + ', ' + data['Ship To Address1'] +', '+ data['Ship To Address2'] +', '+\\\n",
    "                       data['Ship To City'] + ', ' + data['Ship To State'] +', '+ data['Ship To Zip']\n",
    "data['Ship to Intmed'] = data['Intmed Ship To'] + ', ' + data['Intmed Ship To Address1'] +', '+ data['Intmed Ship To City'] +', '+\\\n",
    "                       data['Intmed Ship To State'] + ', ' + data['Intmed Ship To Zip'] \n",
    "#Drop the column\n",
    "data.drop(columns={'Ship To Customer Name','Ship To Address1','Ship To Address2','Ship To City','Ship To State','Ship To Zip','Intmed Ship To','Intmed Ship To Address1','Intmed Ship To City','Intmed Ship To State','Intmed Ship To Zip'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b11093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qty field - SupplyQty with OH, RS\n",
    "conditions = [data['Supply Qty'] > 0, data['OH Qty'] > 0, data['Res Qty'] > 0]\n",
    "choices = [data['Supply Qty'], data['OH Qty'], data['Res Qty']]\n",
    "data['Qty'] = np.select(conditions, choices)\n",
    "#Ramya\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f70fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# _SO_Status\n",
    "# SO SSD < Today()\tSO SSD Past\n",
    "# SO SAD >= (SO PD and SO PD not null)\tSO SSD Late\n",
    "# Only PBO: SO SSD < Today()+28d\tPBO within executn per\n",
    "# (SO SSD w/o IO/PO AND SO SSD) < Today()+28d\tSO w/o Supply\n",
    "# SO PD is null\tSO PD Null\n",
    "from pandas.tseries.offsets import BDay\n",
    "lst_sply=['Planned Buy', 'Planned order', 'Purchase requisition']\n",
    "data['current_date'] = datetime.today()\n",
    "condtn1=data['Ship Date'] < data['current_date']\n",
    "condtn2=(data['PD_Order'].notnull()) & (data['SAD'] > data['PD_Order'])\n",
    "\n",
    "condtn3=(data['Demand']=='Order Pre Bld') & (data['Ship Date'] < (data['current_date']+timedelta(days=28))) & \\\n",
    "            (data['IO/PO No.']!='')\n",
    "\n",
    "condtn4=(data['Demand']!='Order Pre Bld') & (data['Ship Date'] < (data['current_date']+timedelta(days=28))) & \\\n",
    "            (data['IO/PO No.']=='')  & data['Supply Type'].isin(lst_sply)\n",
    "\n",
    "condtn5=(data['PD_Order'].isnull() & (data['current_date'] < (data['Booked Dt'] + BDay(5))) & \\\n",
    "             (data['Demand']=='Order'))\n",
    "condtnX=(~data['Demand'].isin(['Order Flow Stk']) & data['Supply Qty']>0)\n",
    "condtnY=(data['Supply Qty']>0)\n",
    "\n",
    "data['_SO_Status'] =''\n",
    "data['_SO_Status'] = np.where(condtn1 & condtnX, 'SO SSD Past',  \\\n",
    "                              np.where(condtn2 & condtnX,'SO SSD Late', \\\n",
    "                                      np.where((condtn3 & condtnY),'PBO w/n Exec Per', \\\n",
    "                                              np.where(condtn4 & condtnY, 'SO w/o Supply', \\\n",
    "                                                      np.where(condtn5 & condtnY, 'SO PD Null','')))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Transit Time\n",
    "tt = pd.read_csv(r'E:\\_Projects\\lookups\\sit_transit_days.csv', encoding='utf-8', low_memory=False)\n",
    "data = pd.merge(data, tt, left_on=['Whse','Org Code'], right_on=['SOURCE','CTRY'], how='left',validate='m:1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce63a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyodbc\n",
    "# Hari/Ramiya: Instead of reading CSV from SUSDAY1322, read directly from MS Acess DB\n",
    "# E:\\RACE\\SIT-II\\SIT-II.accdb; query is \"qry_ship-meth_TT\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc TRANSIT DAYS\n",
    "# Land --> Land\n",
    "# Sea/Air --> for all mcids and ATM, SCO, KIOSK products --> Sea\n",
    "# Sea/Air --> for other products --> Air\n",
    "data['Transit_recom_mode']=''\n",
    "condtn1 = data['Default']=='Land'\n",
    "condtn2 = (data['Default']=='Sea/Air') & (data['Products'].isin(['ATM','SCO','Kiosk'])) & (data['PID Type'].isin(['M','U']))\n",
    "condtn3 = (data['Default'].notnull()) & (data['Transit_recom_mode']=='')\n",
    "data['Transit_recom_mode'] = np.where(condtn1, 'Land', \\\n",
    "                                     np.where(condtn2, 'Sea', \\\n",
    "                                             np.where(condtn3, 'Air', '')))\n",
    "condtns = data['Transit_recom_mode']=='Land', data['Transit_recom_mode']=='Sea', data['Transit_recom_mode']=='Air'\n",
    "values = data['Land'], data['Sea'], data['Air']\n",
    "data['Transit_recom_days'] = np.select(condtns, values)\n",
    "\n",
    "data.drop(columns={'SOURCE','CTRY','Default','Sea','Land','Air'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _IO/PO_Status\n",
    "# (IO/PO PD/SSD + Transit Time) > SO SSD\n",
    "# PO PD is null, only if IO SSD + 5 days\n",
    "# sply_type=['PD','PO','Purchase requisition','InTransit']\n",
    "# sply_type2=['IREQ','PD','Planned Buy','Planned Order','PO','Purchase requisition']\n",
    "# # data['IO_PO check'] =''\n",
    "# # data['IO_PO check'] = np.where(data['PD_PO'].notnull(), data['PD_PO'], \\\n",
    "# #                               np.where(data['IO SSD'].notnull(), data['IO SSD'],data['IO_PO check']))\n",
    "# # data['IO_PO check'] = pd.to_datetime(data['IO_PO check'])\n",
    "# data['IO_PO check2'] =  data['Plant SSD'] + pd.to_timedelta(data['Transit_recom_days'], unit='D')\n",
    "\n",
    "# condtn1a = (data['IO_PO check2'] > data['Ship Date']) & (data['Supply Type']=='PD')\n",
    "# condtn1b = (data['IO_PO check2'] > data['Ship Date']) & (data['PD_PO'].isnull()) & ~(data['Supply Type'].isin(sply_type))\n",
    "\n",
    "# condtn2 = data['IO/PO No.'].notnull() & data['PD_PO'].isnull() & (data['Supply Type'].isin(sply_type2)) & \\\n",
    "#             (data['current_date'] < data['IO BOOKED DATE']+timedelta(days=5) )\n",
    "# condtn3 = 'intransit'\n",
    "# data['_IO_Status'] = np.where(condtn1a | condtn1b, 'IO SSD Late', \\\n",
    "#                              np.where(condtn2, 'PO PD Null',''))\n",
    "# data.drop(columns={'current_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d62387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrubbing\n",
    "data['Region'] = data['Region'].str.upper()\n",
    "if data['LOB'].dtype == object:\n",
    "    data['LOB'] = data['LOB'].str[0:3]\n",
    "data['LOB'] = np.where((data['Products']=='SCO') & (data['LOB']=='FIN'), '', data['LOB'])\n",
    "data['LOB Prod'] = np.where(data['Products'].isin(['ATM','PC Core']),'Bank','Com') \n",
    "data['LOB Prod'] = np.where(data['LOB Prod'].isnull(),'Com',data['LOB Prod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83acbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns={'current_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hari/Ramiya\n",
    "# Concatenate Default\tSea\tLand\tAir\n",
    "# data['Transit_Days'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#including Dataref corresponding to current date.\n",
    "file_name = file[7:].lower()\n",
    "data['Dataref'] = str(d) + ' ' + file_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f0f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updaing Offer pfbase don Pit type: 20-02-2023\n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['K']),'kits',data['Offer PF']) \n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['C']),'Cables',data['Offer PF']) \n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['CPP']),'CPP',data['Offer PF'])\n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['IPP']),'IPP',data['Offer PF'])\n",
    "data['Offer PF'] = np.where(data['PID Type'].isin(['RAW']),'RAW',data['Offer PF'])\n",
    "data['Products'] = np.where(data['Products'].isnull(),'Other',data['Products'])\n",
    "data['Offer PF'] = np.where(data['Offer PF'].isnull(),'Other',data['Offer PF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3bfb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated validation flag and Supply Type \n",
    "data['Validation Flag'] = data['Validation Flag'].replace({'On hand - RS' : 'OH-RS','On hand':'OH', 'PLAN':'Plan', 'InTransit':'IT', \\\n",
    "                                                          'PD OK':'PD OK', 'WIP/Planned Order':'WIP/Plnd Ord', 'New/Release PO':'New/Rel PO',\\\n",
    "                                                          'Scheduled':'Schd', 'Flow':'Flow', 'Purchase requisition':'Pur Req', 'Planned order':'Plnd Ord',\\\n",
    "                                                          'Shipment in receiving':'In Rcvng', 'Past Due PD':'Past PD', 'WIP/PlannedOrder':'WIP/Plnd Ord'})\n",
    "\n",
    "data['Supply Type'] = data['Supply Type'].replace({'On hand - RS' : 'OH-RS','On hand':'OH', 'Planned Buy':'Plnd-Buy', 'InTransit':'IT', \\\n",
    "                                                          'PD OK':'PD OK', 'WIP/Planned Order':'WIP/Plnd Ord', 'New/Release PO':'New/Rel PO',\\\n",
    "                                                          'Scheduled':'Schd', 'Flow':'Flow', 'Purchase requisition':'Pur Req', 'Planned order':'Plnd Ord',\\\n",
    "                                                          'Shipment in receiving':'In Rcvng', 'Past Due PD':'Past PD', 'WIP/PlannedOrder':'WIP/Plnd Ord', 'Planned Make':'Plnd-Make', 'Planned Buy':'Plnd-Buy'})\n",
    "\n",
    "#Update by ramya\n",
    "cond= data['Validation Flag'].isin(['']) & data['Supply Type'].isin(['PO'])\n",
    "data['Validation Flag'] = np.where(cond,'PO',data['Validation Flag'])\n",
    "\n",
    "#updated LOB \n",
    "data['LOB'] = np.where(data['LOB'].isin(['CUS']),'',data['LOB'])\n",
    "data['LOB'] = np.where(data['LOB'].isin(['HSR']),'HOSP',data['LOB'])\n",
    "\n",
    "#Update Demand\n",
    "data['Demand'] = np.where(data['Demand'].isin(['Order Int']),'Ord Int',data['Demand'])\n",
    "data['Demand'] = np.where(data['Demand'].isin(['Order Pre Bld']),'PBO',data['Demand'])\n",
    "data['Demand'] = np.where(data['Demand'].isin(['Order Flow Stk']),'Flow Stk',data['Demand'])\n",
    "\n",
    "conditions = [data['Inv Trigger']=='Shipment',data['Inv Trigger']=='Delivery',data['Inv Trigger']=='Other', \\\n",
    "              data['Inv Trigger']=='Installation',data['Inv Trigger']=='Customer Certification']\n",
    "choices = ['Ship','Del','Oth','Inst','Cert']\n",
    "data['Inv Trigger'] = np.select(conditions, choices)\n",
    "data['Inv Trigger'] = np.where (data['Inv Trigger'].astype(str)=='0','',data['Inv Trigger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc data['CTS']\n",
    "# Calc: considering ATS status and OH qty by order level)\n",
    "# For one order, If \"SO Avail to Ship\"==OK/Yes; and Check enough OH/RS for rest of the lines --> then CTS\n",
    "df = data.copy()\n",
    "df = df[['DEMAND VAL','OH Qty','Res Qty','Supply Qty']]\n",
    "df = df.groupby(['DEMAND VAL'])['OH Qty','Res Qty','Supply Qty'].sum().reset_index()\n",
    "df['total'] = df['OH Qty'] + df['Res Qty']\n",
    "df['CTS'] = np.where(df['total'] > df['Supply Qty'],'CTS','No CTS')\n",
    "data = pd.merge(data, df[['DEMAND VAL','CTS']], left_on=['DEMAND VAL'], right_on=['DEMAND VAL'],how='left',validate='m:1')\n",
    "\n",
    "data['Dmd Cnt'] =  1 / data.groupby('DEMAND VAL')['DEMAND VAL'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb4c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPM names\n",
    "\n",
    "cpm_names = pd.read_csv(r'E:\\_Projects\\orders\\rcv\\\\CPM_Names.csv',encoding='utf-8', low_memory=False)\n",
    "cpm_names.rename(columns = {'Customer Number': 'Master Customer Number','CPM Name 2023':'CPM','Manager Name 2023':'CPM Mngr'}, inplace = True)\n",
    "cpm_names.drop_duplicates(inplace=True,subset=['Master Customer Number'],keep='first')\n",
    "data = data.merge(cpm_names[['Master Customer Number','CPM','CPM Mngr']],on='Master Customer Number',how='left',validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3451c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#comments\n",
    "path = r'E:\\_Projects\\workbench'\n",
    "comments =  pd.read_csv(path + r'\\comments_log\\comments_unique.csv', encoding='utf-8', low_memory=False)\n",
    "#read wb_cols\n",
    "col_map = pd.read_excel(path + r'\\map\\wb_cols.xlsx', sheet_name='field_settings_ms_STD', engine='openpyxl')\n",
    "\n",
    "#iid columns\n",
    "col_iid_list = col_map[col_map['Key ID'].isin(['IID'])]\n",
    "col_iid_list = ['IID'] + col_iid_list['Field Name'].tolist()\n",
    "comments_iid = comments[col_iid_list]\n",
    "comments_iid = comments_iid[comments_iid['IID'].notnull()]\n",
    "\n",
    "#uid columns\n",
    "col_uid_list = col_map[col_map['Key ID'].isin(['UID'])]\n",
    "col_uid_list = ['UID'] + col_uid_list['Field Name'].tolist()\n",
    "comments_uid = comments[col_uid_list]\n",
    "comments_uid = comments_uid[comments_uid['UID'].notnull()]\n",
    "\n",
    "#merge uiid and iid\n",
    "data = data.merge(comments_iid, left_on='IID', right_on = 'IID', how='left',validate='m:1')\n",
    "data = data.merge(comments_uid, left_on='UID', right_on = 'UID', how='left',validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate DMD SSD and SPLY SSD\n",
    "data['Calc SO SSD']=''\n",
    "cond1 = (data['Whse']==data['SO Ship Org'])\n",
    "cond2 = (data['Whse'].isin(['CHE','CHS','GF1','US','USL','UF2','EPL','EPI']))\n",
    "data['Calc SO SSD']=np.where(cond1 & cond2, data['Ship Date'],data['Calc SO SSD'])\n",
    "data['Calc SO SSD']=np.where((~cond1) & cond2, data['Ship Date'] - pd.to_timedelta(data['Transit_recom_days'], unit='D'), \\\n",
    "                              data['Calc SO SSD'])\n",
    "align_past('Calc SO SSD')\n",
    "# data['Calc SO SSD'] = data['final']\n",
    "data.drop(columns={'Previous_week_Date'}, inplace=True)\n",
    "\n",
    "data['SO SSD Override++'] = pd.to_datetime(data['SO SSD Override++'],errors='coerce')\n",
    "data['Calc SO SSD Final'] = np.where(data['SO SSD Override++'].notnull(), data['SO SSD Override++'], \\\n",
    "                                np.where(data['Calc SO SSD'].notnull(), data['Calc SO SSD'], data['Ship Date']))\n",
    "#----------------------------------------------------------------------------#\n",
    "\n",
    "# data['Calc Sply SSD']=''\n",
    "# data['temp'] = pd.to_datetime('today')\n",
    "# cond3 = (data['Supply Type'].isin(['OH','OH-RS']))\n",
    "# cond4 = (data['Supply Type'].isin(['PD']))\n",
    "# data['Calc Sply SSD'] = np.where(cond2 & cond3, data['temp'] - timedelta(days=14), data['Calc Sply SSD'])\n",
    "# data['Calc Sply SSD'] = np.where(cond2 & cond4, data['PD_PO'] + pd.to_timedelta(data['Transit_recom_days'], unit='D'),\\\n",
    "#                                                                                          data['Calc Sply SSD'])\n",
    "# data['Calc Sply SSD'] = data['Calc Sply SSD'].astype('datetime64[ns]')\n",
    "# align_past('Calc Sply SSD')\n",
    "# data['Calc Sply SSD'] = data['final']\n",
    "# data.drop(columns={'final'}, inplace=True)\n",
    "\n",
    "# data.drop(columns={'temp'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ebaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  data['Open Qty'] = data['Open Qty'].astype('Int64')\n",
    "# data['Open Qty'] = data['Open Qty'].values.astype(np.int64)\n",
    "# data['Res Qty'] = data['Res Qty'].values.astype(np.int64)\n",
    "# data['OH Qty'] = data['OH Qty'].values.astype(np.int64)\n",
    "# data['Supply Qty'] = data['Supply Qty'].values.astype(np.int64)\n",
    "# data['Qty'] = data['Supply Qty'].values.astype(np.int64)\n",
    "# data['Supply Qty'].head(10)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change dtypes str to qty\n",
    "# col_list=['Open Qty','Res Qty','OH Qty','Supply Qty','Qty']\n",
    "# for col in col_list:\n",
    "#     data[col] = data[col].values.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# Status on IO: Late\n",
    "race_df  =  pd.read_excel(r'E:\\_Projects\\lookups\\RACE_IO_SSD.xlsx', engine='openpyxl')\n",
    "race_df.sort_values(by=['Actual Ship Date'], ascending = True,inplace = True)\n",
    "race_df.drop_duplicates(subset=['Order Number'],keep='last',inplace=True)\n",
    "data = data.merge(race_df[['Order Number','Actual Ship Date']],left_on='IO No.',right_on='Order Number',how='left',validate='m:1')\n",
    "data['Actual Ship Date'] = np.where(data['Actual Ship Date'].isnull(),data['IO SSD'],data['Actual Ship Date'])\n",
    "data['Actual Ship Date']  = data['Actual Ship Date'] + pd.to_timedelta(data['Transit_recom_days'], unit='D')\n",
    "data['Plant SSD Trans'] =  data['Plant SSD'] + pd.to_timedelta(data['Transit_recom_days'], unit='D')\n",
    "\n",
    "\n",
    "cond = (data['Plant SSD'].isnull() | data['Validation Flag'].isin(['Plan']))\n",
    "logic_1= (data['Whse'] == data['SO Ship Org']) & (data['Plant SSD']>=data['Ship Date'])\n",
    "logic_2 = (data['Whse'] != data['SO Ship Org']) & (data['Supply Type']=='IT') & (data['Actual Ship Date'] >=data['Ship Date'])\n",
    "logic_3 = (data['Whse'] != data['SO Ship Org']) & ( data['Plant SSD Trans'] >=data['Ship Date']) & (data['Supply Type']!='IT')\n",
    "\n",
    "conds = [~cond & logic_1,~cond & logic_2,~cond & logic_3]\n",
    "choices = ['Late','Late','Late']\n",
    "\n",
    "data['_IO_Status'] = np.select(condlist=conds, choicelist=choices)\n",
    "data['_IO_Status']  = np.where(data['_IO_Status']=='0','',data['_IO_Status'])\n",
    "data.drop(columns={'Plant SSD Trans','Actual Ship Date','Order Number'}, axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_offer = pd.read_csv(r'E:\\_Projects\\lookups\\preferred_offer.csv', \\\n",
    "                    encoding='utf-8', low_memory=False)\n",
    "pref_offer_grpd = pref_offer.groupby('PID')['Type'].max()\n",
    "data = pd.merge(data, pref_offer_grpd, left_on='PID', right_on='PID', how='left',validate='m:1')\n",
    "data.rename(columns={'Type' : 'Preferred Offer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_map = pd.read_excel(r'E:\\_Projects\\workbench\\map\\wb_cols.xlsx', sheet_name='field_list', engine='openpyxl')\n",
    "col_map = col_map['Fields'].unique().tolist()\n",
    "data = data[col_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9fdb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove enter in all columns\n",
    "data = data.replace('\\n',';', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3980b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export wb data into local folder\n",
    "out_name = 'workbench_data_' + file[7:].lower() + \\\n",
    "            '_' + str(datetime.strptime(d, '%d-%b-%Y').year).zfill(4) + str(datetime.strptime(d, '%d-%b-%Y').month).zfill(2) + \\\n",
    "            str(datetime.strptime(d, '%d-%b-%Y').day).zfill(2)\n",
    "\n",
    "out_name_wPO = 'workbench_data_' + file[7:].lower() + \\\n",
    "            '_wPO_' + str(datetime.strptime(d, '%d-%b-%Y').year).zfill(4) + str(datetime.strptime(d, '%d-%b-%Y').month).zfill(2) + \\\n",
    "            str(datetime.strptime(d, '%d-%b-%Y').day).zfill(2)\n",
    "\n",
    "\n",
    "#save local in csv\n",
    "data['Key Account'] = data['Key Account'].apply(lambda x: \"OTHER\" if pd.isna(x) or x.strip() == \"\" else x)\n",
    "\n",
    "dst_local = r'E:\\_Projects\\_outputs\\orders\\snd\\\\' \n",
    "data.to_csv(dst_local + out_name + '_v2.csv', index=False)\n",
    "\n",
    "#save server tool in csv\n",
    "dst_tool = r'E:\\_Projects\\workbench\\snd\\\\'\n",
    "data.to_csv(dst_tool + out_name + '.zip', compression={'method': 'zip', 'archive_name': out_name + '.csv'}, index=False)\n",
    "try:\n",
    "    #input()\n",
    "    #sharpoint_Path =r'\\\\wtc1501cifs.prod.local\\ORGSHARE\\TEAMS\\ERP Shared Folder\\NCR Atleos\\Atleos Operations Planning\\Global Ops Analytics\\Metrics\\Production\\workbench\\snd\\\\'\n",
    "    data.to_csv(r'\\\\wtc1501cifs.prod.local\\ORGSHARE\\TEAMS\\ERP Shared Folder\\NCR Atleos\\Atleos Operations Planning\\Global Ops Analytics\\Metrics\\Production\\workbench\\snd\\\\' + out_name + '.zip', compression={'method': 'zip', 'archive_name': out_name + '.csv'}, index=False)\n",
    "    #data.to_csv(sharpoint_Path + out_name + '.zip', compression={'method': 'zip', 'archive_name': out_name + '.csv'}, index=False)\n",
    "except:\n",
    "    print('Server location not working')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAll allocation script\n",
    "file_name = file[7:].lower()\n",
    "if file_name == 'eumocp':\n",
    "    %run Orders_Allocation.ipynb\n",
    "    #%run alloc_v2.py enn eumocp 20240514\n",
    "    #%run alloc_v2.py jab eumocp 20240514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_date = datetime.now(pytz.timezone('Asia/Calcutta'))\n",
    "today_date = today_date.strftime('%Y%m%d')\n",
    "pervious_date='20240115'\n",
    "pervious_date_enn='20231225'\n",
    "pervious_date_USI='20230828'\n",
    "print(today_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13bbdb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if file_name =='eumocp':\n",
    "    %run Orders.ipynb\n",
    "    if PD_Enn ==0:\n",
    "        enn_alloc= pd.read_csv(r'E:\\_Projects\\_outputs\\orders\\snd\\wb_enn_allocation_'+today_date+'.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "        enn_alloc['Plant'] = 'EN2'\n",
    "        enn_alloc['Demand'] = 'PO-ENN'\n",
    "    else:\n",
    "        enn_alloc= pd.read_csv(r'E:\\_Projects\\_outputs\\orders\\snd\\wb_enn_allocation_'+pervious_date_ENN+'.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "        enn_alloc['Plant'] = 'EN2'\n",
    "        enn_alloc['Demand'] = 'PO-ENN'\n",
    "\n",
    "\n",
    "    usi_alloc= pd.read_csv(r'E:\\_Projects\\_outputs\\orders\\snd\\wb_usi_allocation_'+pervious_date_USI+'.csv',encoding='utf-8', low_memory=False, keep_default_na='')\n",
    "    usi_alloc['Plant'] = 'UN2'\n",
    "    usi_alloc['Demand'] = 'PO-USI'\n",
    "\n",
    "    alloc = pd.concat([enn_alloc,usi_alloc],axis=0)\n",
    "    alloc = alloc[data.columns]\n",
    "\n",
    "    set_date_frmt = ['Booked Dt','Request Dt','PD_Order','Ship Date','SAD','IO BOOKED DATE','IO SSD','IO SAD','PD_PO','CRSD','PRD','Plant Ship date','NEED BY DATE','Plant SSD']\n",
    "    for field in set_date_frmt:\n",
    "        alloc[field] = pd.to_datetime(alloc[field])\n",
    "\n",
    "    data = pd.concat([data,alloc],axis=0)\n",
    "    \n",
    "    data['Key Account'] = data['Key Account'].apply(lambda x: \"OTHER\" if pd.isna(x) or x.strip() == \"\" else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of columns to delete\n",
    "columns_to_delete = ['Preferred Offer']\n",
    "\n",
    "# Drop the specified columns\n",
    "data.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "\n",
    "data = pd.merge(data, pref_offer_grpd, left_on='PID', right_on='PID', how='left',validate='m:1')\n",
    "data.rename(columns={'Type' : 'Preferred Offer'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dst_local = 'E:\\_Projects\\_outputs\\orders\\snd\\\\'\n",
    "data.to_csv(dst_local + out_name_wPO + '_v2.csv', index=False)\n",
    "data.to_csv('E:\\_Projects\\Production Status\\workbench_data_eumocp_wPO.csv', index=False)\n",
    "\n",
    "\n",
    "# dst_dump = r'\\\\dayorg1\\orgshare\\TEAMS\\ERP Shared Folder\\Global S&OP\\workbench\\snd\\\\'\n",
    "# data.to_csv(dst_dump + out_name + '.csv', index=False)\n",
    "\n",
    "# dst_tool = r'\\\\dayorg1\\orgshare\\TEAMS\\ERP Shared Folder\\Global S&OP\\Supply Plan\\workbench\\snd\\\\'\n",
    "# data.to_csv(dst_tool + out_name + '.zip', compression={'method': 'zip', 'archive_name': out_name + '.csv'}, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c996f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
