{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eb19138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b329ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = r'E:\\_Projects\\MCID Tagging\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd0f31e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 8) (2060334945.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    Date_Input = '2025-12-17\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 8)\n"
     ]
    }
   ],
   "source": [
    "EUM_OCP = 'OCP_IA_EUMOCP_17-DEC-2025'\n",
    "\n",
    "# 0 - Pul Data from SQL / 1 - Load Data from local\n",
    "SQL_Data = 0\n",
    "SQL_MCID = 0\n",
    "\n",
    "## YYYY-MM-DD\n",
    "Date_Input = '2025-12-17'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3310b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_ia = pd.read_csv('Output\\\\ref\\\\'+EUM_OCP+'.csv')\n",
    "    #data_ia = pd.read_excel('Output\\\\ref\\\\'+EUM_OCP+'.xlsx')\n",
    "    data_ia['Region'] = data_ia['Region'].str.upper()\n",
    "except:\n",
    "    print(EUM_OCP+' is pulling from server')\n",
    "    #data_ia = pd.read_csv(r'E:\\_Projects\\_outputs\\orders\\snd\\workbench_data_eumocp_wPO_20240920_v2.csv',encoding='utf-8', low_memory=False)\n",
    "    data_ia = pd.read_excel(r'\\\\susday014.prod.local\\cdunix\\ERP\\RR\\Inbound\\OCP\\IA\\rcv\\\\'+EUM_OCP+'.xlsx')\n",
    "    data_ia.to_csv('Output\\\\ref\\\\'+EUM_OCP+'.csv',index=False)\n",
    "    data_ia['Region'] = data_ia['Region'].str.upper()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1616b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d566b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ia_bkp = data_ia\n",
    "# data_ia = data_ia_bkp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63150962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if SQL_MCID == 0:\n",
    "\n",
    "    # Enable thick mode\n",
    "    oracledb.init_oracle_client(lib_dir=r\"E:\\softwares\\instantclient-basic-windows.x64-23.5.0.24.07\\instantclient_23_5\")\n",
    "\n",
    "    # Connection details\n",
    "    dsn_tns = oracledb.makedsn('153.84.75.216', 1521, service_name='ebs_ATMPROD')\n",
    "\n",
    "\n",
    "    # Use a context manager to handle the connection\n",
    "    with oracledb.connect(user='erp_user', password='erp_pass', dsn=dsn_tns) as conn:\n",
    "        data_sod_bom = pd.DataFrame()\n",
    "\n",
    "        query = f\"\"\"\n",
    "        select DISTINCT ass.segment1 PID \n",
    "        FROM \n",
    "        (select segment1,description,inventory_item_id,item_type,organization_id,creation_date from mtl_system_items_b) ass, \n",
    "        (select segment1,description,inventory_item_id,item_type,organization_id from mtl_system_items_b) comp, \n",
    "        (select assembly_item_id,bill_sequence_id,organization_id from bom_structures_b where  alternate_bom_designator is null) s, \n",
    "        bom_components_b  c, ORG_ORGANIZATION_DEFINITIONS org \n",
    "        where ass.organization_id = org.organization_id and comp.organization_id = org.organization_id  \n",
    "        and s.organization_id = org.organization_id and ass.organization_id = s.organization_id  \n",
    "        and comp.organization_id = s.organization_id  \n",
    "        and c.disable_date is null AND comp.item_type IN ('FEATURE','MFG-BASE-MOD','CID') \n",
    "        and s.bill_sequence_id = c.bill_sequence_id and ass.inventory_item_id = s.assembly_item_id \n",
    "        and comp.inventory_item_id = c.component_item_id  \n",
    "        and  ass.segment1 LIKE '%MC%'  AND org.organization_code = 'MST' \n",
    "        and ass.creation_date >= '01-Jul-24'\n",
    "        and comp.segment1 NOT LIKE ('%-R%')\n",
    "        and comp.segment1 NOT LIKE ('%-D%')\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch data into a pandas DataFrame and append to the main DataFrame\n",
    "        data_sod_MCID = pd.read_sql(query, con=conn)\n",
    "        \n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    data_sod_MCID.to_csv('Output/ref/MCID_BY_FEATURE_F_D.csv', index=False)\n",
    "else:\n",
    "    data_sod_MCID = pd.read_csv('Output/ref/MCID_BY_FEATURE_F_D.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load the text file into a DataFrame\n",
    "OSS = pd.read_csv(Path+'Input\\OSS.txt', delimiter='|')  # Adjust the delimiter as needed\n",
    "\n",
    "OSS = OSS.drop_duplicates(subset=['CLASS'], keep='first')\n",
    "\n",
    "####################################################\n",
    "# A-01-Order MCID update\n",
    "\n",
    "data_ia['Class']  = data_ia['Class'].astype(str)\n",
    "\n",
    "OSS['CLASS']  = OSS['CLASS'].astype(str)\n",
    "\n",
    "data_ia = pd.merge(data_ia, OSS['CLASS'], left_on = 'Class', right_on='CLASS', how='inner')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc58643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "folder_path = Path + 'Input\\\\'\n",
    "\n",
    "\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "\n",
    "out_files = [file for file in files if file.endswith('.out')]\n",
    "\n",
    "\n",
    "dataframes = []\n",
    "for file in out_files:\n",
    "    #print(file)\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    OSS = pd.read_csv(file_path, delimiter='|')  # Assuming tab-delimited format\n",
    "    dataframes.append(OSS)\n",
    "\n",
    "\n",
    "for i, OSS in enumerate(dataframes):\n",
    "    print(f\"DataFrame {i+1} from file {out_files[i]}:\")\n",
    "    #print(df)\n",
    "    \n",
    "#print(OSS.columns)\n",
    "\n",
    "df_oss = OSS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf7610",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ia = data_ia.loc[data_ia['PID Type'] == 'M']\n",
    "data_ia = data_ia[['PID']]\n",
    "\n",
    "data_ia = pd.concat([data_sod_MCID,data_ia],ignore_index=True)\n",
    "\n",
    "data_ia.drop_duplicates(inplace=True,subset=['PID'],keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce74d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ia.to_csv('Output/ref/eumocp.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8b814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sod_bom = pd.read_csv(r'\\\\dayorg1\\orgshare\\TEAMS\\ERP Shared Folder\\_Global Supply Chain\\_Demand Planning\\snd\\\\' + \\\n",
    " #                          'bom 20240801.csv', encoding='utf-8', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_sod_bom_bkp = data_sod_bom\n",
    "# data_sod_bom = data_sod_bom_bkp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54201c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "MCIDS = data_ia['PID'].drop_duplicates()\n",
    "#print(MCIDS.head())\n",
    "\n",
    "\n",
    "pid_list = MCIDS.tolist()\n",
    "#pid_list = ', '.join(f\"'{pid}'\" for pid in pid_list)\n",
    "\n",
    "#print(pid_str)  # To check the formatted string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7445e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import oracledb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if SQL_Data == 0:\n",
    "\n",
    "    # Enable thick mode\n",
    "    oracledb.init_oracle_client(lib_dir=r\"E:\\softwares\\instantclient-basic-windows.x64-23.5.0.24.07\\instantclient_23_5\")\n",
    "\n",
    "    # Connection details\n",
    "    dsn_tns = oracledb.makedsn('153.84.75.216', 1521, service_name='ebs_ATMPROD')\n",
    "\n",
    "\n",
    "    \n",
    "    def chunk_list(lst, chunk_size):\n",
    "        for i in range(0, len(lst), chunk_size):\n",
    "            yield lst[i:i + chunk_size]\n",
    "\n",
    "    \n",
    "    with oracledb.connect(user='erp_user', password='erp_pass', dsn=dsn_tns) as conn:\n",
    "        data_sod_bom = pd.DataFrame()\n",
    "\n",
    "        for chunk in chunk_list(pid_list, 1000):\n",
    "            pid_str = ', '.join(f\"'{pid}'\" for pid in chunk)\n",
    "\n",
    "            query = f\"\"\"\n",
    "            SELECT DISTINCT SUBSTR(ass.segment1, 1, 4) AS Class, ass.segment1 MCID, comp.segment1 Item,\n",
    "                   1 AS Component_Qty, comp.description AS Product_Description, comp.item_type Product_Type,\n",
    "                   SYSDATE as Effective_Date\n",
    "            FROM\n",
    "                (SELECT segment1, description, inventory_item_id, item_type, organization_id FROM mtl_system_items_b) ass,\n",
    "                (SELECT segment1, description, inventory_item_id, item_type, organization_id FROM mtl_system_items_b) comp,\n",
    "                (SELECT assembly_item_id, bill_sequence_id, organization_id FROM bom_structures_b WHERE alternate_bom_designator IS NULL) s,\n",
    "                bom_components_b c, ORG_ORGANIZATION_DEFINITIONS org\n",
    "            WHERE ass.organization_id = org.organization_id\n",
    "              AND comp.organization_id = org.organization_id\n",
    "              AND s.organization_id = org.organization_id\n",
    "              AND ass.organization_id = s.organization_id\n",
    "              AND comp.organization_id = s.organization_id\n",
    "              AND c.disable_date IS NULL\n",
    "              AND comp.item_type IN ('FEATURE', 'CID')\n",
    "              AND s.bill_sequence_id = c.bill_sequence_id\n",
    "              AND ass.inventory_item_id = s.assembly_item_id\n",
    "              AND comp.inventory_item_id = c.component_item_id\n",
    "              AND ass.segment1 IN ({pid_str})\n",
    "            \"\"\"\n",
    "\n",
    "            # Fetch data into a pandas DataFrame and append to the main DataFrame\n",
    "            chunk_df = pd.read_sql(query, con=conn)\n",
    "            data_sod_bom = pd.concat([data_sod_bom, chunk_df], ignore_index=True)\n",
    "    data_sod_bom.rename(columns={'CLASS': 'Class'}, inplace=True)\n",
    "    data_sod_bom.rename(columns={'ITEM': 'Item'}, inplace=True)\n",
    "    data_sod_bom.rename(columns={'COMPONENT_QTY': 'Component Qty'}, inplace=True)\n",
    "    data_sod_bom.rename(columns={'PRODUCT_DESCRIPTION': 'Product Description'}, inplace=True)\n",
    "    data_sod_bom.rename(columns={'PRODUCT_TYPE': 'Product Type'}, inplace=True)\n",
    "    data_sod_bom.rename(columns={'EFFECTIVE_DATE': 'Effective Date'}, inplace=True)\n",
    "\n",
    "    ############### SQL Extract Description remapping\n",
    "    data_sod_bom.drop_duplicates(subset=['MCID','Item'],inplace=True)\n",
    "\n",
    "    MCID_ref = pd.read_csv(r'E:\\_Projects\\MCID Tagging\\Input\\MCID_Tagging _Reference.csv')\n",
    "\n",
    "    MCID_ref.drop_duplicates(subset=['Basemodels'],inplace=True)\n",
    "\n",
    "    MCID_ref = MCID_ref[['Basemodels','Description']]\n",
    "\n",
    "    MCID_ref.rename(columns={'Basemodels' : 'Item'},inplace=True)\n",
    "    MCID_ref.rename(columns={'Basemodels' : 'Item'},inplace=True)\n",
    "\n",
    "    data_sod_bom = pd.merge(data_sod_bom, MCID_ref, on='Item', how='left')\n",
    "    data_sod_bom.loc[data_sod_bom['Description'].notnull(),'Product Description'] = data_sod_bom['Description']\n",
    "\n",
    "    data_sod_bom = data_sod_bom[['Class','MCID','Item','Component Qty','Product Description','Product Type','Effective Date']]\n",
    "\n",
    "    data_sod_bom.to_csv('Output/ref/SQL_Extract.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_sod_bom_Bk = data_sod_bom.copy() \n",
    "else:\n",
    "    data_sod_bom = pd.read_csv('Output/ref/SQL_Extract.csv')\n",
    "    data_sod_bom_Bk = data_sod_bom.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sod_bom_MCC = data_sod_bom[data_sod_bom['Item'].str.contains('MCC')]\n",
    "#data_sod_bom_MCC.to_csv(\"data_sod_bom_MCC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b48215",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sod_bom = data_sod_bom.dropna(subset=['Item'], how='any')\n",
    "data_sod_bom = data_sod_bom[data_sod_bom['Item'].str.contains('-F') & ~(data_sod_bom['Item'].str.contains('-F000'))]\n",
    "data_sod_bom = data_sod_bom[['MCID', 'Item']]\n",
    "data_sod_bom.drop_duplicates(subset=['MCID', 'Item'], keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298531b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = pd.merge(data_ia, data_sod_bom, left_on=['PID'], right_on=['MCID'], how='left')\n",
    "\n",
    "#data_out.to_csv('Output/workout.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_bom = data_out.loc[data_out['MCID'].isnull()]\n",
    "na_bom = na_bom[['PID']]\n",
    "na_bom.to_csv(Path+'/Output/na_bom.csv', index=False)\n",
    "# del na_bom\n",
    "# BOM not avaialble for listed MCID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e84f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ref = pd.read_csv(Path+'\\Input\\MCID_Tagging _Reference.csv', \\\n",
    "           encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = data_out[data_out['MCID'].notnull()]\n",
    "data_out = pd.merge(data_out, data_ref, left_on='Item', right_on='Basemodels', how='left')\n",
    "#data_out.to_csv(Path+'/Output/data_out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695cbe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Class = pd.read_excel(Path+'\\Input\\Class.xlsx')\n",
    "data_Class.drop_duplicates(subset=['Class'], keep=False, inplace=True)\n",
    "data_Class['Class'] = data_Class['Class'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f955b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out['Class'] = data_out['MCID'].str[:4]\n",
    "\n",
    "na_feat = data_out.loc[data_out['Tag'].isnull()]\n",
    "\n",
    "na_feat = na_feat[na_feat['MCID'].str[:4] == na_feat['Item'].str[:4]]\n",
    "na_feat['Class']= na_feat['Item'].str[:4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26309feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#na_feat.to_csv(Path+'/Output/data_out_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15020172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "na_feat = pd.merge(na_feat,data_Class, on='Class', how='inner')\n",
    "\n",
    "na_feat.drop_duplicates(subset=['Item'],inplace=True)\n",
    "\n",
    "#print(na_feat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#na_feat_des = data_sod_bom_Bk[['PID','Product Description']].copy()\n",
    "na_feat_des = data_sod_bom_Bk[['Item','Product Description']].copy()\n",
    "\n",
    "na_feat_des.drop_duplicates(subset=['Item'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0267ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_feat = na_feat[['Class','Item','Range_y']]\n",
    "na_feat = pd.merge(na_feat,na_feat_des,on='Item', how='left')\n",
    "\n",
    "#na_feat.to_csv(Path+'/Output/na_data.csv', index=False)\n",
    "na_feat[['Class','Item','Product Description','Range_y']].to_csv(Path+'/Output/na_feat.csv', index=False)\n",
    "\n",
    "na_feat_print = pd.DataFrame()\n",
    "na_feat_print = na_feat[['Class','Item','Product Description','Range_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_out.to_csv(\"data_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a62ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#na_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddef3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# na_feat = na_feat[['MCID']]\n",
    "# na_feat.rename(columns={'MCID' : 'MCID_na'}, inplace=True)\n",
    "# data_out = pd.merge(data_out, na_feat, left_on='PID', right_on='MCID_na', how='left')\n",
    "# na_mcid = data_out.loc[data_out['MCID_na'].notnull()]\n",
    "# na_mcid.to_csv(Path+'/Output/ref/na_mcid.csv', index=False)\n",
    "# data_out = data_out.loc[data_out['MCID_na'].isnull()]\n",
    "# del na_mcid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa41311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5462546",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = data_out.loc[data_out['Tag'] != 'OTHER']\n",
    "data_out = data_out.sort_values(['MCID','Sort value'], ascending=True)\n",
    "\n",
    "data_out = data_out[data_out['Tag'].notna() & (data_out['Tag'].str.strip() != '')]\n",
    "\n",
    "#print(data_out['Tag'])\n",
    "#data_out.to_csv(\"data_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1038a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Tag' column to strings\n",
    "data_out['Tag'] = data_out['Tag'].astype(str)\n",
    "\n",
    "# Now perform the groupby and aggregation\n",
    "data_final = data_out.groupby(['MCID'], as_index=False).agg({'Tag': ' '.join})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = pd.merge(data_final,data_sod_bom_MCC,left_on='MCID',right_on='MCID',how='left')\n",
    "data_final.to_csv(Path+'/Output/ref/Check.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = data_final['Item'].notnull()\n",
    "data_final['Tag'] = np.where(cond1,data_final['Item'] +' '+data_final['Tag'],data_final['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd742f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_out.to_csv('./snd/out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv('Output/ref/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac867c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb716517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ac19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append data_final to consolidated TAG file\n",
    "#data_tagging=pd.read_excel(r'E:\\RACE\\RACE_GLOBAL\\MCID_Tagging _Master.xlsx',engine='openpyxl')\n",
    "data_tagging=pd.read_excel(Path+'Input\\MCID_Tagging _Master.xlsx',engine='openpyxl')\n",
    "data_tagging['ref'] = 'Master'\n",
    "\n",
    "data_final.rename(columns={'Tag':'Tag Name'},inplace =True)\n",
    "data_final['Class'] = data_final['MCID'].str[:4]\n",
    "date = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "data_final['Append_Date'] = date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f3167",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = pd.concat([data_tagging,data_final],axis=0)\n",
    "data_final['Append_Date'] = np.where(data_final['Append_Date'].isnull(),Date_Input,data_final['Append_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30efdc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_final.sort_values(['Append_Date'],ascending=False)\n",
    "\n",
    "data_final.to_csv(Path+'Output\\\\ref\\\\Validate_MCID_Tagging_Master.csv', index=False)\n",
    "#data_final.drop_duplicates(subset=['MCID'], keep='first', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca25013",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Duplicates\n",
    "\n",
    "# Create a mask to identify rows where 'ref' is 'Master'\n",
    "master_mask = data_final['ref'] == 'Master'\n",
    "\n",
    "# Drop duplicates based on 'MCID', prioritizing rows where 'ref' is 'Master'\n",
    "data_final = data_final.loc[master_mask].append(data_final.loc[~master_mask]).drop_duplicates(subset=['MCID'], keep='first')\n",
    "\n",
    "data_final = data_final.sort_values(['Append_Date'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv(Path+'Output\\\\ref\\\\data_append.csv', index=False)\n",
    "data_final=data_final[['Class','MCID','Tag Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'MCID_Tagging _Master.xlsx'\n",
    "sheet_name = 'MCID_Tagging__Master_TABLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_excel(Path +'\\Input\\\\' + file_name,sheet_name = sheet_name , index=False)\n",
    "\n",
    "data_final.to_excel(r'E:\\RACE\\RACE_GLOBAL\\\\' + file_name,sheet_name = sheet_name , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OSS Restriction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lead time\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "# Define the connection string\n",
    "conn_str = (\n",
    "    r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'\n",
    "    r'DBQ=E:\\RACE\\SIT-II\\SIT-II.accdb;'\n",
    ")\n",
    "\n",
    "# Establish the connection\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "# Query the data\n",
    "query = 'SELECT * FROM [t_sourcing_rules]'\n",
    "t_sourcing_rules = pd.read_sql(query, conn)\n",
    "\n",
    "query = 'SELECT * FROM [ref-cntry_ship_method]'\n",
    "ref_cntry_ship_method = pd.read_sql(query, conn)\n",
    "\n",
    "query = 'SELECT * FROM [Ref-plant LT]'\n",
    "ref_plant_lt = pd.read_sql(query, conn)\n",
    "\n",
    "query = 'SELECT * FROM [Ref-Dist-Country]'\n",
    "ref_dist_country = pd.read_sql(query, conn)\n",
    "\n",
    "query = 'SELECT * FROM [Ref-Class-ShipMethod-Oride]'\n",
    "ref_class_shipmethod_oride = pd.read_sql(query, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "t_sourcing_rules['Plant'] = t_sourcing_rules['Plant'].str.upper()\n",
    "ref_cntry_ship_method['SOURCE'] = ref_cntry_ship_method['SOURCE'].str.upper()\n",
    "ref_plant_lt['Plant'] = ref_plant_lt['Plant'].str.upper()\n",
    "\n",
    "# Perform the LEFT JOINs\n",
    "merged_df = pd.merge(t_sourcing_rules, ref_cntry_ship_method, how='left', left_on=['Country Code', 'Plant'], right_on=['CTRY', 'SOURCE'])\n",
    "\n",
    "ref_plant_lt['Class'] = ref_plant_lt['Class'].astype(str)\n",
    "merged_df['Class'] = merged_df['Class'].astype(str)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(merged_df, ref_plant_lt, how='left', left_on=['Plant', 'Class'], right_on=['Plant', 'Class'])\n",
    "#merged_df.to_csv(Path+'Output\\merged_df.csv', index=False)\n",
    "\n",
    "merged_df = pd.merge(merged_df, ref_class_shipmethod_oride, how='left', left_on='Class', right_on='class')\n",
    "#merged_df.to_csv(Path+'Output\\merged_df.csv', index=False)\n",
    "\n",
    "\n",
    "# Perform the RIGHT JOIN\n",
    "merged_df = pd.merge(merged_df, ref_dist_country, how='left', left_on='Country Code', right_on='Dist-Ctry-Code')\n",
    "\n",
    "# Add the calculated column\n",
    "merged_df['Default-Ship-Meth'] = merged_df.apply(lambda row: 'Customer' if pd.notnull(row['Dist-Ctry-Code']) else row['Default'], axis=1)\n",
    "\n",
    "# Select the required columns\n",
    "result_df = merged_df[['Region', 'Country Code', 'Country Name', 'Zone', 'Prod Range', 'Prod Range Description', 'Class', 'SIT_TOOL', 'Plant', 'Default-Ship-Meth', 'Sea', 'Land', 'Air', 'Unit LT', 'Kit LT']]\n",
    "\n",
    "# Sort the DataFrame\n",
    "LT = result_df.sort_values(by=['SIT_TOOL', 'Region', 'Country Code', 'Prod Range', 'Class'], ascending=[False, True, True, True, True])\n",
    "LT['Region'] = LT['Region'].str.upper()\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "LT.to_excel(r'E:\\RACE\\RACE_GLOBAL\\LT.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80df57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IA-01-EUM_OCP_IA_Q\n",
    "\n",
    "# Perform the query\n",
    "result = data_ia_bkp[data_ia_bkp['PID'].str.contains('M', na=False)]\n",
    "\n",
    "\n",
    "\n",
    "# If you need to join with another DataFrame (like the INNER JOIN in your query)\n",
    "# Load the text file into a DataFrame\n",
    "\n",
    "#OSS = pd.read_csv(Path+'Input\\OSS.txt', delimiter='|')  # Adjust the delimiter as needed\n",
    "\n",
    "\n",
    "####################################################\n",
    "# A-01-Order MCID update\n",
    "\n",
    "result['Class']  = result['Class'].astype(str)\n",
    "\n",
    "OSS['CLASS']  = OSS['CLASS'].astype(str)\n",
    "\n",
    "OSS = OSS[~OSS['INVENTORY_ITEM'].str.contains('%M%|-R000|F000|-C', regex=True)]\n",
    "\n",
    "OSS = OSS.drop_duplicates(subset=['CLASS'], keep='first')\n",
    "\n",
    "\n",
    "\n",
    "final_result = result.merge(OSS['CLASS'], left_on='Class',right_on=['CLASS'], how='inner')\n",
    "\n",
    "#print(final_result)\n",
    "\n",
    "\n",
    "# Select the required columns\n",
    "columns = [\n",
    "    'Demand', 'Region', 'SO Ship Org', 'Org Code', 'Area', 'Org Name', 'DEMAND VAL', 'Line Number',\n",
    "    'Customer_End Customer', 'CDP', 'CIS', 'Booked Dt', 'Request Dt', 'PD_Order', 'Ship Date', 'SAD',\n",
    "    'Inv Trigger', 'Product Range', 'Class', 'PID', 'PID Desc', 'PID Type', 'Open Qty', 'Res Qty',\n",
    "    'OH Qty', 'Supply Qty', 'ERP Line Status', 'Whse', 'IR No.', 'IO No.', 'IO BOOKED DATE', 'IO SSD',\n",
    "    'IO SAD', 'Validation Flag', 'Supply Type', 'Supply Detail', 'PD_PO', 'Buyer Name', 'Supplier',\n",
    "    'MakeBuy', 'Demand Class', 'Seiban Number', 'WOT Quote', 'Customer PO', 'CRSD', 'CRSD Miss',\n",
    "    'Ship Set', 'PRD', 'Order Type', 'Hold Type', 'Hold Name', 'LOB', 'Ship To Customer Name',\n",
    "    'Ship To Address1', 'Ship To Address2', 'Ship To City', 'Ship To State', 'Ship To Zip',\n",
    "    'Intmed Ship To', 'Intmed Ship To Address1', 'Intmed Ship To City', 'Intmed Ship To State',\n",
    "    'Intmed Ship To Zip', 'Salesperson', 'Complex Staging Flag', 'SSID', 'CCHold', 'Hardware Package',\n",
    "    'Master Customer Number', 'PO Age', 'Order Bklg Value-USD', 'Plant Ship date', 'NEED BY DATE'\n",
    "]\n",
    "\n",
    "df_eum = final_result[columns]\n",
    "df_eum_ocp = df_eum\n",
    "#df_eum.to_csv(Path+'Output\\\\ref\\\\IA-01-EUM_OCP_IA_Q.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IA-06-OSS country mapping update\n",
    "# Load the data into DataFrames\n",
    "#df_eum = pd.read_csv(r'E:\\_Projects\\MCID Tagging\\Output\\OSS\\IA-01-EUM_OCP_IA_Q.csv')\n",
    "df_country = pd.read_csv(Path+'Input\\Country_mapping.csv')\n",
    "\n",
    "\n",
    "# Perform the LEFT JOIN\n",
    "merged_df = pd.merge(df_eum, df_country, left_on='Whse', right_on='ORg', how='left')\n",
    "\n",
    "# Apply the conditional logic for Whse_1 and Expr1\n",
    "merged_df['Whse_1'] = merged_df.apply(lambda row: row['Whse'] if pd.isnull(row['Country code']) else row['Country code'], axis=1)\n",
    "merged_df['Expr1'] = merged_df.apply(lambda row: row['Region_x'] if pd.isnull(row['Region_y']) else row['Region_y'], axis=1)\n",
    "\n",
    "# Select the required columns\n",
    "columns = [\n",
    "    'Demand', 'Region_x', 'SO Ship Org', 'Org Code', 'Area', 'Org Name', 'DEMAND VAL', 'Line Number',\n",
    "    'Customer_End Customer', 'CDP', 'CIS', 'Booked Dt', 'Request Dt', 'PD_Order', 'Ship Date', 'SAD',\n",
    "    'Inv Trigger', 'Product Range', 'Class', 'PID', 'PID Desc', 'PID Type', 'Open Qty', 'Res Qty',\n",
    "    'OH Qty', 'Supply Qty', 'ERP Line Status', 'Whse', 'IR No.', 'IO No.', 'IO BOOKED DATE', 'IO SSD',\n",
    "    'IO SAD', 'Validation Flag', 'Supply Type', 'Supply Detail', 'PD_PO', 'Buyer Name', 'Supplier',\n",
    "    'MakeBuy', 'Demand Class', 'Seiban Number', 'WOT Quote', 'Customer PO', 'CRSD', 'CRSD Miss',\n",
    "    'Ship Set', 'PRD', 'Order Type', 'Hold Type', 'Hold Name', 'LOB', 'Ship To Customer Name',\n",
    "    'Ship To Address1', 'Ship To Address2', 'Ship To City', 'Ship To State', 'Ship To Zip',\n",
    "    'Intmed Ship To', 'Intmed Ship To Address1', 'Intmed Ship To City', 'Intmed Ship To State',\n",
    "    'Intmed Ship To Zip', 'Salesperson', 'Complex Staging Flag', 'SSID', 'CCHold', 'Hardware Package',\n",
    "    'Master Customer Number', 'PO Age', 'Order Bklg Value-USD', 'Plant Ship date', 'NEED BY DATE',\n",
    "    'Whse_1', 'Expr1', 'Country code'\n",
    "]\n",
    "\n",
    "df_ia = merged_df[columns].drop_duplicates()\n",
    "#df_ia.to_csv(r'E:\\_Projects\\MCID Tagging\\Output\\ref\\IA-06-OSS_country_mapping_update.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IA-07-Country_sourcing\n",
    "\n",
    "# Load the data into DataFrames\n",
    "#df_ia = pd.read_csv(r'E:\\_Projects\\MCID Tagging\\Output\\OSS\\IA-06-OSS_country_mapping_update.csv')\n",
    "\n",
    "df_tt = LT\n",
    "df_tt['Class'] = df_tt['Class'].astype(str)\n",
    "df_ia['Class'] = df_ia['Class'].astype(str)\n",
    "\n",
    "# Perform the LEFT JOIN\n",
    "merged_df = pd.merge(df_ia, df_tt, left_on=['Whse_1', 'Class'], right_on=['Country Code', 'Class'], how='left')\n",
    "\n",
    "\n",
    "# Select the required columns\n",
    "df_country_sourcing = merged_df[['Plant', 'Class', 'Region_x', 'Whse']]\n",
    "df_country_sourcing.rename(columns={'Region_x': 'Region'}, inplace=True)\n",
    "\n",
    "\n",
    "df_country_sourcing.drop_duplicates(inplace=True)\n",
    "\n",
    "#df_country_sourcing.to_csv(Path+'Output\\\\ref\\\\IA-07-Country_sourcing.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IA-08-Plant maping\n",
    "# Load the data into DataFrames\n",
    "#df_country_sourcing = pd.read_csv(r'E:\\_Projects\\MCID Tagging\\Output\\OSS\\IA-07-Country_sourcing.csv')\n",
    "df_tt_sit = LT\n",
    "\n",
    "# Perform the LEFT JOIN\n",
    "merged_df = pd.merge(df_country_sourcing, df_tt_sit, on=['Class', 'Region'], how='left')\n",
    "\n",
    "#merged_df.to_csv(r'E:\\\\_Projects\\\\MCID Tagging\\\\Output\\\\source_rule.csv', index=False)\n",
    "\n",
    "# Apply the conditional logic for Final_plant\n",
    "#merged_df['Final_plant'] = merged_df.apply(lambda row: row['Plant_y'] if pd.isnull(row['Plant_x']) else row['Plant_x'], axis=1)\n",
    "\n",
    "merged_df['Final_plant'] = merged_df.apply(\n",
    "    lambda row: row['Plant_y'] if pd.isnull(row['Plant_x']) and not pd.isnull(row['Plant_y']) else row['Plant_x'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Select the required columns and remove duplicates\n",
    "df_source_rule = merged_df[['Class', 'Region', 'Final_plant', 'Whse']].drop_duplicates()\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df_source_rule.to_csv(r'E:\\\\_Projects\\\\MCID Tagging\\\\Output\\\\ref\\\\source_rule_local_update.csv', index=False)\n",
    "\n",
    "#print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87780b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IA-05-OSS restriction features\n",
    "\n",
    "# Load the data into DataFrames\n",
    "df_order_mcid = data_sod_MCID\n",
    "df_mcid_feature = data_sod_bom_Bk\n",
    "\n",
    "#print(df_mcid_feature.columns)\n",
    "# Load the text file into a DataFrame\n",
    "#df_oss = pd.read_csv(Path+'Input\\OSS.txt', delimiter='|')  # Adjust the delimiter as needed\n",
    "\n",
    "\n",
    "####################################################\n",
    "# A-01-Order MCID update\n",
    "\n",
    "df_oss['CLASS']  = df_oss['CLASS'].astype(str)\n",
    "\n",
    "\n",
    "df_mcid_feature.rename(columns={'Item': 'PID'}, inplace=True)\n",
    "\n",
    "print(df_mcid_feature.columns)\n",
    "\n",
    "df_mcid_feature.drop_duplicates(subset=['Class','MCID','PID'], keep='first')\n",
    "\n",
    "print(df_mcid_feature.columns)\n",
    "# Perform the LEFT JOINs\n",
    "merged_df1 = pd.merge(df_mcid_feature[['Class','MCID','PID']], df_oss, left_on='PID', right_on='INVENTORY_ITEM', how='left')\n",
    "\n",
    "#merged_df2 = pd.merge(df_order_mcid, merged_df1, left_on='PID', right_on='MCID', how='left')\n",
    "\n",
    "merged_df2 = pd.merge(df_order_mcid, merged_df1, left_on='PID', right_on='MCID', how='right')\n",
    "\n",
    "\n",
    "\n",
    "# Apply the WHERE clause conditions\n",
    "filtered_df = merged_df2[\n",
    "    \n",
    "    ~merged_df2['PID_y'].str.contains(r'\\*MC') &\n",
    "    ~merged_df2['PID_y'].str.contains(r'\\*-R000') &\n",
    "    ~merged_df2['PID_y'].str.contains(r'\\*MC\\*')\n",
    "]\n",
    "\n",
    "# Select the required columns and remove duplicates\n",
    "columns = [\n",
    "    'PID_y', 'DESCRIPTION', 'MCID', 'Ennoconn', 'Budapest', 'Chennai (Dummy)', 'Chennai (Primary)',\n",
    "    'Chennai (Secondary)', 'Columbus', 'Dundee (NPI)', 'Easypoint', 'Jabil', 'Manaus', 'Midland',\n",
    "    'Sao Paulo', 'Shiloh (Dummy)', 'Waterloo'\n",
    "]\n",
    "df_oss_restriction = filtered_df[columns].drop_duplicates()\n",
    "\n",
    "df_oss_restriction.rename(columns={'PID_y':'PID'},inplace=True)\n",
    "\n",
    "df_oss_restriction['PID'] = df_oss_restriction['PID'].str.upper()\n",
    "\n",
    "# Define the list of substrings to check\n",
    "substrings = ['R000', '-C', 'F000', 'M']\n",
    "\n",
    "# Create a boolean mask where 'desi' does not contain any of the substrings, ignoring case\n",
    "mask = ~df_oss_restriction['PID'].str.contains('|'.join(substrings), case=False)\n",
    "\n",
    "# Filter the DataFrame using the mask\n",
    "df_oss_restriction = df_oss_restriction[mask]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df_oss_restriction.to_csv(Path+'Output\\\\ref\\\\OSS_restriction_feature_w_MCID.csv', index=False)\n",
    "\n",
    "#print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IA-09-OSS restriction features order\n",
    "\n",
    "\n",
    "# Load the data into DataFrames\n",
    "#df_source_rule = pd.read_csv(r'E:\\_Projects\\MCID Tagging\\Output\\OSS\\source_rule_local_update.csv')\n",
    "#df_oss_restriction = pd.read_csv(r'E:\\_Projects\\MCID Tagging\\Output\\OSS\\OSS_restriction_feature_w_MCID.csv')\n",
    "#df_eum_ocp = pd.read_csv(r'E:\\_Projects\\MCID Tagging\\Output\\OSS\\IA-01-EUM_OCP_IA_Q.csv')\n",
    "\n",
    "# Perform the LEFT JOINs\n",
    "merged_df1 = pd.merge(df_eum_ocp, df_source_rule, on=['Class', 'Whse'], how='left')\n",
    "#merged_df1.to_csv(Path+'Output\\merged_df1.csv', index=False)\n",
    "\n",
    "merged_df1.drop_duplicates(inplace=True)\n",
    "df_oss_restriction.drop_duplicates(inplace=True)\n",
    "\n",
    "#print(merged_df1.columns)\n",
    "merged_df2 = pd.merge(merged_df1, df_oss_restriction, left_on='PID', right_on='MCID', how='left')\n",
    "merged_df2.drop_duplicates(inplace=True)\n",
    "#merged_df2.to_csv(Path+'Output\\df_oss_restriction.csv', index=False)\n",
    "\n",
    "# Apply the conditional logic for the WHERE clause\n",
    "conditions = [\n",
    "    (merged_df2['Final_plant'] == 'CHE', merged_df2['Chennai (Primary)']),\n",
    "    (merged_df2['Final_plant'] == 'CHS', merged_df2['Chennai (Secondary)']),\n",
    "    (merged_df2['Final_plant'] == 'EC', merged_df2['Ennoconn']),\n",
    "    (merged_df2['Final_plant'] == 'JAB', merged_df2['Jabil'])\n",
    "]\n",
    "\n",
    "merged_df2['Condition_Result'] = ''\n",
    "for condition, result in conditions:\n",
    "    merged_df2.loc[condition, 'Condition_Result'] = result\n",
    "\n",
    "#merged_df2.to_csv(Path+'Output\\df_oss_restriction.csv', index=False)\n",
    "\n",
    "    \n",
    "filtered_df = merged_df2[merged_df2['Condition_Result'].isnull()]\n",
    "\n",
    "\n",
    "#print(filtered_df.columns)\n",
    "# Select the required columns and remove duplicates\n",
    "columns = [\n",
    "    'Final_plant', 'PID_y', 'DESCRIPTION', 'Demand', 'Region_x', 'SO Ship Org', 'Org Code', 'Area', 'Org Name',\n",
    "    'DEMAND VAL', 'Line Number', 'Customer_End Customer', 'CDP', 'CIS', 'Booked Dt', 'Request Dt', 'PD_Order',\n",
    "    'Ship Date', 'SAD', 'Inv Trigger', 'Product Range', 'Class', 'PID_x', 'PID Desc', 'PID Type', 'Open Qty',\n",
    "    'Res Qty', 'OH Qty', 'Supply Qty', 'ERP Line Status', 'Whse', 'IR No.', 'IO No.', 'IO BOOKED DATE', 'IO SSD',\n",
    "    'IO SAD', 'Validation Flag', 'Supply Type', 'Supply Detail', 'PD_PO', 'Buyer Name', 'Supplier', 'MakeBuy',\n",
    "    'Demand Class', 'Seiban Number', 'WOT Quote', 'Customer PO', 'CRSD', 'CRSD Miss', 'Ship Set', 'PRD',\n",
    "    'Order Type', 'Hold Type', 'Hold Name', 'LOB', 'Ship To Customer Name', 'Ship To Address1', 'Ship To Address2',\n",
    "    'Ship To City', 'Ship To State', 'Ship To Zip', 'Intmed Ship To', 'Intmed Ship To Address1', 'Intmed Ship To City',\n",
    "    'Intmed Ship To State', 'Intmed Ship To Zip', 'Salesperson', 'Complex Staging Flag', 'SSID', 'CCHold',\n",
    "    'Hardware Package', 'Master Customer Number', 'PO Age', 'Order Bklg Value-USD', 'Plant Ship date', 'NEED BY DATE'\n",
    "]\n",
    "result = filtered_df[columns].drop_duplicates()\n",
    "\n",
    "result.rename(columns={'PID_y':'OSS restriction feature w MCID_PID'}, inplace=True)\n",
    "result.rename(columns={'Region_x':'Region'}, inplace=True)\n",
    "result.rename(columns={'DESCRIPTION':'Description'}, inplace=True)\n",
    "result.rename(columns={'PID_x':'IA-01-EUM_OCP_IA_Q_PID'}, inplace=True)\n",
    "\n",
    "#result = result[result['OSS restriction feature w MCID_PID'].notnull()]\n",
    "# Save the result to a new CSV file\n",
    "result.to_csv(Path+'Output\\OSS_restriction_MCID.csv', index=False)\n",
    "\n",
    "#print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46dcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### OSS Blank MCID | Feature\n",
    "exception_Feature = pd.read_excel(Path+'Input\\OSS_Exceptional.xlsx',sheet_name = 'Feature')\n",
    "exception_MCID = pd.read_excel(Path+'Input\\OSS_Exceptional.xlsx',sheet_name = 'MCID')\n",
    "\n",
    "exception_Feature.drop_duplicates(inplace=True)\n",
    "exception_MCID.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "data = pd.merge(result,exception_Feature,left_on='OSS restriction feature w MCID_PID',right_on='Feature', how='left')\n",
    "\n",
    "data = pd.merge(data,exception_MCID, left_on='IA-01-EUM_OCP_IA_Q_PID', right_on ='MCID', how='left')\n",
    "\n",
    "data = data[data['Description'].isnull()]\n",
    "data = data[~data['OSS restriction feature w MCID_PID'].str.contains('249', na=False)]\n",
    "data = data[~data['OSS restriction feature w MCID_PID'].str.startswith('7', na=False)]\n",
    "data = data[~data['OSS restriction feature w MCID_PID'].str.startswith('69', na=False)]\n",
    "data = data[data['Feature'].isnull()]\n",
    "data = data[data['MCID'].isnull()]\n",
    "\n",
    "\n",
    "columns_to_delete = ['Feature','MCID']\n",
    "\n",
    "\n",
    "data.drop(columns=columns_to_delete, inplace=True)\n",
    " \n",
    "\n",
    "data.to_csv(Path+'Output\\OSS_restriction_Blank.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b328bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_feat_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7449ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_ia, data_sod_bom, data_out, na_bom, na_feat, data_final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
